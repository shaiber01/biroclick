........................................................................ [  0%]
........................................................................ [  1%]
........................................................................ [  2%]
......................................................................F. [  3%]
FFFFFF....FF....FFFFFF....FF....F.........F.FF............F............. [  3%]
.F........FF.F.......................................................... [  4%]
........................................................................ [  5%]
........................................................................ [  6%]
........................................................................ [  6%]
........................................................................ [  7%]
........................................................................ [  8%]
........................................................................ [  9%]
........................................................................ [ 10%]
........................................................................ [ 10%]
........................................................................ [ 11%]
........................................................................ [ 12%]
........................................................................ [ 13%]
.............F.................F........................................ [ 13%]
.....................F.................................................. [ 14%]
........................................................................ [ 15%]
........................................................................ [ 16%]
........................................................................ [ 17%]
........................................................................ [ 17%]
......................................................F..F............FF [ 18%]
...............................FF......................................F [ 19%]
.F.....................................F.....FFFF....................F.. [ 20%]
FFF..............F.......F..........F.....................F............. [ 20%]
.........................................................F....F..F.F.... [ 21%]
.................F...................................................... [ 22%]
........................................................................ [ 23%]
........................................................................ [ 23%]
........................................................................ [ 24%]
........................................................................ [ 25%]
..FF.........F.F........F.F.......................F..........F.......... [ 26%]
.....................FFFF.F........F.F......F.....F.F................... [ 27%]
........................................................................ [ 27%]
...........................................F.F.......................... [ 28%]
........................................................................ [ 29%]
........................................................................ [ 30%]
.......................................s................................ [ 30%]
........................................................................ [ 31%]
........................................................................ [ 32%]
........................................................................ [ 33%]
........................................................................ [ 34%]
........................................................................ [ 34%]
........................................................................ [ 35%]
........................................................................ [ 36%]
.....................................F.................................. [ 37%]
........................................................................ [ 37%]
........................................................................ [ 38%]
........................................................................ [ 39%]
........................................................................ [ 40%]
..........................................F............................. [ 41%]
............................................F..F....FFF.FFFFFFF.....FFF. [ 41%]
........................................................................ [ 42%]
........................................................................ [ 43%]
....................................FFF.F..........F.................... [ 44%]
...............................................F........................ [ 44%]
........................................................................ [ 45%]
........................................................................ [ 46%]
........................................................................ [ 47%]
........................................................................ [ 47%]
........................................................................ [ 48%]
...................................F.....................FF.....F....... [ 49%]
.....................FF.F..F..FFF....F.................................. [ 50%]
........................................................................ [ 51%]
........................................................................ [ 51%]
........................................................................ [ 52%]
.....................................................F......FF....F..FFF [ 53%]
...FFF............................F..................................... [ 54%]
...........................................................F............ [ 54%]
.....F............F..................................................... [ 55%]
..........................................F............................. [ 56%]
........................................................................ [ 57%]
........................................................................ [ 58%]
........................................FFFFF..F.......FFF.FFF.......... [ 58%]
..F..................................................................... [ 59%]
........................................................................ [ 60%]
........................................................................ [ 61%]
...............................F......................................F. [ 61%]
........................................................................ [ 62%]
........................................................................ [ 63%]
........................................................................ [ 64%]
........................................................................ [ 64%]
........................................................................ [ 65%]
........................................................................ [ 66%]
........................................................................ [ 67%]
........................................................................ [ 68%]
........................................................................ [ 68%]
........................................................................ [ 69%]
........................................................................ [ 70%]
........................................................................ [ 71%]
........................................................................ [ 71%]
........................................................................ [ 72%]
........................................................................ [ 73%]
........................................................................ [ 74%]
........................................................................ [ 75%]
........................................................................ [ 75%]
........................................................................ [ 76%]
........................................................................ [ 77%]
........................................................................ [ 78%]
........................................................................ [ 78%]
........................................................................ [ 79%]
........................................................................ [ 80%]
........................................................................ [ 81%]
........................................................................ [ 82%]
........................................................................ [ 82%]
........................................................................ [ 83%]
........................................................................ [ 84%]
........................................................................ [ 85%]
........................................................................ [ 85%]
........................................................................ [ 86%]
........................................................................ [ 87%]
........................................................................ [ 88%]
........................................................................ [ 88%]
........................................................................ [ 89%]
........................................................................ [ 90%]
.............................................F.......................... [ 91%]
....F................................................................... [ 92%]
..............................................F..F..F...F.FF............ [ 92%]
.............................F.......................................... [ 93%]
.........................FFFFFFFFFFFFFFF....F.F......................... [ 94%]
........................................................................ [ 95%]
........................................................................ [ 95%]
........................................................................ [ 96%]
.F.F..................................F......F.......................... [ 97%]
.....................F..FFF.................F........................... [ 98%]
.........F.............F.FF..F...FF..................................... [ 99%]
............................................................F...F....... [ 99%]
..................                                                       [100%]
=================================== FAILURES ===================================
_ TestCodeGeneratorNode.test_generator_code_extraction[llm_output1-sim codesim codesim codesim codesim codesim codesim codesim codesim codesim code-expected_outputs1] _

self = <tests.agents.code.test_code_generator.TestCodeGeneratorNode object at 0x127e78a00>
llm_output = {'simulation_code': 'sim codesim codesim codesim codesim codesim codesim codesim codesim codesim code'}
expected_code = 'sim codesim codesim codesim codesim codesim codesim codesim codesim codesim code'
expected_outputs = []
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 0, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}

    @pytest.mark.parametrize(
        "llm_output, expected_code, expected_outputs",
        [
            (
                {"code": "valid code" * 10, "expected_outputs": ["Ez.csv"]},
                "valid code" * 10,
                ["Ez.csv"],
            ),
            (
                {"simulation_code": "sim code" * 10},
                "sim code" * 10,
                [],
            ),
            (
                dict(LONG_FALLBACK_PAYLOAD),
                LONG_FALLBACK_JSON,
                [],
            ),
        ],
    )
    def test_generator_code_extraction(self, llm_output, expected_code, expected_outputs, base_state):
        """Test extraction of code from various LLM output formats."""
        with patch("src.agents.code.build_agent_prompt"), \
             patch("src.agents.code.check_context_or_escalate", return_value=None), \
             patch("src.agents.code.call_agent_with_metrics") as mock_llm, \
             patch("src.agents.code.build_user_content_for_code_generator"):
    
            mock_llm.return_value = llm_output
    
            result = code_generator_node(base_state)
    
            assert result["workflow_phase"] == "code_generation"
>           assert result["code"] == expected_code
E           assert '{\n  "simula...esim code"\n}' == 'sim codesim ... codesim code'
E             
E             + {
E             - sim codesim codesim codesim codesim codesim codesim codesim codesim codesim code
E             +   "simulation_code": "sim codesim codesim codesim codesim codesim codesim codesim codesim codesim code"
E             ? +++   +++++++++++++++++++                                                                             ++
E             + }

tests/agents/code/test_code_generator.py:506: AssertionError
_ TestCodeGeneratorNode.test_generator_stub_code_output[# TODO: Implement simulation] _

self = <tests.agents.code.test_code_generator.TestCodeGeneratorNode object at 0x127e78c80>
mock_uc = <MagicMock name='build_user_content_for_code_generator' id='5192324768'>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5192325440'>
mock_check = <MagicMock name='check_context_or_escalate' id='5192325104'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192321408'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 0, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}
stub_code = '# TODO: Implement simulation'

    @pytest.mark.parametrize("stub_code", [
        "# TODO: Implement simulation",
        "STUB: code here",
        "PLACEHOLDER: add code",
        "# Replace this with actual code",
        "would be generated",
        "TODO: fix this",
    ])
    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.check_context_or_escalate")
    @patch("src.agents.code.call_agent_with_metrics")
    @patch("src.agents.code.build_user_content_for_code_generator")
    def test_generator_stub_code_output(self, mock_uc, mock_llm, mock_check, mock_prompt, base_state, stub_code):
        """Test error when LLM returns stub code with various stub markers."""
        initial_revision_count = base_state.get("code_revision_count", 0)
        mock_check.return_value = None
        mock_prompt.return_value = "System Prompt"
        mock_uc.return_value = "User Content"
        mock_llm.return_value = {"code": stub_code}
    
        result = code_generator_node(base_state)
    
        # Strict assertions - verify stub detection
        assert result["workflow_phase"] == "code_generation"
        expected_count = min(initial_revision_count + 1, base_state["runtime_config"]["max_code_revisions"])
>       assert result["code_revision_count"] == expected_count
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'code_revision_count'

tests/agents/code/test_code_generator.py:534: KeyError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=True, empty=True). Code generation must produce valid simulation code.
____ TestCodeGeneratorNode.test_generator_stub_code_output[STUB: code here] ____

self = <tests.agents.code.test_code_generator.TestCodeGeneratorNode object at 0x127e78af0>
mock_uc = <MagicMock name='build_user_content_for_code_generator' id='5133093584'>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5104225648'>
mock_check = <MagicMock name='check_context_or_escalate' id='5185627408'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5179013312'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 0, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}
stub_code = 'STUB: code here'

    @pytest.mark.parametrize("stub_code", [
        "# TODO: Implement simulation",
        "STUB: code here",
        "PLACEHOLDER: add code",
        "# Replace this with actual code",
        "would be generated",
        "TODO: fix this",
    ])
    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.check_context_or_escalate")
    @patch("src.agents.code.call_agent_with_metrics")
    @patch("src.agents.code.build_user_content_for_code_generator")
    def test_generator_stub_code_output(self, mock_uc, mock_llm, mock_check, mock_prompt, base_state, stub_code):
        """Test error when LLM returns stub code with various stub markers."""
        initial_revision_count = base_state.get("code_revision_count", 0)
        mock_check.return_value = None
        mock_prompt.return_value = "System Prompt"
        mock_uc.return_value = "User Content"
        mock_llm.return_value = {"code": stub_code}
    
        result = code_generator_node(base_state)
    
        # Strict assertions - verify stub detection
        assert result["workflow_phase"] == "code_generation"
        expected_count = min(initial_revision_count + 1, base_state["runtime_config"]["max_code_revisions"])
>       assert result["code_revision_count"] == expected_count
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'code_revision_count'

tests/agents/code/test_code_generator.py:534: KeyError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=True, empty=True). Code generation must produce valid simulation code.
_ TestCodeGeneratorNode.test_generator_stub_code_output[PLACEHOLDER: add code] _

self = <tests.agents.code.test_code_generator.TestCodeGeneratorNode object at 0x127e78b40>
mock_uc = <MagicMock name='build_user_content_for_code_generator' id='5145097760'>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5142774512'>
mock_check = <MagicMock name='check_context_or_escalate' id='5192319728'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192319392'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 0, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}
stub_code = 'PLACEHOLDER: add code'

    @pytest.mark.parametrize("stub_code", [
        "# TODO: Implement simulation",
        "STUB: code here",
        "PLACEHOLDER: add code",
        "# Replace this with actual code",
        "would be generated",
        "TODO: fix this",
    ])
    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.check_context_or_escalate")
    @patch("src.agents.code.call_agent_with_metrics")
    @patch("src.agents.code.build_user_content_for_code_generator")
    def test_generator_stub_code_output(self, mock_uc, mock_llm, mock_check, mock_prompt, base_state, stub_code):
        """Test error when LLM returns stub code with various stub markers."""
        initial_revision_count = base_state.get("code_revision_count", 0)
        mock_check.return_value = None
        mock_prompt.return_value = "System Prompt"
        mock_uc.return_value = "User Content"
        mock_llm.return_value = {"code": stub_code}
    
        result = code_generator_node(base_state)
    
        # Strict assertions - verify stub detection
        assert result["workflow_phase"] == "code_generation"
        expected_count = min(initial_revision_count + 1, base_state["runtime_config"]["max_code_revisions"])
>       assert result["code_revision_count"] == expected_count
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'code_revision_count'

tests/agents/code/test_code_generator.py:534: KeyError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=True, empty=True). Code generation must produce valid simulation code.
_ TestCodeGeneratorNode.test_generator_stub_code_output[# Replace this with actual code] _

self = <tests.agents.code.test_code_generator.TestCodeGeneratorNode object at 0x127e78b90>
mock_uc = <MagicMock name='build_user_content_for_code_generator' id='5192331488'>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5192320400'>
mock_check = <MagicMock name='check_context_or_escalate' id='5192328128'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192324432'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 0, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}
stub_code = '# Replace this with actual code'

    @pytest.mark.parametrize("stub_code", [
        "# TODO: Implement simulation",
        "STUB: code here",
        "PLACEHOLDER: add code",
        "# Replace this with actual code",
        "would be generated",
        "TODO: fix this",
    ])
    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.check_context_or_escalate")
    @patch("src.agents.code.call_agent_with_metrics")
    @patch("src.agents.code.build_user_content_for_code_generator")
    def test_generator_stub_code_output(self, mock_uc, mock_llm, mock_check, mock_prompt, base_state, stub_code):
        """Test error when LLM returns stub code with various stub markers."""
        initial_revision_count = base_state.get("code_revision_count", 0)
        mock_check.return_value = None
        mock_prompt.return_value = "System Prompt"
        mock_uc.return_value = "User Content"
        mock_llm.return_value = {"code": stub_code}
    
        result = code_generator_node(base_state)
    
        # Strict assertions - verify stub detection
        assert result["workflow_phase"] == "code_generation"
        expected_count = min(initial_revision_count + 1, base_state["runtime_config"]["max_code_revisions"])
>       assert result["code_revision_count"] == expected_count
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'code_revision_count'

tests/agents/code/test_code_generator.py:534: KeyError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=False, empty=True). Code generation must produce valid simulation code.
__ TestCodeGeneratorNode.test_generator_stub_code_output[would be generated] ___

self = <tests.agents.code.test_code_generator.TestCodeGeneratorNode object at 0x127e78be0>
mock_uc = <MagicMock name='build_user_content_for_code_generator' id='5192323760'>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5192322416'>
mock_check = <MagicMock name='check_context_or_escalate' id='5192319056'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192329472'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 0, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}
stub_code = 'would be generated'

    @pytest.mark.parametrize("stub_code", [
        "# TODO: Implement simulation",
        "STUB: code here",
        "PLACEHOLDER: add code",
        "# Replace this with actual code",
        "would be generated",
        "TODO: fix this",
    ])
    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.check_context_or_escalate")
    @patch("src.agents.code.call_agent_with_metrics")
    @patch("src.agents.code.build_user_content_for_code_generator")
    def test_generator_stub_code_output(self, mock_uc, mock_llm, mock_check, mock_prompt, base_state, stub_code):
        """Test error when LLM returns stub code with various stub markers."""
        initial_revision_count = base_state.get("code_revision_count", 0)
        mock_check.return_value = None
        mock_prompt.return_value = "System Prompt"
        mock_uc.return_value = "User Content"
        mock_llm.return_value = {"code": stub_code}
    
        result = code_generator_node(base_state)
    
        # Strict assertions - verify stub detection
        assert result["workflow_phase"] == "code_generation"
        expected_count = min(initial_revision_count + 1, base_state["runtime_config"]["max_code_revisions"])
>       assert result["code_revision_count"] == expected_count
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'code_revision_count'

tests/agents/code/test_code_generator.py:534: KeyError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=False, empty=True). Code generation must produce valid simulation code.
____ TestCodeGeneratorNode.test_generator_stub_code_output[TODO: fix this] _____

self = <tests.agents.code.test_code_generator.TestCodeGeneratorNode object at 0x127e78c30>
mock_uc = <MagicMock name='build_user_content_for_code_generator' id='5192330144'>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5192323088'>
mock_check = <MagicMock name='check_context_or_escalate' id='5192320064'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192320736'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 0, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}
stub_code = 'TODO: fix this'

    @pytest.mark.parametrize("stub_code", [
        "# TODO: Implement simulation",
        "STUB: code here",
        "PLACEHOLDER: add code",
        "# Replace this with actual code",
        "would be generated",
        "TODO: fix this",
    ])
    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.check_context_or_escalate")
    @patch("src.agents.code.call_agent_with_metrics")
    @patch("src.agents.code.build_user_content_for_code_generator")
    def test_generator_stub_code_output(self, mock_uc, mock_llm, mock_check, mock_prompt, base_state, stub_code):
        """Test error when LLM returns stub code with various stub markers."""
        initial_revision_count = base_state.get("code_revision_count", 0)
        mock_check.return_value = None
        mock_prompt.return_value = "System Prompt"
        mock_uc.return_value = "User Content"
        mock_llm.return_value = {"code": stub_code}
    
        result = code_generator_node(base_state)
    
        # Strict assertions - verify stub detection
        assert result["workflow_phase"] == "code_generation"
        expected_count = min(initial_revision_count + 1, base_state["runtime_config"]["max_code_revisions"])
>       assert result["code_revision_count"] == expected_count
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'code_revision_count'

tests/agents/code/test_code_generator.py:534: KeyError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=True, empty=True). Code generation must produce valid simulation code.
____ TestCodeGeneratorNode.test_generator_stub_code_respects_max_revisions _____

self = <tests.agents.code.test_code_generator.TestCodeGeneratorNode object at 0x127e78eb0>
mock_uc = <MagicMock name='build_user_content_for_code_generator' id='5192332496'>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5192332160'>
mock_check = <MagicMock name='check_context_or_escalate' id='5192330816'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192330480'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 3, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}

    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.check_context_or_escalate")
    @patch("src.agents.code.call_agent_with_metrics")
    @patch("src.agents.code.build_user_content_for_code_generator")
    def test_generator_stub_code_respects_max_revisions(self, mock_uc, mock_llm, mock_check, mock_prompt, base_state):
        """Test that code revision count respects max limit even when stub code is generated."""
        mock_check.return_value = None
        mock_prompt.return_value = "System Prompt"
        mock_uc.return_value = "User Content"
        mock_llm.return_value = {"code": "# TODO: Implement simulation"}
    
        # Set current revisions to max
        max_revs = base_state["runtime_config"]["max_code_revisions"]
        base_state["code_revision_count"] = max_revs
    
        result = code_generator_node(base_state)
    
        # Strict assertions - verify max cap is respected
        assert result["workflow_phase"] == "code_generation"
>       assert result["code_revision_count"] == max_revs
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'code_revision_count'

tests/agents/code/test_code_generator.py:593: KeyError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=True, empty=True). Code generation must produce valid simulation code.
_ TestCodeGeneratorNode.test_generator_stub_code_respects_custom_max_revisions _

self = <tests.agents.code.test_code_generator.TestCodeGeneratorNode object at 0x127e78fa0>
mock_uc = <MagicMock name='build_user_content_for_code_generator' id='5192332832'>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5192334176'>
mock_check = <MagicMock name='check_context_or_escalate' id='5192334848'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192334512'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 7, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}

    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.check_context_or_escalate")
    @patch("src.agents.code.call_agent_with_metrics")
    @patch("src.agents.code.build_user_content_for_code_generator")
    def test_generator_stub_code_respects_custom_max_revisions(self, mock_uc, mock_llm, mock_check, mock_prompt, base_state):
        """Test that code revision count respects custom runtime_config max_code_revisions."""
        mock_check.return_value = None
        mock_prompt.return_value = "System Prompt"
        mock_uc.return_value = "User Content"
        mock_llm.return_value = {"code": "# TODO: Implement simulation"}
    
        # Set custom max in runtime_config
        custom_max = 7
        base_state["runtime_config"]["max_code_revisions"] = custom_max
        base_state["code_revision_count"] = custom_max
    
        result = code_generator_node(base_state)
    
        # Should respect runtime_config max, not just default MAX_CODE_REVISIONS
>       assert result["code_revision_count"] == custom_max
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'code_revision_count'

tests/agents/code/test_code_generator.py:617: KeyError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=True, empty=True). Code generation must produce valid simulation code.
_ TestCodeGeneratorNode.test_generator_short_code_without_stub_markers[print('short run')] _

self = <tests.agents.code.test_code_generator.TestCodeGeneratorNode object at 0x127e792c0>
mock_check = <MagicMock name='check_context_or_escalate' id='5191660080'>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5191647648'>
mock_user_content = <MagicMock name='build_user_content_for_code_generator' id='5191647312'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5191648656'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 0, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}
short_code = "print('short run')"

    @pytest.mark.parametrize("short_code", [
        "print('short run')",  # 18 chars
        "x = 1",  # 5 chars
        "a",  # 1 char
        "   ",  # 3 chars whitespace
        "\n\n",  # 2 chars newlines
        "A" * 49,  # 49 chars (just under 50)
    ])
    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.build_user_content_for_code_generator")
    @patch("src.agents.code.call_agent_with_metrics")
    @patch("src.agents.code.check_context_or_escalate")
    def test_generator_short_code_without_stub_markers(self, mock_check, mock_llm, mock_user_content, mock_prompt, base_state, short_code):
        """Short code lacking stub markers should still trigger revision path."""
        initial_revision_count = base_state.get("code_revision_count", 0)
        mock_check.return_value = None
        mock_prompt.return_value = "System Prompt"
        mock_user_content.return_value = "User Content"
        mock_llm.return_value = {"code": short_code}
    
        result = code_generator_node(base_state)
    
        # Strict assertions - verify short code detection
        assert result["workflow_phase"] == "code_generation"
        expected_count = min(initial_revision_count + 1, base_state["runtime_config"]["max_code_revisions"])
>       assert result["code_revision_count"] == expected_count
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'code_revision_count'

tests/agents/code/test_code_generator.py:746: KeyError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=False, empty=True). Code generation must produce valid simulation code.
_ TestCodeGeneratorNode.test_generator_short_code_without_stub_markers[x = 1] __

self = <tests.agents.code.test_code_generator.TestCodeGeneratorNode object at 0x127e79130>
mock_check = <MagicMock name='check_context_or_escalate' id='5142774512'>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5145097760'>
mock_user_content = <MagicMock name='build_user_content_for_code_generator' id='5185627408'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5104225648'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 0, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}
short_code = 'x = 1'

    @pytest.mark.parametrize("short_code", [
        "print('short run')",  # 18 chars
        "x = 1",  # 5 chars
        "a",  # 1 char
        "   ",  # 3 chars whitespace
        "\n\n",  # 2 chars newlines
        "A" * 49,  # 49 chars (just under 50)
    ])
    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.build_user_content_for_code_generator")
    @patch("src.agents.code.call_agent_with_metrics")
    @patch("src.agents.code.check_context_or_escalate")
    def test_generator_short_code_without_stub_markers(self, mock_check, mock_llm, mock_user_content, mock_prompt, base_state, short_code):
        """Short code lacking stub markers should still trigger revision path."""
        initial_revision_count = base_state.get("code_revision_count", 0)
        mock_check.return_value = None
        mock_prompt.return_value = "System Prompt"
        mock_user_content.return_value = "User Content"
        mock_llm.return_value = {"code": short_code}
    
        result = code_generator_node(base_state)
    
        # Strict assertions - verify short code detection
        assert result["workflow_phase"] == "code_generation"
        expected_count = min(initial_revision_count + 1, base_state["runtime_config"]["max_code_revisions"])
>       assert result["code_revision_count"] == expected_count
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'code_revision_count'

tests/agents/code/test_code_generator.py:746: KeyError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=False, empty=True). Code generation must produce valid simulation code.
___ TestCodeGeneratorNode.test_generator_short_code_without_stub_markers[a] ____

self = <tests.agents.code.test_code_generator.TestCodeGeneratorNode object at 0x127e79180>
mock_check = <MagicMock name='check_context_or_escalate' id='5133093584'>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5179013312'>
mock_user_content = <MagicMock name='build_user_content_for_code_generator' id='5179014320'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5179014656'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 0, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}
short_code = 'a'

    @pytest.mark.parametrize("short_code", [
        "print('short run')",  # 18 chars
        "x = 1",  # 5 chars
        "a",  # 1 char
        "   ",  # 3 chars whitespace
        "\n\n",  # 2 chars newlines
        "A" * 49,  # 49 chars (just under 50)
    ])
    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.build_user_content_for_code_generator")
    @patch("src.agents.code.call_agent_with_metrics")
    @patch("src.agents.code.check_context_or_escalate")
    def test_generator_short_code_without_stub_markers(self, mock_check, mock_llm, mock_user_content, mock_prompt, base_state, short_code):
        """Short code lacking stub markers should still trigger revision path."""
        initial_revision_count = base_state.get("code_revision_count", 0)
        mock_check.return_value = None
        mock_prompt.return_value = "System Prompt"
        mock_user_content.return_value = "User Content"
        mock_llm.return_value = {"code": short_code}
    
        result = code_generator_node(base_state)
    
        # Strict assertions - verify short code detection
        assert result["workflow_phase"] == "code_generation"
        expected_count = min(initial_revision_count + 1, base_state["runtime_config"]["max_code_revisions"])
>       assert result["code_revision_count"] == expected_count
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'code_revision_count'

tests/agents/code/test_code_generator.py:746: KeyError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=False, empty=True). Code generation must produce valid simulation code.
__ TestCodeGeneratorNode.test_generator_short_code_without_stub_markers[   ] ___

self = <tests.agents.code.test_code_generator.TestCodeGeneratorNode object at 0x127e791d0>
mock_check = <MagicMock name='check_context_or_escalate' id='5192328800'>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5192328464'>
mock_user_content = <MagicMock name='build_user_content_for_code_generator' id='5192321072'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192322752'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 0, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}
short_code = '   '

    @pytest.mark.parametrize("short_code", [
        "print('short run')",  # 18 chars
        "x = 1",  # 5 chars
        "a",  # 1 char
        "   ",  # 3 chars whitespace
        "\n\n",  # 2 chars newlines
        "A" * 49,  # 49 chars (just under 50)
    ])
    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.build_user_content_for_code_generator")
    @patch("src.agents.code.call_agent_with_metrics")
    @patch("src.agents.code.check_context_or_escalate")
    def test_generator_short_code_without_stub_markers(self, mock_check, mock_llm, mock_user_content, mock_prompt, base_state, short_code):
        """Short code lacking stub markers should still trigger revision path."""
        initial_revision_count = base_state.get("code_revision_count", 0)
        mock_check.return_value = None
        mock_prompt.return_value = "System Prompt"
        mock_user_content.return_value = "User Content"
        mock_llm.return_value = {"code": short_code}
    
        result = code_generator_node(base_state)
    
        # Strict assertions - verify short code detection
        assert result["workflow_phase"] == "code_generation"
        expected_count = min(initial_revision_count + 1, base_state["runtime_config"]["max_code_revisions"])
>       assert result["code_revision_count"] == expected_count
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'code_revision_count'

tests/agents/code/test_code_generator.py:746: KeyError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=False, empty=True). Code generation must produce valid simulation code.
__ TestCodeGeneratorNode.test_generator_short_code_without_stub_markers[\n\n] __

self = <tests.agents.code.test_code_generator.TestCodeGeneratorNode object at 0x127e79220>
mock_check = <MagicMock name='check_context_or_escalate' id='5192333840'>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5192333504'>
mock_user_content = <MagicMock name='build_user_content_for_code_generator' id='5192329808'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192329136'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 0, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}
short_code = '\n\n'

    @pytest.mark.parametrize("short_code", [
        "print('short run')",  # 18 chars
        "x = 1",  # 5 chars
        "a",  # 1 char
        "   ",  # 3 chars whitespace
        "\n\n",  # 2 chars newlines
        "A" * 49,  # 49 chars (just under 50)
    ])
    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.build_user_content_for_code_generator")
    @patch("src.agents.code.call_agent_with_metrics")
    @patch("src.agents.code.check_context_or_escalate")
    def test_generator_short_code_without_stub_markers(self, mock_check, mock_llm, mock_user_content, mock_prompt, base_state, short_code):
        """Short code lacking stub markers should still trigger revision path."""
        initial_revision_count = base_state.get("code_revision_count", 0)
        mock_check.return_value = None
        mock_prompt.return_value = "System Prompt"
        mock_user_content.return_value = "User Content"
        mock_llm.return_value = {"code": short_code}
    
        result = code_generator_node(base_state)
    
        # Strict assertions - verify short code detection
        assert result["workflow_phase"] == "code_generation"
        expected_count = min(initial_revision_count + 1, base_state["runtime_config"]["max_code_revisions"])
>       assert result["code_revision_count"] == expected_count
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'code_revision_count'

tests/agents/code/test_code_generator.py:746: KeyError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=False, empty=True). Code generation must produce valid simulation code.
_ TestCodeGeneratorNode.test_generator_short_code_without_stub_markers[AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA] _

self = <tests.agents.code.test_code_generator.TestCodeGeneratorNode object at 0x127e79270>
mock_check = <MagicMock name='check_context_or_escalate' id='5192334512'>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5192334848'>
mock_user_content = <MagicMock name='build_user_content_for_code_generator' id='5192334176'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192332832'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 0, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}
short_code = 'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA'

    @pytest.mark.parametrize("short_code", [
        "print('short run')",  # 18 chars
        "x = 1",  # 5 chars
        "a",  # 1 char
        "   ",  # 3 chars whitespace
        "\n\n",  # 2 chars newlines
        "A" * 49,  # 49 chars (just under 50)
    ])
    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.build_user_content_for_code_generator")
    @patch("src.agents.code.call_agent_with_metrics")
    @patch("src.agents.code.check_context_or_escalate")
    def test_generator_short_code_without_stub_markers(self, mock_check, mock_llm, mock_user_content, mock_prompt, base_state, short_code):
        """Short code lacking stub markers should still trigger revision path."""
        initial_revision_count = base_state.get("code_revision_count", 0)
        mock_check.return_value = None
        mock_prompt.return_value = "System Prompt"
        mock_user_content.return_value = "User Content"
        mock_llm.return_value = {"code": short_code}
    
        result = code_generator_node(base_state)
    
        # Strict assertions - verify short code detection
        assert result["workflow_phase"] == "code_generation"
        expected_count = min(initial_revision_count + 1, base_state["runtime_config"]["max_code_revisions"])
>       assert result["code_revision_count"] == expected_count
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'code_revision_count'

tests/agents/code/test_code_generator.py:746: KeyError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=False, empty=True). Code generation must produce valid simulation code.
____ TestCodeGeneratorNode.test_generator_code_extraction_empty_code_string ____

self = <tests.agents.code.test_code_generator.TestCodeGeneratorNode object at 0x127e79450>
mock_uc = <MagicMock name='build_user_content_for_code_generator' id='5192331488'>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5192319392'>
mock_check = <MagicMock name='check_context_or_escalate' id='5192319728'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192321408'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 0, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}

    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.check_context_or_escalate")
    @patch("src.agents.code.call_agent_with_metrics")
    @patch("src.agents.code.build_user_content_for_code_generator")
    def test_generator_code_extraction_empty_code_string(self, mock_uc, mock_llm, mock_check, mock_prompt, base_state):
        """Test code extraction when LLM returns empty code string."""
        initial_revision_count = base_state.get("code_revision_count", 0)
        mock_check.return_value = None
        mock_prompt.return_value = "System Prompt"
        mock_uc.return_value = "User Content"
        mock_llm.return_value = {"code": "", "expected_outputs": []}
    
        result = code_generator_node(base_state)
    
        # Empty code should trigger stub detection
        assert result["workflow_phase"] == "code_generation"
        assert result["code"] == ""
        expected_count = min(initial_revision_count + 1, base_state["runtime_config"]["max_code_revisions"])
>       assert result["code_revision_count"] == expected_count
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'code_revision_count'

tests/agents/code/test_code_generator.py:863: KeyError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=False, empty=True). Code generation must produce valid simulation code.
__ TestCodeGeneratorNode.test_generator_code_extraction_whitespace_only_code ___

self = <tests.agents.code.test_code_generator.TestCodeGeneratorNode object at 0x127e794a0>
mock_uc = <MagicMock name='build_user_content_for_code_generator' id='5192328128'>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5192324432'>
mock_check = <MagicMock name='check_context_or_escalate' id='5192323760'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192322416'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 0, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}

    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.check_context_or_escalate")
    @patch("src.agents.code.call_agent_with_metrics")
    @patch("src.agents.code.build_user_content_for_code_generator")
    def test_generator_code_extraction_whitespace_only_code(self, mock_uc, mock_llm, mock_check, mock_prompt, base_state):
        """Test code extraction when LLM returns whitespace-only code."""
        initial_revision_count = base_state.get("code_revision_count", 0)
        whitespace_code = "   \n\t  \n   "
        mock_check.return_value = None
        mock_prompt.return_value = "System Prompt"
        mock_uc.return_value = "User Content"
        mock_llm.return_value = {"code": whitespace_code, "expected_outputs": []}
    
        result = code_generator_node(base_state)
    
        # Whitespace-only code should trigger stub detection (after strip)
        assert result["workflow_phase"] == "code_generation"
        assert result["code"] == whitespace_code
        expected_count = min(initial_revision_count + 1, base_state["runtime_config"]["max_code_revisions"])
>       assert result["code_revision_count"] == expected_count
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'code_revision_count'

tests/agents/code/test_code_generator.py:885: KeyError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=False, empty=True). Code generation must produce valid simulation code.
_____ TestCodeGeneratorNode.test_generator_stub_detection_case_insensitive _____

self = <tests.agents.code.test_code_generator.TestCodeGeneratorNode object at 0x127e79630>
mock_uc = <MagicMock name='build_user_content_for_code_generator' id='5192324768'>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5192324096'>
mock_check = <MagicMock name='check_context_or_escalate' id='5192323424'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192322080'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 0, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}

    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.check_context_or_escalate")
    @patch("src.agents.code.call_agent_with_metrics")
    @patch("src.agents.code.build_user_content_for_code_generator")
    def test_generator_stub_detection_case_insensitive(self, mock_uc, mock_llm, mock_check, mock_prompt, base_state):
        """Test that stub detection is case-insensitive."""
        initial_revision_count = base_state.get("code_revision_count", 0)
        mock_check.return_value = None
        mock_prompt.return_value = "System Prompt"
        mock_uc.return_value = "User Content"
        # Test various case combinations
        test_cases = [
            "# todo: implement",
            "# TODO: implement",
            "# Todo: implement",
            "stub: code here",
            "STUB: code here",
            "Stub: code here",
            "placeholder text",
            "PLACEHOLDER text",
        ]
    
        for stub_code in test_cases:
            mock_llm.return_value = {"code": stub_code}
            result = code_generator_node(base_state)
    
            assert result["workflow_phase"] == "code_generation"
            expected_count = min(initial_revision_count + 1, base_state["runtime_config"]["max_code_revisions"])
>           assert result["code_revision_count"] == expected_count
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           KeyError: 'code_revision_count'

tests/agents/code/test_code_generator.py:1025: KeyError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=True, empty=True). Code generation must produce valid simulation code.
______________ TestCodeReviewerNode.test_reviewer_needs_revision _______________

self = <tests.agents.code.test_code_reviewer.TestCodeReviewerNode object at 0x127ed03e0>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5192320064'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192320736'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 0, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}

    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.call_agent_with_metrics")
    def test_reviewer_needs_revision(self, mock_llm, mock_prompt, base_state):
        """Test reviewer requesting revision."""
        mock_prompt.return_value = "Prompt"
        mock_llm.return_value = {
            "verdict": "needs_revision",
            "feedback": "Fix boundary conditions",
            "issues": ["Boundary issue"]
        }
    
        initial_revision_count = base_state.get("code_revision_count", 0)
        result = code_reviewer_node(base_state)
    
        # Verify all required fields
        assert result["workflow_phase"] == "code_review"
        assert result["last_code_review_verdict"] == "needs_revision"
        assert result["code_revision_count"] == initial_revision_count + 1
>       assert result["reviewer_feedback"] == "Fix boundary conditions"
E       AssertionError: assert 'Missing verdict in review' == 'Fix boundary conditions'
E         
E         - Fix boundary conditions
E         + Missing verdict in review

tests/agents/code/test_code_reviewer.py:119: AssertionError
_______ TestCodeReviewerNode.test_reviewer_neither_feedback_nor_summary ________

self = <tests.agents.code.test_code_reviewer.TestCodeReviewerNode object at 0x127e9e450>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5133093584'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5104225648'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 0, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}

    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.call_agent_with_metrics")
    def test_reviewer_neither_feedback_nor_summary(self, mock_llm, mock_prompt, base_state):
        """Test reviewer when both feedback and summary are missing."""
        mock_prompt.return_value = "Prompt"
        mock_llm.return_value = {
            "verdict": "needs_revision",
            # No feedback, no summary
        }
    
        result = code_reviewer_node(base_state)
    
        # Should use fallback message
        assert "reviewer_feedback" in result
>       assert result["reviewer_feedback"] == "Missing verdict or feedback in review"
E       AssertionError: assert 'Missing verdict in review' == 'Missing verd...ack in review'
E         
E         - Missing verdict or feedback in review
E         ?                 ------------
E         + Missing verdict in review

tests/agents/code/test_code_reviewer.py:162: AssertionError
_______________ TestCodeReviewerNode.test_reviewer_max_revisions _______________

self = <tests.agents.code.test_code_reviewer.TestCodeReviewerNode object at 0x127df2690>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5179014320'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5179014656'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 3, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}

    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.call_agent_with_metrics")
    def test_reviewer_max_revisions(self, mock_llm, mock_prompt, base_state):
        """Test reviewer hitting max revisions triggers escalation."""
        mock_prompt.return_value = "Prompt"
        mock_llm.return_value = {"verdict": "needs_revision", "feedback": "Fix it"}
    
        base_state["code_revision_count"] = MAX_CODE_REVISIONS
    
        result = code_reviewer_node(base_state)
    
        # Should not increment past max
        assert result["code_revision_count"] == MAX_CODE_REVISIONS
        assert result["last_code_review_verdict"] == "needs_revision"
>       assert result["reviewer_feedback"] == "Fix it"
E       AssertionError: assert 'Missing verdict in review' == 'Fix it'
E         
E         - Fix it
E         + Missing verdict in review

tests/agents/code/test_code_reviewer.py:180: AssertionError
____________ TestCodeReviewerNode.test_reviewer_missing_verdict_key ____________

self = <tests.agents.code.test_code_reviewer.TestCodeReviewerNode object at 0x127ee9810>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5192330144'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192319056'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 0, 'current_stage_id': 'stage_1_sim', 'current_stage_type': 'SINGLE_STRUCTURE', ...}

    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.call_agent_with_metrics")
    def test_reviewer_missing_verdict_key(self, mock_llm, mock_prompt, base_state):
        """Test handling when LLM response is missing 'verdict' key."""
        mock_prompt.return_value = "Prompt"
        mock_llm.return_value = {
            "issues": [],
            "feedback": "Missing verdict"
        }
    
        # Code uses .get("verdict", "needs_revision") so should default to needs_revision
        result = code_reviewer_node(base_state)
    
        # Should default to needs_revision when verdict is missing
        assert result["last_code_review_verdict"] == "needs_revision"
        assert result["code_revision_count"] == 1
>       assert result["reviewer_feedback"] == "Missing verdict"
E       AssertionError: assert 'Missing verdict in review' == 'Missing verdict'
E         
E         - Missing verdict
E         + Missing verdict in review
E         ?                ++++++++++

tests/agents/code/test_code_reviewer.py:444: AssertionError
_________ TestCodeReviewerNode.test_reviewer_escalation_message_format _________

self = <tests.agents.code.test_code_reviewer.TestCodeReviewerNode object at 0x127ef08c0>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5192323088'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192320400'>
base_state = {'code': 'import meep as mp\n# Valid simulation code structure\n# ... more lines ...', 'code_revision_count': 3, 'current_stage_id': 'stage_5', 'current_stage_type': 'SINGLE_STRUCTURE', ...}

    @patch("src.agents.code.build_agent_prompt")
    @patch("src.agents.code.call_agent_with_metrics")
    def test_reviewer_escalation_message_format(self, mock_llm, mock_prompt, base_state):
        """Test that escalation message has correct format when max revisions hit."""
        mock_prompt.return_value = "Prompt"
        mock_llm.return_value = {
            "verdict": "needs_revision",
            "feedback": "Complex feedback with\nmultiple lines\nand details"
        }
    
        base_state["code_revision_count"] = MAX_CODE_REVISIONS
        base_state["current_stage_id"] = "stage_5"
    
        result = code_reviewer_node(base_state)
    
        question = result["pending_user_questions"][0]
    
        # Verify all required elements
        assert "Code review limit reached" in question
        assert "Stage: stage_5" in question
        assert f"Attempts: {MAX_CODE_REVISIONS}" in question
>       assert "Complex feedback with" in question
E       AssertionError: assert 'Complex feedback with' in 'Code review limit reached.\n\n- Stage: stage_5\n- Attempts: 3/3\n- Latest reviewer feedback:\n  Missing verdict in re...HINT: Reset counter and retry with your guidance\n- SKIP_STAGE: Skip this stage and continue\n- STOP: End the workflow'

tests/agents/code/test_code_reviewer.py:701: AssertionError
_____________ TestDesignReviewerNode.test_reviewer_needs_revision ______________

self = <tests.agents.design.test_design_reviewer.TestDesignReviewerNode object at 0x127ed0770>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5145097760'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192329472'>
mock_check = <MagicMock name='check_context_or_escalate' id='5192324096'>
base_state = {'assumptions': {'global_assumptions': [{'description': 'Existing assumption', 'id': 'existing_1'}]}, 'current_stage_id': 'stage_1_sim', 'design_revision_count': 0, 'paper_domain': 'nanophotonics', ...}

    @patch("src.agents.base.check_context_or_escalate")
    @patch("src.agents.design.build_agent_prompt")
    @patch("src.agents.design.call_agent_with_metrics")
    def test_reviewer_needs_revision(self, mock_llm, mock_prompt, mock_check, base_state):
        """Test reviewer requesting revision."""
        mock_check.return_value = None
        mock_prompt.return_value = "Prompt"
        initial_count = base_state.get("design_revision_count", 0)
        mock_llm.return_value = {
            "verdict": "needs_revision",
            "feedback": "Add more details",
            "issues": ["Missing parameters"]
        }
    
        result = design_reviewer_node(base_state)
    
        # Verify exact verdict
        assert result["last_design_review_verdict"] == "needs_revision"
        # Verify revision count was incremented
        assert result["design_revision_count"] == initial_count + 1
        # Verify feedback is exactly as provided
>       assert result["reviewer_feedback"] == "Add more details"
E       AssertionError: assert '' == 'Add more details'
E         
E         - Add more details

tests/agents/design/test_design_reviewer.py:169: AssertionError
______________ TestDesignReviewerNode.test_reviewer_max_revisions ______________

self = <tests.agents.design.test_design_reviewer.TestDesignReviewerNode object at 0x127ebc290>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5192323760'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192323424'>
mock_check = <MagicMock name='check_context_or_escalate' id='5192322416'>
base_state = {'assumptions': {'global_assumptions': [{'description': 'Existing assumption', 'id': 'existing_1'}]}, 'current_stage_id': 'stage_1_sim', 'design_revision_count': 3, 'paper_domain': 'nanophotonics', ...}

    @patch("src.agents.base.check_context_or_escalate")
    @patch("src.agents.design.build_agent_prompt")
    @patch("src.agents.design.call_agent_with_metrics")
    def test_reviewer_max_revisions(self, mock_llm, mock_prompt, mock_check, base_state):
        """Test reviewer hitting max revisions."""
        mock_check.return_value = None
        mock_prompt.return_value = "Prompt"
        mock_llm.return_value = {"verdict": "needs_revision", "feedback": "more"}
    
        max_revisions = base_state.get("runtime_config", {}).get("max_design_revisions", MAX_DESIGN_REVISIONS)
        base_state["design_revision_count"] = max_revisions  # Already at max
    
        result = design_reviewer_node(base_state)
    
        # Should not increment count beyond max, but should capture feedback
        assert result["design_revision_count"] == max_revisions
        assert result["last_design_review_verdict"] == "needs_revision"
        # Verify feedback is exactly as provided
>       assert result["reviewer_feedback"] == "more"
E       AssertionError: assert '' == 'more'
E         
E         - more

tests/agents/design/test_design_reviewer.py:195: AssertionError
_ TestDesignReviewerNode.test_reviewer_missing_verdict_defaults_to_needs_revision _

self = <tests.agents.design.test_design_reviewer.TestDesignReviewerNode object at 0x127df2e00>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5192324432'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192319728'>
mock_check = <MagicMock name='check_context_or_escalate' id='5192334512'>
base_state = {'assumptions': {'global_assumptions': [{'description': 'Existing assumption', 'id': 'existing_1'}]}, 'current_stage_id': 'stage_1_sim', 'design_revision_count': 0, 'paper_domain': 'nanophotonics', ...}

    @patch("src.agents.base.check_context_or_escalate")
    @patch("src.agents.design.build_agent_prompt")
    @patch("src.agents.design.call_agent_with_metrics")
    def test_reviewer_missing_verdict_defaults_to_needs_revision(self, mock_llm, mock_prompt, mock_check, base_state):
        """Test handling when LLM returns JSON without 'verdict' (fail-closed safety)."""
        mock_check.return_value = None
        mock_prompt.return_value = "Prompt"
        mock_llm.return_value = {"feedback": "Some feedback but no verdict"}
        initial_count = base_state.get("design_revision_count", 0)
    
        result = design_reviewer_node(base_state)
    
        # Fail-closed: missing verdict should default to needs_revision (safer)
        assert result["last_design_review_verdict"] == "needs_revision"
        # Verify issues default to empty list (or from output if present)
        issues = result.get("reviewer_issues", [])
        assert isinstance(issues, list)
        # Verify revision count IS incremented for needs_revision
        assert result["design_revision_count"] == initial_count + 1
        # Verify workflow phase is set
        assert result["workflow_phase"] == "design_review"
        # Verify feedback IS set for needs_revision (uses feedback from LLM response)
>       assert result["reviewer_feedback"] == "Some feedback but no verdict"
E       AssertionError: assert '' == 'Some feedback but no verdict'
E         
E         - Some feedback but no verdict

tests/agents/design/test_design_reviewer.py:250: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  src.agents.design:design.py:222 Design reviewer output missing 'verdict'. Defaulting to 'needs_revision'.
_________ TestAdaptPromptsNode.test_adapt_prompts_success_with_domain __________

self = <tests.agents.planning.test_adapt_prompts.TestAdaptPromptsNode object at 0x1303ca710>
mock_build_prompt = <MagicMock name='build_agent_prompt' id='5185627408'>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5145097760'>

    @patch("src.agents.planning.call_agent_with_metrics")
    @patch("src.agents.planning.build_agent_prompt")
    def test_adapt_prompts_success_with_domain(self, mock_build_prompt, mock_llm):
        """Test successful adaptation with paper_domain update."""
        mock_build_prompt.return_value = "system prompt"
        mock_llm.return_value = {
            "prompt_modifications": ["a1", "a2"],
            "paper_domain": "new_domain"
        }
    
        state = {"paper_text": "some paper text", "paper_domain": "old_domain"}
        result = adapt_prompts_node(state)
    
        # Verify return values
        assert result["prompt_adaptations"] == ["a1", "a2"]
>       assert result["paper_domain"] == "new_domain"
               ^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'paper_domain'

tests/agents/planning/test_adapt_prompts.py:29: KeyError
__________ TestAdaptPromptsNode.test_adapt_prompts_state_not_mutated ___________

self = <tests.agents.planning.test_adapt_prompts.TestAdaptPromptsNode object at 0x1303f72f0>
mock_build_prompt = <MagicMock name='build_agent_prompt' id='5192331488'>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5192330144'>

    @patch("src.agents.planning.call_agent_with_metrics")
    @patch("src.agents.planning.build_agent_prompt")
    def test_adapt_prompts_state_not_mutated(self, mock_build_prompt, mock_llm):
        """Test that original state is not mutated."""
        mock_build_prompt.return_value = "system prompt"
        mock_llm.return_value = {
            "prompt_modifications": ["a1"],
            "paper_domain": "new_domain"
        }
    
        original_state = {"paper_text": "text", "paper_domain": "old_domain"}
        state_copy = original_state.copy()
        result = adapt_prompts_node(state_copy)
    
        # Verify state was not mutated
        assert state_copy == original_state
        assert state_copy["paper_domain"] == "old_domain"
        assert "prompt_adaptations" not in state_copy
    
        # Verify result is separate
>       assert result["paper_domain"] == "new_domain"
               ^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'paper_domain'

tests/agents/planning/test_adapt_prompts.py:365: KeyError
_______________ TestPlanReviewerNode.test_reviewer_llm_rejection _______________

self = <tests.agents.planning.test_plan_reviewer.TestPlanReviewerNode object at 0x13045bac0>
mock_llm = <MagicMock name='call_agent_with_metrics' id='5192319728'>
mock_validate = <MagicMock name='validate_state_or_warn' id='5192329808'>

    @patch("src.agents.planning.validate_state_or_warn")
    @patch("src.agents.planning.call_agent_with_metrics")
    def test_reviewer_llm_rejection(self, mock_llm, mock_validate):
        """Test handling of LLM rejection."""
        mock_validate.return_value = []
        mock_llm.return_value = {
            "verdict": "needs_revision",
            "feedback": "Please improve X"
        }
        state = {
            "plan": {"stages": [{"stage_id": "s1", "targets": ["t"]}]},
            "replan_count": 0
        }
    
        result = plan_reviewer_node(state)
    
        assert result["workflow_phase"] == "plan_review"
        assert result["last_plan_review_verdict"] == "needs_revision"
>       assert result["planner_feedback"] == "Please improve X"
E       AssertionError: assert '' == 'Please improve X'
E         
E         - Please improve X

tests/agents/planning/test_plan_reviewer.py:550: AssertionError
____________ TestDeadlockTrigger.test_asks_clarification_on_unclear ____________

self = <tests.agents.supervision.test_supervisor_deadlock_misc.TestDeadlockTrigger object at 0x1305fbe30>
mock_call = <MagicMock name='call_agent_with_metrics' id='5192325776'>
mock_context = <MagicMock name='check_context_or_escalate' id='5192326112'>

    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    @patch("src.agents.supervision.supervisor.call_agent_with_metrics")
    def test_asks_clarification_on_unclear(self, mock_call, mock_context):
        """Should ask clarification on unclear response."""
        mock_context.return_value = None
        state = {
            "ask_user_trigger": "deadlock_detected",
            "user_responses": {"Question": "I'm not sure"},
            "current_stage_id": "stage1",
            "progress": {"stages": [], "user_interactions": []},
        }
    
        result = supervisor_node(state)
    
        assert result["supervisor_verdict"] == "ask_user"
        assert "pending_user_questions" in result
        assert len(result["pending_user_questions"]) > 0
        assert any("GENERATE_REPORT" in q or "REPLAN" in q or "STOP" in q
                  for q in result["pending_user_questions"])
>       assert result["ask_user_trigger"] is None
E       AssertionError: assert 'deadlock_detected' is None

tests/agents/supervision/test_supervisor_deadlock_misc.py:119: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
____________ TestDeadlockTrigger.test_handles_empty_user_responses _____________

self = <tests.agents.supervision.test_supervisor_deadlock_misc.TestDeadlockTrigger object at 0x13055e550>
mock_call = <MagicMock name='call_agent_with_metrics' id='5192333840'>
mock_context = <MagicMock name='check_context_or_escalate' id='5192326448'>

    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    @patch("src.agents.supervision.supervisor.call_agent_with_metrics")
    def test_handles_empty_user_responses(self, mock_call, mock_context):
        """Should handle empty user responses gracefully."""
        mock_context.return_value = None
        state = {
            "ask_user_trigger": "deadlock_detected",
            "user_responses": {},
            "current_stage_id": "stage1",
            "progress": {"stages": [], "user_interactions": []},
        }
    
        result = supervisor_node(state)
    
        # Empty response should trigger clarification request
        assert result["supervisor_verdict"] == "ask_user"
        assert "pending_user_questions" in result
>       assert result["ask_user_trigger"] is None
E       AssertionError: assert 'deadlock_detected' is None

tests/agents/supervision/test_supervisor_deadlock_misc.py:197: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
____ TestDeadlockTriggerEdgeCases.test_handles_invalid_user_responses_type _____

self = <tests.agents.supervision.test_supervisor_deadlock_misc.TestDeadlockTriggerEdgeCases object at 0x1318782d0>
mock_call = <MagicMock name='call_agent_with_metrics' id='5192331488'>
mock_context = <MagicMock name='check_context_or_escalate' id='5192327792'>

    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    @patch("src.agents.supervision.supervisor.call_agent_with_metrics")
    def test_handles_invalid_user_responses_type(self, mock_call, mock_context):
        """Should handle invalid user_responses type gracefully."""
        mock_context.return_value = None
        state = {
            "ask_user_trigger": "deadlock_detected",
            "user_responses": "not a dict",  # Invalid type
            "current_stage_id": "stage1",
            "progress": {"stages": []},
        }
    
        # Should handle gracefully (supervisor_node logs warning and uses empty dict)
        result = supervisor_node(state)
    
        # Should still clear trigger
>       assert result["ask_user_trigger"] is None
E       AssertionError: assert 'deadlock_detected' is None

tests/agents/supervision/test_supervisor_deadlock_misc.py:449: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  src.agents.supervision.supervisor:supervisor.py:354 user_responses has invalid type <class 'str'>, expected dict. Defaulting to empty dict.
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
________ TestDeadlockTriggerEdgeCases.test_handles_none_user_responses _________

self = <tests.agents.supervision.test_supervisor_deadlock_misc.TestDeadlockTriggerEdgeCases object at 0x131878410>
mock_call = <MagicMock name='call_agent_with_metrics' id='5192332832'>
mock_context = <MagicMock name='check_context_or_escalate' id='5192334176'>

    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    @patch("src.agents.supervision.supervisor.call_agent_with_metrics")
    def test_handles_none_user_responses(self, mock_call, mock_context):
        """Should handle None user_responses."""
        mock_context.return_value = None
        state = {
            "ask_user_trigger": "deadlock_detected",
            "user_responses": None,
            "current_stage_id": "stage1",
            "progress": {"stages": []},
        }
    
        result = supervisor_node(state)
    
>       assert result["ask_user_trigger"] is None
E       AssertionError: assert 'deadlock_detected' is None

tests/agents/supervision/test_supervisor_deadlock_misc.py:467: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  src.agents.supervision.supervisor:supervisor.py:354 user_responses has invalid type <class 'NoneType'>, expected dict. Defaulting to empty dict.
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
_________ TestExecutionFailureLimitTrigger.test_resets_count_on_retry __________

self = <tests.agents.supervision.test_supervisor_limits.TestExecutionFailureLimitTrigger object at 0x131879590>
mock_context = <MagicMock name='check_context_or_escalate' id='5192334176'>

    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    def test_resets_count_on_retry(self, mock_context):
        """Should reset execution failure count on RETRY."""
        mock_context.return_value = None
        state = {
            "ask_user_trigger": "execution_failure_limit",
            "user_responses": {"Question": "RETRY with more memory"},
            "execution_failure_count": 3,
        }
    
        result = supervisor_node(state)
    
        assert result["execution_failure_count"] == 0
        assert result["supervisor_verdict"] == "retry_generate_code"  # Retry with guidance
        assert result["ask_user_trigger"] is None  # Should be cleared
        assert "supervisor_feedback" in result
>       assert "User guidance:" in result["supervisor_feedback"]
E       AssertionError: assert 'User guidance:' in 'User guidance applied to execution_feedback'

tests/agents/supervision/test_supervisor_limits.py:305: AssertionError
________ TestExecutionFailureLimitTrigger.test_resets_count_on_guidance ________

self = <tests.agents.supervision.test_supervisor_limits.TestExecutionFailureLimitTrigger object at 0x1318796d0>
mock_context = <MagicMock name='check_context_or_escalate' id='5145097760'>

    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    def test_resets_count_on_guidance(self, mock_context):
        """Should reset execution failure count on GUIDANCE."""
        mock_context.return_value = None
        state = {
            "ask_user_trigger": "execution_failure_limit",
            "user_responses": {"Question": "GUIDANCE: reduce resolution"},
            "execution_failure_count": 5,
        }
    
        result = supervisor_node(state)
    
        assert result["execution_failure_count"] == 0
        assert result["supervisor_verdict"] == "retry_generate_code"  # Retry with guidance
        assert result["ask_user_trigger"] is None  # Should be cleared
        assert "supervisor_feedback" in result
>       assert "User guidance:" in result["supervisor_feedback"]
E       AssertionError: assert 'User guidance:' in 'User guidance applied to execution_feedback'

tests/agents/supervision/test_supervisor_limits.py:324: AssertionError
__________ TestClarificationTrigger.test_continues_with_clarification __________

self = <tests.agents.supervision.test_supervisor_limits.TestClarificationTrigger object at 0x13187a210>
mock_context = <MagicMock name='check_context_or_escalate' id='5192329136'>

    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    def test_continues_with_clarification(self, mock_context):
        """Should continue workflow with user clarification."""
        mock_context.return_value = None
        state = {
            "ask_user_trigger": "clarification",
            "user_responses": {"Question": "The wavelength should be 550nm"},
            "progress": {"stages": [], "user_interactions": []},
        }
    
        result = supervisor_node(state)
    
        assert result["supervisor_verdict"] == "ok_continue"
        assert result["ask_user_trigger"] is None
        assert "supervisor_feedback" in result
        assert "User clarification" in result["supervisor_feedback"]
>       assert "550nm" in result["supervisor_feedback"]
E       AssertionError: assert '550nm' in 'User clarification recorded in user_context'

tests/agents/supervision/test_supervisor_limits.py:982: AssertionError
__________ TestClarificationTrigger.test_preserves_clarification_text __________

self = <tests.agents.supervision.test_supervisor_limits.TestClarificationTrigger object at 0x1318ec8a0>
mock_context = <MagicMock name='check_context_or_escalate' id='5179014656'>

    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    def test_preserves_clarification_text(self, mock_context):
        """Should preserve full clarification text in feedback."""
        mock_context.return_value = None
        clarification = "Use FDTD with PML boundaries and mesh size 10nm"
        state = {
            "ask_user_trigger": "clarification",
            "user_responses": {"Question": clarification},
            "progress": {"stages": [], "user_interactions": []},
        }
    
        result = supervisor_node(state)
    
        assert result["supervisor_verdict"] == "ok_continue"
>       assert clarification in result["supervisor_feedback"]
E       AssertionError: assert 'Use FDTD with PML boundaries and mesh size 10nm' in 'User clarification recorded in user_context'

tests/agents/supervision/test_supervisor_limits.py:1016: AssertionError
___________ TestUserResponseVariations.test_handles_unicode_response ___________

self = <tests.agents.supervision.test_supervisor_limits.TestUserResponseVariations object at 0x1318ed810>
mock_context = <MagicMock name='check_context_or_escalate' id='5192326448'>

    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    def test_handles_unicode_response(self, mock_context):
        """Should handle unicode characters in response."""
        mock_context.return_value = None
        state = {
            "ask_user_trigger": "clarification",
            "user_responses": {"Question": "Use =550nm for the wavelength"},
            "progress": {"stages": [], "user_interactions": []},
        }
    
        result = supervisor_node(state)
    
        assert result["supervisor_verdict"] == "ok_continue"
>       assert "=550nm" in result["supervisor_feedback"]
E       AssertionError: assert '=550nm' in 'User clarification recorded in user_context'

tests/agents/supervision/test_supervisor_limits.py:1717: AssertionError
_____________ TestMaterialCheckpointTrigger.test_handles_need_help _____________

self = <tests.agents.supervision.test_supervisor_material_checkpoint.TestMaterialCheckpointTrigger object at 0x1318ed940>
mock_context = <MagicMock name='check_context_or_escalate' id='5192326112'>

    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    def test_handles_need_help(self, mock_context):
        """Should ask for more details on NEED_HELP response."""
        mock_context.return_value = None
    
        state = {
            "ask_user_trigger": "material_checkpoint",
            "user_responses": {"Material question": "NEED_HELP"},
            "pending_validated_materials": [{"material_id": "gold"}],
        }
    
        result = supervisor_node(state)
    
        # Verify exact verdict
        assert result["supervisor_verdict"] == "ask_user"
        # Verify question is set
        assert len(result["pending_user_questions"]) == 1
        assert "details" in result["pending_user_questions"][0].lower()
        assert "material issue" in result["pending_user_questions"][0].lower()
        # Verify materials not cleared
        assert "pending_validated_materials" not in result or result.get("pending_validated_materials") == [{"material_id": "gold"}]
        # Verify trigger cleared
>       assert result.get("ask_user_trigger") is None
E       AssertionError: assert 'material_checkpoint' is None
E        +  where 'material_checkpoint' = <built-in method get of dict object at 0x136886100>('ask_user_trigger')
E        +    where <built-in method get of dict object at 0x136886100> = {'archive_errors': [], 'ask_user_trigger': 'material_checkpoint', 'awaiting_user_input': True, 'pending_user_questions': ['Please provide more details about the material issue.'], ...}.get

tests/agents/supervision/test_supervisor_material_checkpoint.py:135: AssertionError
_________ TestMaterialCheckpointTrigger.test_handles_unclear_response __________

self = <tests.agents.supervision.test_supervisor_material_checkpoint.TestMaterialCheckpointTrigger object at 0x131a0b530>
mock_context = <MagicMock name='check_context_or_escalate' id='5179013312'>

    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    def test_handles_unclear_response(self, mock_context):
        """Should ask for clarification on unclear response."""
        mock_context.return_value = None
    
        state = {
            "ask_user_trigger": "material_checkpoint",
            "user_responses": {"Material question": "maybe"},
            "pending_validated_materials": [{"material_id": "gold"}],
        }
    
        result = supervisor_node(state)
    
        # Verify exact verdict
        assert result["supervisor_verdict"] == "ask_user"
        # Verify question contains unclear message
        assert len(result["pending_user_questions"]) == 1
        assert "unclear" in result["pending_user_questions"][0].lower()
        # Response is uppercased, so check for uppercase version
        assert "MAYBE" in result["pending_user_questions"][0]
        # Verify trigger cleared
>       assert result.get("ask_user_trigger") is None
E       assert 'material_checkpoint' is None
E        +  where 'material_checkpoint' = <built-in method get of dict object at 0x136886f40>('ask_user_trigger')
E        +    where <built-in method get of dict object at 0x136886f40> = {'archive_errors': [], 'ask_user_trigger': 'material_checkpoint', 'awaiting_user_input': True, 'pending_user_questions...our response 'MAYBE' is unclear. Please clarify: APPROVE, CHANGE_DATABASE, CHANGE_MATERIAL, NEED_HELP, or STOP?"], ...}.get

tests/agents/supervision/test_supervisor_material_checkpoint.py:158: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
____ TestMaterialCheckpointTrigger.test_handles_rejection_without_specifics ____

self = <tests.agents.supervision.test_supervisor_material_checkpoint.TestMaterialCheckpointTrigger object at 0x1318f8af0>
mock_context = <MagicMock name='check_context_or_escalate' id='5133093584'>

    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    def test_handles_rejection_without_specifics(self, mock_context):
        """Should ask for clarification on rejection without specifying what to change."""
        mock_context.return_value = None
    
        state = {
            "ask_user_trigger": "material_checkpoint",
            "user_responses": {"Material question": "NO"},
            "pending_validated_materials": [{"material_id": "gold"}],
        }
    
        result = supervisor_node(state)
    
        # Verify exact verdict
        assert result["supervisor_verdict"] == "ask_user"
        # Verify question asks for clarification
        assert len(result["pending_user_questions"]) == 1
        assert "didn't specify" in result["pending_user_questions"][0].lower() or \
               "specify" in result["pending_user_questions"][0].lower()
        # Verify trigger cleared
>       assert result.get("ask_user_trigger") is None
E       assert 'material_checkpoint' is None
E        +  where 'material_checkpoint' = <built-in method get of dict object at 0x135ccc780>('ask_user_trigger')
E        +    where <built-in method get of dict object at 0x135ccc780> = {'archive_errors': [], 'ask_user_trigger': 'material_checkpoint', 'awaiting_user_input': True, 'pending_user_questions...t didn't specify what to change. Please clarify: APPROVE, CHANGE_DATABASE, CHANGE_MATERIAL, NEED_HELP, or STOP?"], ...}.get

tests/agents/supervision/test_supervisor_material_checkpoint.py:180: AssertionError
_ TestMaterialCheckpointTrigger.test_handles_missing_pending_materials_on_approve _

self = <tests.agents.supervision.test_supervisor_material_checkpoint.TestMaterialCheckpointTrigger object at 0x1318f8c00>
mock_context = <MagicMock name='check_context_or_escalate' id='5145097760'>

    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    def test_handles_missing_pending_materials_on_approve(self, mock_context):
        """Should ask user when approving but no materials pending."""
        mock_context.return_value = None
    
        state = {
            "ask_user_trigger": "material_checkpoint",
            "user_responses": {"Material question": "APPROVE"},
            "pending_validated_materials": [],
            "validated_materials": [],
        }
    
        result = supervisor_node(state)
    
        # Verify exact verdict
        assert result["supervisor_verdict"] == "ask_user"
        # Verify error message is set
        assert len(result["pending_user_questions"]) == 1
        assert "no materials" in result["pending_user_questions"][0].lower() or \
               "error" in result["pending_user_questions"][0].lower()
        # Verify materials remain empty
        assert result.get("validated_materials") == []
        assert result.get("pending_validated_materials") == []
        # Verify trigger cleared
>       assert result.get("ask_user_trigger") is None
E       AssertionError: assert 'material_checkpoint' is None
E        +  where 'material_checkpoint' = <built-in method get of dict object at 0x135f12380>('ask_user_trigger')
E        +    where <built-in method get of dict object at 0x135f12380> = {'archive_errors': [], 'ask_user_trigger': 'material_checkpoint', 'awaiting_user_input': True, 'pending_user_questions...ials were extracted for validation. Please specify materials manually using CHANGE_MATERIAL or CHANGE_DATABASE.'], ...}.get

tests/agents/supervision/test_supervisor_material_checkpoint.py:206: AssertionError
____________ TestSupervisorNode.test_triggers_backtrack_on_failure _____________

self = <tests.agents.supervision.test_supervisor_node_core.TestSupervisorNode object at 0x1318eeea0>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192327120'>
mock_context = <MagicMock name='check_context_or_escalate' id='5192322080'>
mock_call = <MagicMock name='call_agent_with_metrics' id='5192325776'>
validated_supervisor_response = {'error_analysis': {'confidence': 'high', 'error_persistence': 'not_applicable', 'error_type': 'none', 'root_cause_hyp...'Proceed to next stage if available', 'priority': 'medium', 'rationale': 'Current stage completed successfully'}], ...}

    @patch("src.agents.supervision.supervisor.call_agent_with_metrics")
    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    @patch("src.agents.supervision.supervisor.build_agent_prompt")
    def test_triggers_backtrack_on_failure(self, mock_prompt, mock_context, mock_call, validated_supervisor_response):
        """Should trigger backtrack on stage failure (using validated mock)."""
        mock_context.return_value = None
        mock_prompt.return_value = "prompt"
    
        mock_response = validated_supervisor_response.copy()
        mock_response["verdict"] = "backtrack_to_stage"
        mock_response["backtrack_target"] = "design"
        mock_response["reasoning"] = "design flawed"
        mock_call.return_value = mock_response
    
        state = {
            "current_stage_id": "stage1",
            "physics_verdict": "fail",
            "supervisor_call_count": 0,
            "plan": {"stages": []},
            "progress": {"stages": []},
        }
    
        result = supervisor_node(state)
    
        # STRICT: Verify exact verdict
        assert result["supervisor_verdict"] == "backtrack_to_stage"
        # STRICT: Verify backtrack_decision is set correctly
>       assert "backtrack_decision" in result
E       AssertionError: assert 'backtrack_decision' in {'archive_errors': [], 'should_stop': False, 'supervisor_feedback': 'Stage 1 analysis shows acceptable match with pape...r). Validation hierarchy progressing well. Proceeding to completion.', 'supervisor_verdict': 'backtrack_to_stage', ...}

tests/agents/supervision/test_supervisor_node_core.py:110: AssertionError
______________ TestSupervisorNode.test_returns_supervisor_verdict ______________

self = <tests.agents.supervision.test_supervisor_node_core.TestSupervisorNode object at 0x1318f96a0>
mock_derive = <MagicMock name='_derive_stage_completion_outcome' id='5192333840'>
mock_update = <MagicMock name='update_progress_stage_status' id='5192332832'>
mock_archive = <MagicMock name='archive_stage_outputs_to_progress' id='5192329136'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192324432'>
mock_context = <MagicMock name='check_context_or_escalate' id='5192326448'>
mock_call = <MagicMock name='call_agent_with_metrics' id='5192321744'>
validated_supervisor_response = {'error_analysis': {'confidence': 'high', 'error_persistence': 'not_applicable', 'error_type': 'none', 'root_cause_hyp...'Proceed to next stage if available', 'priority': 'medium', 'rationale': 'Current stage completed successfully'}], ...}

    @patch("src.agents.supervision.supervisor.call_agent_with_metrics")
    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    @patch("src.agents.supervision.supervisor.build_agent_prompt")
    @patch("src.agents.supervision.supervisor.archive_stage_outputs_to_progress")
    @patch("src.agents.supervision.supervisor.update_progress_stage_status")
    @patch("src.agents.supervision.supervisor._derive_stage_completion_outcome")
    def test_returns_supervisor_verdict(self, mock_derive, mock_update, mock_archive, mock_prompt, mock_context, mock_call, validated_supervisor_response):
        """Should return supervisor verdict from LLM output (using validated mock)."""
        mock_context.return_value = None
        mock_prompt.return_value = "prompt"
        mock_derive.return_value = ("completed_success", "OK")
    
        mock_response = validated_supervisor_response.copy()
        mock_response["verdict"] = "ok_continue"
        mock_response["reasoning"] = "All checks passed"
        mock_call.return_value = mock_response
    
        state = {
            "current_stage_id": "stage1",
            "supervisor_call_count": 5,
            "plan": {"stages": []},
            "progress": {"stages": []},
        }
    
        result = supervisor_node(state)
    
        # STRICT: Verify exact verdict
        assert result.get("supervisor_verdict") == "ok_continue"
        # STRICT: Verify feedback is set from reasoning
>       assert result.get("supervisor_feedback") == "All checks passed"
E       AssertionError: assert 'Stage 1 anal...o completion.' == 'All checks passed'
E         
E         - All checks passed
E         + Stage 1 analysis shows acceptable match with paper figures. Peak position at 702nm vs 700nm in paper (0.3% error). Validation hierarchy progressing well. Proceeding to completion.

tests/agents/supervision/test_supervisor_node_core.py:199: AssertionError
________________ TestSupervisorNode.test_handles_finish_verdict ________________

self = <tests.agents.supervision.test_supervisor_node_core.TestSupervisorNode object at 0x1318f97b0>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192334176'>
mock_context = <MagicMock name='check_context_or_escalate' id='5192328464'>
mock_call = <MagicMock name='call_agent_with_metrics' id='5192323088'>
validated_supervisor_response = {'error_analysis': {'confidence': 'high', 'error_persistence': 'not_applicable', 'error_type': 'none', 'root_cause_hyp...'Proceed to next stage if available', 'priority': 'medium', 'rationale': 'Current stage completed successfully'}], ...}

    @patch("src.agents.supervision.supervisor.call_agent_with_metrics")
    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    @patch("src.agents.supervision.supervisor.build_agent_prompt")
    def test_handles_finish_verdict(self, mock_prompt, mock_context, mock_call, validated_supervisor_response):
        """Should handle finish verdict when workflow complete (using validated mock)."""
        mock_context.return_value = None
        mock_prompt.return_value = "prompt"
    
        mock_response = validated_supervisor_response.copy()
        mock_response["verdict"] = "all_complete"
        mock_response["should_stop"] = True
        mock_response["reasoning"] = "All stages complete"
        mock_call.return_value = mock_response
    
        state = {
            "current_stage_id": None,
            "completed_stages": ["stage0", "stage1"],
            "supervisor_call_count": 0,
            "plan": {"stages": []},
            "progress": {"stages": []},
        }
    
        result = supervisor_node(state)
    
        # STRICT: Verify exact verdict
        assert result["supervisor_verdict"] == "all_complete"
        # STRICT: Verify should_stop is propagated
        assert result.get("should_stop") is True
        # STRICT: Verify feedback is set
>       assert result.get("supervisor_feedback") == "All stages complete"
E       AssertionError: assert 'Stage 1 anal...o completion.' == 'All stages complete'
E         
E         - All stages complete
E         + Stage 1 analysis shows acceptable match with paper figures. Peak position at 702nm vs 700nm in paper (0.3% error). Validation hierarchy progressing well. Proceeding to completion.

tests/agents/supervision/test_supervisor_node_core.py:232: AssertionError
_________ TestSupervisorNode.test_handles_missing_verdict_in_response __________

self = <tests.agents.supervision.test_supervisor_node_core.TestSupervisorNode object at 0x131895150>
mock_derive = <MagicMock name='_derive_stage_completion_outcome' id='5192330816'>
mock_update = <MagicMock name='update_progress_stage_status' id='5192331488'>
mock_archive = <MagicMock name='archive_stage_outputs_to_progress' id='5192333168'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192325104'>
mock_context = <MagicMock name='check_context_or_escalate' id='5192326784'>
mock_call = <MagicMock name='call_agent_with_metrics' id='5192329472'>

    @patch("src.agents.supervision.supervisor.call_agent_with_metrics")
    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    @patch("src.agents.supervision.supervisor.build_agent_prompt")
    @patch("src.agents.supervision.supervisor.archive_stage_outputs_to_progress")
    @patch("src.agents.supervision.supervisor.update_progress_stage_status")
    @patch("src.agents.supervision.supervisor._derive_stage_completion_outcome")
    def test_handles_missing_verdict_in_response(self, mock_derive, mock_update, mock_archive, mock_prompt, mock_context, mock_call):
        """Should default to ok_continue when verdict is missing from LLM response."""
        mock_context.return_value = None
        mock_prompt.return_value = "prompt"
        mock_derive.return_value = ("completed_success", "OK")
        # Response missing verdict field
        mock_call.return_value = {"reasoning": "Some reasoning"}
    
        state = {
            "current_stage_id": "stage1",
            "supervisor_call_count": 0,
            "plan": {"stages": []},
            "progress": {"stages": []},
        }
    
        result = supervisor_node(state)
    
        # STRICT: Should default to ok_continue when verdict missing
        assert result.get("supervisor_verdict") == "ok_continue"
        # STRICT: Feedback should still be set
>       assert result.get("supervisor_feedback") == "Some reasoning"
E       AssertionError: assert '' == 'Some reasoning'
E         
E         - Some reasoning

tests/agents/supervision/test_supervisor_node_core.py:266: AssertionError
____ TestSupervisorNode.test_complex_flow_with_context_update_and_archiving ____

self = <tests.agents.supervision.test_supervisor_node_core.TestSupervisorNode object at 0x131a06ed0>
mock_derive = <MagicMock name='_derive_stage_completion_outcome' id='5192329808'>
mock_update = <MagicMock name='update_progress_stage_status' id='5192329472'>
mock_archive = <MagicMock name='archive_stage_outputs_to_progress' id='5192326784'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192325104'>
mock_context = <MagicMock name='check_context_or_escalate' id='5192333168'>
mock_call = <MagicMock name='call_agent_with_metrics' id='5192331488'>
validated_supervisor_response = {'error_analysis': {'confidence': 'high', 'error_persistence': 'not_applicable', 'error_type': 'none', 'root_cause_hyp...'Proceed to next stage if available', 'priority': 'medium', 'rationale': 'Current stage completed successfully'}], ...}

    @patch("src.agents.supervision.supervisor.call_agent_with_metrics")
    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    @patch("src.agents.supervision.supervisor.build_agent_prompt")
    @patch("src.agents.supervision.supervisor.archive_stage_outputs_to_progress")
    @patch("src.agents.supervision.supervisor.update_progress_stage_status")
    @patch("src.agents.supervision.supervisor._derive_stage_completion_outcome")
    def test_complex_flow_with_context_update_and_archiving(self, mock_derive, mock_update, mock_archive, mock_prompt, mock_context, mock_call, validated_supervisor_response):
        """Should handle complex flow with context update, LLM call, and archiving."""
        # Context check returns update
        mock_context.return_value = {"context_budget": 1000}
        mock_prompt.return_value = "updated prompt"
        mock_derive.return_value = ("completed_success", "All good")
        mock_response = validated_supervisor_response.copy()
        mock_response["verdict"] = "ok_continue"
        mock_response["reasoning"] = "Proceeding"
        mock_call.return_value = mock_response
    
        state = {
            "current_stage_id": "stage1",
            "supervisor_call_count": 0,
            "plan": {"stages": []},
            "progress": {"stages": []},
        }
    
        result = supervisor_node(state)
    
        # STRICT: Verify all steps executed
        mock_context.assert_called_once_with(state, "supervisor")
        # STRICT: Verify prompt built with updated state (build_agent_prompt uses positional args)
        prompt_call_args = mock_prompt.call_args[0]
        assert prompt_call_args[0] == "supervisor"
        prompt_call_state = prompt_call_args[1]
        assert prompt_call_state.get("context_budget") == 1000
        # STRICT: Verify LLM called
        mock_call.assert_called_once()
        # STRICT: Verify archiving happened with updated state (state is merged with context_update)
        archive_call_args = mock_archive.call_args[0]
        assert archive_call_args[1] == "stage1"
        assert archive_call_args[0].get("context_budget") == 1000
        # STRICT: Verify status update with updated state
        update_call_args = mock_update.call_args[0]
        assert update_call_args[0].get("context_budget") == 1000
        assert update_call_args[1] == "stage1"
        assert update_call_args[2] == "completed_success"
        assert mock_update.call_args[1]["summary"] == "All good"
        # STRICT: Verify result
        assert result.get("supervisor_verdict") == "ok_continue"
>       assert result.get("supervisor_feedback") == "Proceeding"
E       AssertionError: assert 'Stage 1 anal...o completion.' == 'Proceeding'
E         
E         - Proceeding
E         + Stage 1 analysis shows acceptable match with paper figures. Peak position at 702nm vs 700nm in paper (0.3% error). Validation hierarchy progressing well. Proceeding to completion.

tests/agents/supervision/test_supervisor_node_core.py:717: AssertionError
______________ TestSupervisorNode.test_backtrack_verdict_from_llm ______________

self = <tests.agents.supervision.test_supervisor_node_core.TestSupervisorNode object at 0x131aa4140>
mock_prompt = <MagicMock name='build_agent_prompt' id='5191651008'>
mock_context = <MagicMock name='check_context_or_escalate' id='5192334176'>
mock_call = <MagicMock name='call_agent_with_metrics' id='5192328464'>
validated_supervisor_response = {'error_analysis': {'confidence': 'high', 'error_persistence': 'not_applicable', 'error_type': 'none', 'root_cause_hyp...'Proceed to next stage if available', 'priority': 'medium', 'rationale': 'Current stage completed successfully'}], ...}

    @patch("src.agents.supervision.supervisor.call_agent_with_metrics")
    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    @patch("src.agents.supervision.supervisor.build_agent_prompt")
    def test_backtrack_verdict_from_llm(self, mock_prompt, mock_context, mock_call, validated_supervisor_response):
        """Should properly handle backtrack_to_stage verdict from LLM."""
        mock_context.return_value = None
        mock_prompt.return_value = "prompt"
    
        mock_response = validated_supervisor_response.copy()
        mock_response["verdict"] = "backtrack_to_stage"
        mock_response["backtrack_target"] = "stage_0"
        mock_response["reasoning"] = "Design flaw detected"
        mock_call.return_value = mock_response
    
        state = {
            "current_stage_id": "stage1",
            "supervisor_call_count": 0,
            "plan": {"stages": []},
            "progress": {"stages": []},
        }
    
        result = supervisor_node(state)
    
        # STRICT: Verify verdict
        assert result["supervisor_verdict"] == "backtrack_to_stage"
        # STRICT: Verify backtrack_decision structure
>       assert "backtrack_decision" in result
E       AssertionError: assert 'backtrack_decision' in {'archive_errors': [], 'should_stop': False, 'supervisor_feedback': 'Stage 1 analysis shows acceptable match with pape...r). Validation hierarchy progressing well. Proceeding to completion.', 'supervisor_verdict': 'backtrack_to_stage', ...}

tests/agents/supervision/test_supervisor_node_core.py:991: AssertionError
________________ TestSupervisorNode.test_replan_needed_verdict _________________

self = <tests.agents.supervision.test_supervisor_node_core.TestSupervisorNode object at 0x131aa46e0>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192333840'>
mock_context = <MagicMock name='check_context_or_escalate' id='5192321744'>
mock_call = <MagicMock name='call_agent_with_metrics' id='5145097760'>
validated_supervisor_response = {'error_analysis': {'confidence': 'high', 'error_persistence': 'not_applicable', 'error_type': 'none', 'root_cause_hyp...'Proceed to next stage if available', 'priority': 'medium', 'rationale': 'Current stage completed successfully'}], ...}

    @patch("src.agents.supervision.supervisor.call_agent_with_metrics")
    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    @patch("src.agents.supervision.supervisor.build_agent_prompt")
    def test_replan_needed_verdict(self, mock_prompt, mock_context, mock_call, validated_supervisor_response):
        """Should handle replan_needed verdict from LLM."""
        mock_context.return_value = None
        mock_prompt.return_value = "prompt"
    
        mock_response = validated_supervisor_response.copy()
        mock_response["verdict"] = "replan_needed"
        mock_response["reasoning"] = "Plan needs revision due to new findings"
        mock_call.return_value = mock_response
    
        state = {
            "current_stage_id": "stage1",
            "supervisor_call_count": 0,
            "plan": {"stages": []},
            "progress": {"stages": []},
        }
    
        result = supervisor_node(state)
    
        # STRICT: Verify replan_needed verdict is passed through
        assert result["supervisor_verdict"] == "replan_needed"
>       assert result["supervisor_feedback"] == "Plan needs revision due to new findings"
E       AssertionError: assert 'Stage 1 anal...o completion.' == 'Plan needs r... new findings'
E         
E         - Plan needs revision due to new findings
E         + Stage 1 analysis shows acceptable match with paper figures. Peak position at 702nm vs 700nm in paper (0.3% error). Validation hierarchy progressing well. Proceeding to completion.

tests/agents/supervision/test_supervisor_node_core.py:1356: AssertionError
______ TestSupervisorNode.test_reasoning_with_newlines_and_special_chars _______

self = <tests.agents.supervision.test_supervisor_node_core.TestSupervisorNode object at 0x131aa4f50>
mock_derive = <MagicMock name='_derive_stage_completion_outcome' id='5191650336'>
mock_update = <MagicMock name='update_progress_stage_status' id='5191652016'>
mock_archive = <MagicMock name='archive_stage_outputs_to_progress' id='5191655040'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5191649328'>
mock_context = <MagicMock name='check_context_or_escalate' id='5191656720'>
mock_call = <MagicMock name='call_agent_with_metrics' id='5191657392'>
validated_supervisor_response = {'error_analysis': {'confidence': 'high', 'error_persistence': 'not_applicable', 'error_type': 'none', 'root_cause_hyp...'Proceed to next stage if available', 'priority': 'medium', 'rationale': 'Current stage completed successfully'}], ...}

    @patch("src.agents.supervision.supervisor.call_agent_with_metrics")
    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    @patch("src.agents.supervision.supervisor.build_agent_prompt")
    @patch("src.agents.supervision.supervisor.archive_stage_outputs_to_progress")
    @patch("src.agents.supervision.supervisor.update_progress_stage_status")
    @patch("src.agents.supervision.supervisor._derive_stage_completion_outcome")
    def test_reasoning_with_newlines_and_special_chars(self, mock_derive, mock_update, mock_archive, mock_prompt, mock_context, mock_call, validated_supervisor_response):
        """Should preserve newlines and special characters in reasoning/feedback."""
        mock_context.return_value = None
        mock_prompt.return_value = "prompt"
        mock_derive.return_value = ("completed_success", "OK")
        mock_response = validated_supervisor_response.copy()
        mock_response["verdict"] = "ok_continue"
        mock_response["reasoning"] = "Line 1\nLine 2\n\tIndented\n\"Quoted\""
        mock_call.return_value = mock_response
    
        state = {
            "current_stage_id": "stage1",
            "supervisor_call_count": 0,
            "plan": {"stages": []},
            "progress": {"stages": []},
        }
    
        result = supervisor_node(state)
    
        # STRICT: Special characters should be preserved
>       assert result.get("supervisor_feedback") == "Line 1\nLine 2\n\tIndented\n\"Quoted\""
E       assert 'Stage 1 anal...o completion.' == 'Line 1\nLine...ted\n"Quoted"'
E         
E         + Stage 1 analysis shows acceptable match with paper figures. Peak position at 702nm vs 700nm in paper (0.3% error). Validation hierarchy progressing well. Proceeding to completion.
E         - Line 1
E         - Line 2
E         - 	Indented
E         - "Quoted"

tests/agents/supervision/test_supervisor_node_core.py:2073: AssertionError
___________ TestNormalSupervision.test_normal_supervision_calls_llm ____________

self = <tests.agents.supervision.test_supervisor_recovery.TestNormalSupervision object at 0x131a451d0>
mock_update_status = <MagicMock name='update_progress_stage_status' id='5192331488'>
mock_archive = <MagicMock name='archive_stage_outputs_to_progress' id='5192333168'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192325104'>
mock_context = <MagicMock name='check_context_or_escalate' id='5192326784'>
mock_call = <MagicMock name='call_agent_with_metrics' id='5192329472'>

    @patch("src.agents.supervision.supervisor.call_agent_with_metrics")
    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    @patch("src.agents.supervision.supervisor.build_agent_prompt")
    @patch("src.agents.supervision.supervisor.archive_stage_outputs_to_progress")
    @patch("src.agents.supervision.supervisor.update_progress_stage_status")
    def test_normal_supervision_calls_llm(self, mock_update_status, mock_archive, mock_prompt, mock_context, mock_call):
        """Should call LLM for normal supervision."""
        mock_context.return_value = None
        mock_prompt.return_value = "system_prompt"
        mock_call.return_value = {"verdict": "ok_continue", "reasoning": "All good"}
        mock_archive.return_value = None
    
        state = {
            "ask_user_trigger": None,  # No trigger = normal supervision
            "current_stage_id": "stage1",
            "plan": {"stages": []},
            "progress": {"stages": []},
            "workflow_phase": "design",
        }
    
        result = supervisor_node(state)
    
        # Verify LLM was called
        mock_call.assert_called_once()
        call_args = mock_call.call_args
        assert call_args.kwargs["agent_name"] == "supervisor"
        assert call_args.kwargs["system_prompt"] == "system_prompt"
        assert "user_content" in call_args.kwargs
        assert call_args.kwargs["state"] == state
    
        # Verify verdict is set
        assert result["supervisor_verdict"] == "ok_continue"
>       assert result["supervisor_feedback"] == "All good"
E       AssertionError: assert '' == 'All good'
E         
E         - All good

tests/agents/supervision/test_supervisor_recovery.py:1307: AssertionError
_______ TestNormalSupervision.test_normal_supervision_backtrack_verdict ________

self = <tests.agents.supervision.test_supervisor_recovery.TestNormalSupervision object at 0x1318fa9c0>
mock_prompt = <MagicMock name='build_agent_prompt' id='5133093584'>
mock_context = <MagicMock name='check_context_or_escalate' id='5192325776'>
mock_call = <MagicMock name='call_agent_with_metrics' id='5192319056'>

    @patch("src.agents.supervision.supervisor.call_agent_with_metrics")
    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    @patch("src.agents.supervision.supervisor.build_agent_prompt")
    def test_normal_supervision_backtrack_verdict(self, mock_prompt, mock_context, mock_call):
        """Should handle backtrack_to_stage verdict."""
        mock_context.return_value = None
        mock_prompt.return_value = "prompt"
        mock_call.return_value = {
            "verdict": "backtrack_to_stage",
            "backtrack_target": "stage0",
            "reasoning": "Need to restart"
        }
    
        state = {
            "ask_user_trigger": None,
            "plan": {"stages": []},
            "progress": {"stages": []},
        }
    
        result = supervisor_node(state)
    
        # Verify backtrack decision is set
        assert result["supervisor_verdict"] == "backtrack_to_stage"
>       assert "backtrack_decision" in result
E       AssertionError: assert 'backtrack_decision' in {'archive_errors': [], 'supervisor_feedback': '', 'supervisor_verdict': 'backtrack_to_stage', 'workflow_phase': 'supervision'}

tests/agents/supervision/test_supervisor_recovery.py:1437: AssertionError
_________ TestNormalSupervision.test_normal_supervision_replan_verdict _________

self = <tests.agents.supervision.test_supervisor_recovery.TestNormalSupervision object at 0x131897c50>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192322080'>
mock_context = <MagicMock name='check_context_or_escalate' id='5192333840'>
mock_call = <MagicMock name='call_agent_with_metrics' id='5192321744'>

    @patch("src.agents.supervision.supervisor.call_agent_with_metrics")
    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    @patch("src.agents.supervision.supervisor.build_agent_prompt")
    def test_normal_supervision_replan_verdict(self, mock_prompt, mock_context, mock_call):
        """Should handle replan_needed verdict."""
        mock_context.return_value = None
        mock_prompt.return_value = "prompt"
        mock_call.return_value = {
            "verdict": "replan_needed",
            "reasoning": "Plan needs adjustment"
        }
    
        state = {
            "ask_user_trigger": None,
            "plan": {"stages": []},
            "progress": {"stages": []},
        }
    
        result = supervisor_node(state)
    
        assert result["supervisor_verdict"] == "replan_needed"
>       assert result["supervisor_feedback"] == "Plan needs adjustment"
E       AssertionError: assert '' == 'Plan needs adjustment'
E         
E         - Plan needs adjustment

tests/agents/supervision/test_supervisor_recovery.py:1507: AssertionError
__ TestNormalSupervision.test_normal_supervision_backtrack_decision_structure __

self = <tests.agents.supervision.test_supervisor_recovery.TestNormalSupervision object at 0x132bc08c0>
mock_update_status = <MagicMock name='update_progress_stage_status' id='5192327120'>
mock_archive = <MagicMock name='archive_stage_outputs_to_progress' id='5192329136'>
mock_prompt = <MagicMock name='build_agent_prompt' id='5192332832'>
mock_context = <MagicMock name='check_context_or_escalate' id='5192324432'>
mock_call = <MagicMock name='call_agent_with_metrics' id='5192327456'>

    @patch("src.agents.supervision.supervisor.call_agent_with_metrics")
    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    @patch("src.agents.supervision.supervisor.build_agent_prompt")
    @patch("src.agents.supervision.supervisor.archive_stage_outputs_to_progress")
    @patch("src.agents.supervision.supervisor.update_progress_stage_status")
    def test_normal_supervision_backtrack_decision_structure(self, mock_update_status, mock_archive, mock_prompt, mock_context, mock_call):
        """Should set complete backtrack_decision structure."""
        mock_context.return_value = None
        mock_prompt.return_value = "prompt"
        mock_call.return_value = {
            "verdict": "backtrack_to_stage",
            "backtrack_target": "stage0",
            "reasoning": "Need to restart from stage0"
        }
        mock_archive.return_value = None
    
        state = {
            "ask_user_trigger": None,
            "current_stage_id": "stage1",
            "plan": {"stages": []},
            "progress": {"stages": []},
        }
    
        result = supervisor_node(state)
    
        # Verify backtrack decision structure
        assert result["supervisor_verdict"] == "backtrack_to_stage"
>       assert "backtrack_decision" in result
E       AssertionError: assert 'backtrack_decision' in {'archive_errors': [], 'supervisor_feedback': '', 'supervisor_verdict': 'backtrack_to_stage', 'workflow_phase': 'supervision'}

tests/agents/supervision/test_supervisor_recovery.py:1558: AssertionError
_______ TestTriggerHandlerIntegration.test_execution_failure_limit_retry _______

self = <tests.agents.supervision.test_supervisor_recovery.TestTriggerHandlerIntegration object at 0x132b8c390>
mock_context = <MagicMock name='check_context_or_escalate' id='5191649664'>

    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    def test_execution_failure_limit_retry(self, mock_context):
        """Should handle execution_failure_limit with RETRY response."""
        mock_context.return_value = None
    
        state = {
            "ask_user_trigger": "execution_failure_limit",
            "user_responses": {"Q?": "RETRY_WITH_GUIDANCE: increase mesh resolution"},
            "pending_user_questions": ["Q?"],
            "execution_failure_count": 5,
            "plan": {"stages": []},
            "progress": {"stages": [], "user_interactions": []},
        }
    
        result = supervisor_node(state)
    
        # Verify counter is reset
        assert result["execution_failure_count"] == 0
        assert result["supervisor_verdict"] == "retry_generate_code"  # Retry with guidance
>       assert "mesh" in result.get("supervisor_feedback", "").lower()
E       AssertionError: assert 'mesh' in 'user guidance applied to execution_feedback'
E        +  where 'user guidance applied to execution_feedback' = <built-in method lower of str object at 0x127c46d30>()
E        +    where <built-in method lower of str object at 0x127c46d30> = 'User guidance applied to execution_feedback'.lower
E        +      where 'User guidance applied to execution_feedback' = <built-in method get of dict object at 0x136aa8400>('supervisor_feedback', '')
E        +        where <built-in method get of dict object at 0x136aa8400> = {'archive_errors': [], 'ask_user_trigger': None, 'awaiting_user_input': False, 'execution_failure_count': 0, ...}.get

tests/agents/supervision/test_supervisor_recovery.py:2071: AssertionError
_________ TestExecutionFailureLimitTrigger.test_resets_count_on_retry __________

self = <tests.agents.trigger_handlers.test_execution_physics_limits.TestExecutionFailureLimitTrigger object at 0x132ba1f90>
mock_context = <MagicMock name='check_context_or_escalate' id='5192327456'>

    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    def test_resets_count_on_retry(self, mock_context):
        """Should reset execution failure count on RETRY."""
        mock_context.return_value = None
    
        state = {
            "ask_user_trigger": "execution_failure_limit",
            "user_responses": {"Question": "RETRY with more memory"},
            "execution_failure_count": 3,
        }
    
        result = supervisor_node(state)
    
        assert result["execution_failure_count"] == 0
        assert result["supervisor_verdict"] == "retry_generate_code"
        assert "supervisor_feedback" in result
>       assert "more memory" in result["supervisor_feedback"]
E       AssertionError: assert 'more memory' in 'User guidance applied to execution_feedback'

tests/agents/trigger_handlers/test_execution_physics_limits.py:36: AssertionError
________ TestExecutionFailureLimitTrigger.test_resets_count_on_guidance ________

self = <tests.agents.trigger_handlers.test_execution_physics_limits.TestExecutionFailureLimitTrigger object at 0x132ba2c10>
mock_context = <MagicMock name='check_context_or_escalate' id='5192332832'>

    @patch("src.agents.supervision.supervisor.check_context_or_escalate")
    def test_resets_count_on_guidance(self, mock_context):
        """Should reset execution failure count on GUIDANCE."""
        mock_context.return_value = None
    
        state = {
            "ask_user_trigger": "execution_failure_limit",
            "user_responses": {"Question": "GUIDANCE: reduce resolution"},
            "execution_failure_count": 5,
        }
    
        result = supervisor_node(state)
    
        assert result["execution_failure_count"] == 0
        assert result["supervisor_verdict"] == "retry_generate_code"
>       assert "reduce resolution" in result["supervisor_feedback"]
E       AssertionError: assert 'reduce resolution' in 'User guidance applied to execution_feedback'

tests/agents/trigger_handlers/test_execution_physics_limits.py:53: AssertionError
__ TestHandleExecutionFailureLimit.test_handle_execution_failure_limit_retry ___

self = <tests.agents.trigger_handlers.test_execution_physics_limits.TestHandleExecutionFailureLimit object at 0x132ba2850>
mock_state = {'backtrack_decision': {'target_stage_id': 'stage0'}, 'paper_text': 'Short paper text', 'pending_validated_materials':...': 'stage0'}, {'dependencies': ['stage0'], 'stage_id': 'stage1'}, {'dependencies': ['stage1'], 'stage_id': 'stage2'}]}}
mock_result = {'execution_failure_count': 0, 'execution_feedback': 'USER GUIDANCE: Check memory.', 'supervisor_feedback': 'User guidance applied to execution_feedback', 'supervisor_verdict': 'retry_generate_code'}

    def test_handle_execution_failure_limit_retry(self, mock_state, mock_result):
        """Test RETRY resets count and sets feedback."""
        user_input = {"q1": "RETRY_WITH_GUIDANCE: Check memory."}
        initial_count = 5
        mock_result["execution_failure_count"] = initial_count
    
        trigger_handlers.handle_execution_failure_limit(mock_state, mock_result, user_input, "stage1")
    
        assert mock_result["execution_failure_count"] == 0
>       assert "Check memory" in mock_result["supervisor_feedback"]
E       AssertionError: assert 'Check memory' in 'User guidance applied to execution_feedback'

tests/agents/trigger_handlers/test_execution_physics_limits.py:232: AssertionError
_ TestHandleExecutionFailureLimit.test_handle_execution_failure_limit_guidance_keyword _

self = <tests.agents.trigger_handlers.test_execution_physics_limits.TestHandleExecutionFailureLimit object at 0x132b202b0>
mock_state = {'backtrack_decision': {'target_stage_id': 'stage0'}, 'paper_text': 'Short paper text', 'pending_validated_materials':...': 'stage0'}, {'dependencies': ['stage0'], 'stage_id': 'stage1'}, {'dependencies': ['stage1'], 'stage_id': 'stage2'}]}}
mock_result = {'execution_failure_count': 0, 'execution_feedback': 'USER GUIDANCE: reduce resolution', 'supervisor_feedback': 'User guidance applied to execution_feedback', 'supervisor_verdict': 'retry_generate_code'}

    def test_handle_execution_failure_limit_guidance_keyword(self, mock_state, mock_result):
        """Test GUIDANCE keyword resets count."""
        user_input = {"q1": "GUIDANCE: reduce resolution"}
        mock_result["execution_failure_count"] = 2
    
        trigger_handlers.handle_execution_failure_limit(mock_state, mock_result, user_input, "stage1")
    
        assert mock_result["execution_failure_count"] == 0
        assert mock_result["supervisor_verdict"] == "retry_generate_code"
>       assert "reduce resolution" in mock_result["supervisor_feedback"]
E       AssertionError: assert 'reduce resolution' in 'User guidance applied to execution_feedback'

tests/agents/trigger_handlers/test_execution_physics_limits.py:256: AssertionError
_ TestHandleExecutionFailureLimit.test_handle_execution_failure_limit_retry_and_guidance_both_present _

self = <tests.agents.trigger_handlers.test_execution_physics_limits.TestHandleExecutionFailureLimit object at 0x132bf1470>
mock_state = {'backtrack_decision': {'target_stage_id': 'stage0'}, 'paper_text': 'Short paper text', 'pending_validated_materials':...': 'stage0'}, {'dependencies': ['stage0'], 'stage_id': 'stage1'}, {'dependencies': ['stage1'], 'stage_id': 'stage2'}]}}
mock_result = {'execution_failure_count': 0, 'execution_feedback': 'USER GUIDANCE: RETRY with GUIDANCE: fix memory', 'supervisor_feedback': 'User guidance applied to execution_feedback', 'supervisor_verdict': 'retry_generate_code'}

    def test_handle_execution_failure_limit_retry_and_guidance_both_present(self, mock_state, mock_result):
        """Test when both RETRY and GUIDANCE keywords present."""
        user_input = {"q1": "RETRY with GUIDANCE: fix memory"}
        mock_result["execution_failure_count"] = 4
    
        trigger_handlers.handle_execution_failure_limit(mock_state, mock_result, user_input, "stage1")
    
        assert mock_result["execution_failure_count"] == 0
        assert mock_result["supervisor_verdict"] == "retry_generate_code"
>       assert "fix memory" in mock_result["supervisor_feedback"]
E       AssertionError: assert 'fix memory' in 'User guidance applied to execution_feedback'

tests/agents/trigger_handlers/test_execution_physics_limits.py:352: AssertionError
_ TestHandlePhysicsFailureLimit.test_handle_physics_failure_limit_retry_with_guidance _

self = <tests.agents.trigger_handlers.test_execution_physics_limits.TestHandlePhysicsFailureLimit object at 0x132ba2e90>
mock_state = {'backtrack_decision': {'target_stage_id': 'stage0'}, 'paper_text': 'Short paper text', 'pending_validated_materials':...': 'stage0'}, {'dependencies': ['stage0'], 'stage_id': 'stage1'}, {'dependencies': ['stage1'], 'stage_id': 'stage2'}]}}
mock_result = {'physics_failure_count': 0, 'physics_feedback': 'USER GUIDANCE: RETRY with better parameters', 'supervisor_feedback': 'User guidance applied to physics_feedback for code generator', 'supervisor_verdict': 'retry_generate_code'}

    def test_handle_physics_failure_limit_retry_with_guidance(self, mock_state, mock_result):
        """Test RETRY with guidance text."""
        user_input = {"q1": "RETRY with better parameters"}
        mock_result["physics_failure_count"] = 2
    
        trigger_handlers.handle_physics_failure_limit(mock_state, mock_result, user_input, "stage1")
    
        assert mock_result["physics_failure_count"] == 0
        assert mock_result["supervisor_verdict"] == "retry_generate_code"
>       assert "better parameters" in mock_result["supervisor_feedback"]
E       AssertionError: assert 'better parameters' in 'User guidance applied to physics_feedback for code generator'

tests/agents/trigger_handlers/test_execution_physics_limits.py:379: AssertionError
_ TestHandlePhysicsFailureLimit.test_handle_physics_failure_limit_feedback_overwrites_existing _

self = <tests.agents.trigger_handlers.test_execution_physics_limits.TestHandlePhysicsFailureLimit object at 0x132bfb650>
mock_state = {'backtrack_decision': {'target_stage_id': 'stage0'}, 'paper_text': 'Short paper text', 'pending_validated_materials':...': 'stage0'}, {'dependencies': ['stage0'], 'stage_id': 'stage1'}, {'dependencies': ['stage1'], 'stage_id': 'stage2'}]}}
mock_result = {'physics_failure_count': 0, 'physics_feedback': 'USER GUIDANCE: RETRY with new guidance', 'supervisor_feedback': 'User guidance applied to physics_feedback for code generator', 'supervisor_verdict': 'retry_generate_code'}

    def test_handle_physics_failure_limit_feedback_overwrites_existing(self, mock_state, mock_result):
        """Test handler overwrites existing feedback."""
        mock_result["supervisor_feedback"] = "Old feedback"
        user_input = {"q1": "RETRY with new guidance"}
    
        trigger_handlers.handle_physics_failure_limit(mock_state, mock_result, user_input, "stage1")
    
>       assert "new guidance" in mock_result["supervisor_feedback"]
E       AssertionError: assert 'new guidance' in 'User guidance applied to physics_feedback for code generator'

tests/agents/trigger_handlers/test_execution_physics_limits.py:624: AssertionError
_ TestHandlePhysicsFailureLimit.test_handle_execution_failure_limit_feedback_overwrites_existing _

self = <tests.agents.trigger_handlers.test_execution_physics_limits.TestHandlePhysicsFailureLimit object at 0x132c1e710>
mock_state = {'backtrack_decision': {'target_stage_id': 'stage0'}, 'paper_text': 'Short paper text', 'pending_validated_materials':...': 'stage0'}, {'dependencies': ['stage0'], 'stage_id': 'stage1'}, {'dependencies': ['stage1'], 'stage_id': 'stage2'}]}}
mock_result = {'execution_failure_count': 0, 'execution_feedback': 'USER GUIDANCE: RETRY with new guidance', 'supervisor_feedback': 'User guidance applied to execution_feedback', 'supervisor_verdict': 'retry_generate_code'}

    def test_handle_execution_failure_limit_feedback_overwrites_existing(self, mock_state, mock_result):
        """Test handler overwrites existing feedback."""
        mock_result["supervisor_feedback"] = "Old feedback"
        user_input = {"q1": "RETRY with new guidance"}
    
        trigger_handlers.handle_execution_failure_limit(mock_state, mock_result, user_input, "stage1")
    
>       assert "new guidance" in mock_result["supervisor_feedback"]
E       AssertionError: assert 'new guidance' in 'User guidance applied to execution_feedback'

tests/agents/trigger_handlers/test_execution_physics_limits.py:719: AssertionError
____ TestFeedbackFormat.test_execution_retry_feedback_includes_raw_response ____

self = <tests.agents.trigger_handlers.test_execution_physics_limits.TestFeedbackFormat object at 0x132ac8190>
mock_state = {'backtrack_decision': {'target_stage_id': 'stage0'}, 'paper_text': 'Short paper text', 'pending_validated_materials':...': 'stage0'}, {'dependencies': ['stage0'], 'stage_id': 'stage1'}, {'dependencies': ['stage1'], 'stage_id': 'stage2'}]}}
mock_result = {'execution_failure_count': 0, 'execution_feedback': 'USER GUIDANCE: RETRY with more memory allocation please', 'supervisor_feedback': 'User guidance applied to execution_feedback', 'supervisor_verdict': 'retry_generate_code'}

    def test_execution_retry_feedback_includes_raw_response(self, mock_state, mock_result):
        """RETRY feedback should include the raw user response."""
        user_input = {"q1": "RETRY with more memory allocation please"}
        mock_result["execution_failure_count"] = 2
    
        trigger_handlers.handle_execution_failure_limit(mock_state, mock_result, user_input, "stage1")
    
        feedback = mock_result["supervisor_feedback"]
>       assert feedback.startswith("User guidance:")
E       AssertionError: assert False
E        +  where False = <built-in method startswith of str object at 0x127c46d30>('User guidance:')
E        +    where <built-in method startswith of str object at 0x127c46d30> = 'User guidance applied to execution_feedback'.startswith

tests/agents/trigger_handlers/test_execution_physics_limits.py:1122: AssertionError
_____ TestFeedbackFormat.test_physics_retry_feedback_includes_raw_response _____

self = <tests.agents.trigger_handlers.test_execution_physics_limits.TestFeedbackFormat object at 0x132ac82d0>
mock_state = {'backtrack_decision': {'target_stage_id': 'stage0'}, 'paper_text': 'Short paper text', 'pending_validated_materials':...': 'stage0'}, {'dependencies': ['stage0'], 'stage_id': 'stage1'}, {'dependencies': ['stage1'], 'stage_id': 'stage2'}]}}
mock_result = {'physics_failure_count': 0, 'physics_feedback': 'USER GUIDANCE: RETRY with adjusted parameters', 'supervisor_feedback': 'User guidance applied to physics_feedback for code generator', 'supervisor_verdict': 'retry_generate_code'}

    def test_physics_retry_feedback_includes_raw_response(self, mock_state, mock_result):
        """RETRY feedback should include the raw user response."""
        user_input = {"q1": "RETRY with adjusted parameters"}
        mock_result["physics_failure_count"] = 1
    
        trigger_handlers.handle_physics_failure_limit(mock_state, mock_result, user_input, "stage1")
    
        feedback = mock_result["supervisor_feedback"]
>       assert feedback.startswith("User guidance:")
E       AssertionError: assert False
E        +  where False = <built-in method startswith of str object at 0x127c37130>('User guidance:')
E        +    where <built-in method startswith of str object at 0x127c37130> = 'User guidance applied to physics_feedback for code generator'.startswith

tests/agents/trigger_handlers/test_execution_physics_limits.py:1134: AssertionError
_ TestFeedbackFormat.test_execution_retry_empty_guidance_produces_valid_feedback _

self = <tests.agents.trigger_handlers.test_execution_physics_limits.TestFeedbackFormat object at 0x132b21480>
mock_state = {'backtrack_decision': {'target_stage_id': 'stage0'}, 'paper_text': 'Short paper text', 'pending_validated_materials':...': 'stage0'}, {'dependencies': ['stage0'], 'stage_id': 'stage1'}, {'dependencies': ['stage1'], 'stage_id': 'stage2'}]}}
mock_result = {'execution_failure_count': 0, 'execution_feedback': 'USER GUIDANCE: RETRY', 'supervisor_feedback': 'User guidance applied to execution_feedback', 'supervisor_verdict': 'retry_generate_code'}

    def test_execution_retry_empty_guidance_produces_valid_feedback(self, mock_state, mock_result):
        """RETRY without additional text should still produce valid feedback."""
        user_input = {"q1": "RETRY"}
        mock_result["execution_failure_count"] = 1
    
        trigger_handlers.handle_execution_failure_limit(mock_state, mock_result, user_input, "stage1")
    
        feedback = mock_result["supervisor_feedback"]
>       assert feedback.startswith("User guidance:")
E       AssertionError: assert False
E        +  where False = <built-in method startswith of str object at 0x127c46d30>('User guidance:')
E        +    where <built-in method startswith of str object at 0x127c46d30> = 'User guidance applied to execution_feedback'.startswith

tests/agents/trigger_handlers/test_execution_physics_limits.py:1145: AssertionError
_________ TestFeedbackFormat.test_execution_guidance_keyword_feedback __________

self = <tests.agents.trigger_handlers.test_execution_physics_limits.TestFeedbackFormat object at 0x132b215b0>
mock_state = {'backtrack_decision': {'target_stage_id': 'stage0'}, 'paper_text': 'Short paper text', 'pending_validated_materials':...': 'stage0'}, {'dependencies': ['stage0'], 'stage_id': 'stage1'}, {'dependencies': ['stage1'], 'stage_id': 'stage2'}]}}
mock_result = {'execution_failure_count': 0, 'execution_feedback': 'USER GUIDANCE: use less memory', 'supervisor_feedback': 'User guidance applied to execution_feedback', 'supervisor_verdict': 'retry_generate_code'}

    def test_execution_guidance_keyword_feedback(self, mock_state, mock_result):
        """GUIDANCE keyword should produce proper feedback."""
        user_input = {"q1": "GUIDANCE: use less memory"}
        mock_result["execution_failure_count"] = 1
    
        trigger_handlers.handle_execution_failure_limit(mock_state, mock_result, user_input, "stage1")
    
        feedback = mock_result["supervisor_feedback"]
>       assert feedback.startswith("User guidance:")
E       AssertionError: assert False
E        +  where False = <built-in method startswith of str object at 0x127c46d30>('User guidance:')
E        +    where <built-in method startswith of str object at 0x127c46d30> = 'User guidance applied to execution_feedback'.startswith

tests/agents/trigger_handlers/test_execution_physics_limits.py:1156: AssertionError
______ TestMultipleUserResponses.test_physics_uses_last_response_for_raw _______

self = <tests.agents.trigger_handlers.test_execution_physics_limits.TestMultipleUserResponses object at 0x132ac8550>
mock_state = {'backtrack_decision': {'target_stage_id': 'stage0'}, 'paper_text': 'Short paper text', 'pending_validated_materials':...': 'stage0'}, {'dependencies': ['stage0'], 'stage_id': 'stage1'}, {'dependencies': ['stage1'], 'stage_id': 'stage2'}]}}
mock_result = {'physics_failure_count': 0, 'physics_feedback': 'USER GUIDANCE: RETRY with better params', 'supervisor_feedback': 'User guidance applied to physics_feedback for code generator', 'supervisor_verdict': 'retry_generate_code'}

    def test_physics_uses_last_response_for_raw(self, mock_state, mock_result):
        """Raw response should be from the last response."""
        user_input = {"q1": "First response", "q2": "RETRY with better params"}
        mock_result["physics_failure_count"] = 2
    
        trigger_handlers.handle_physics_failure_limit(mock_state, mock_result, user_input, "stage1")
    
        # Raw response should be from last item
>       assert "RETRY with better params" in mock_result["supervisor_feedback"]
E       AssertionError: assert 'RETRY with better params' in 'User guidance applied to physics_feedback for code generator'

tests/agents/trigger_handlers/test_execution_physics_limits.py:1188: AssertionError
____ TestSpecialCharactersInResponses.test_execution_response_with_newlines ____

self = <tests.agents.trigger_handlers.test_execution_physics_limits.TestSpecialCharactersInResponses object at 0x132ac8b90>
mock_state = {'backtrack_decision': {'target_stage_id': 'stage0'}, 'paper_text': 'Short paper text', 'pending_validated_materials':...': 'stage0'}, {'dependencies': ['stage0'], 'stage_id': 'stage1'}, {'dependencies': ['stage1'], 'stage_id': 'stage2'}]}}
mock_result = {'execution_failure_count': 0, 'execution_feedback': 'USER GUIDANCE: RETRY\nwith\nmultiline\nguidance', 'supervisor_feedback': 'User guidance applied to execution_feedback', 'supervisor_verdict': 'retry_generate_code'}

    def test_execution_response_with_newlines(self, mock_state, mock_result):
        """Handler should work with newlines in response."""
        user_input = {"q1": "RETRY\nwith\nmultiline\nguidance"}
        mock_result["execution_failure_count"] = 1
    
        trigger_handlers.handle_execution_failure_limit(mock_state, mock_result, user_input, "stage1")
    
        assert mock_result["execution_failure_count"] == 0
        assert mock_result["supervisor_verdict"] == "retry_generate_code"
>       assert "multiline" in mock_result["supervisor_feedback"]
E       AssertionError: assert 'multiline' in 'User guidance applied to execution_feedback'

tests/agents/trigger_handlers/test_execution_physics_limits.py:1345: AssertionError
____ TestSpecialCharactersInResponses.test_execution_response_with_unicode _____

self = <tests.agents.trigger_handlers.test_execution_physics_limits.TestSpecialCharactersInResponses object at 0x132b21ba0>
mock_state = {'backtrack_decision': {'target_stage_id': 'stage0'}, 'paper_text': 'Short paper text', 'pending_validated_materials':...': 'stage0'}, {'dependencies': ['stage0'], 'stage_id': 'stage1'}, {'dependencies': ['stage1'], 'stage_id': 'stage2'}]}}
mock_result = {'execution_failure_count': 0, 'execution_feedback': 'USER GUIDANCE: RETRY avec des caractres franais: , , , ', 'supervisor_feedback': 'User guidance applied to execution_feedback', 'supervisor_verdict': 'retry_generate_code'}

    def test_execution_response_with_unicode(self, mock_state, mock_result):
        """Handler should work with unicode characters."""
        user_input = {"q1": "RETRY avec des caractres franais: , , , "}
        mock_result["execution_failure_count"] = 1
    
        trigger_handlers.handle_execution_failure_limit(mock_state, mock_result, user_input, "stage1")
    
        assert mock_result["execution_failure_count"] == 0
>       assert "franais" in mock_result["supervisor_feedback"]
E       AssertionError: assert 'franais' in 'User guidance applied to execution_feedback'

tests/agents/trigger_handlers/test_execution_physics_limits.py:1364: AssertionError
___________ TestBoundaryConditions.test_execution_very_long_response ___________

self = <tests.agents.trigger_handlers.test_execution_physics_limits.TestBoundaryConditions object at 0x132ac9310>
mock_state = {'backtrack_decision': {'target_stage_id': 'stage0'}, 'paper_text': 'Short paper text', 'pending_validated_materials':...': 'stage0'}, {'dependencies': ['stage0'], 'stage_id': 'stage1'}, {'dependencies': ['stage1'], 'stage_id': 'stage2'}]}}
mock_result = {'execution_failure_count': 0, 'execution_feedback': 'USER GUIDANCE: RETRY xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx...xx', 'supervisor_feedback': 'User guidance applied to execution_feedback', 'supervisor_verdict': 'retry_generate_code'}

    def test_execution_very_long_response(self, mock_state, mock_result):
        """Handler should work with very long responses."""
        long_guidance = "RETRY " + "x" * 10000
        user_input = {"q1": long_guidance}
        mock_result["execution_failure_count"] = 1
    
        trigger_handlers.handle_execution_failure_limit(mock_state, mock_result, user_input, "stage1")
    
        assert mock_result["execution_failure_count"] == 0
        assert mock_result["supervisor_verdict"] == "retry_generate_code"
        # Full response should be in feedback
>       assert len(mock_result["supervisor_feedback"]) > 10000
E       AssertionError: assert 43 > 10000
E        +  where 43 = len('User guidance applied to execution_feedback')

tests/agents/trigger_handlers/test_execution_physics_limits.py:1468: AssertionError
__________ TestDictKeyOrdering.test_execution_response_order_matters ___________

self = <tests.agents.trigger_handlers.test_execution_physics_limits.TestDictKeyOrdering object at 0x132ac9590>
mock_state = {'backtrack_decision': {'target_stage_id': 'stage0'}, 'paper_text': 'Short paper text', 'pending_validated_materials':...': 'stage0'}, {'dependencies': ['stage0'], 'stage_id': 'stage1'}, {'dependencies': ['stage1'], 'stage_id': 'stage2'}]}}
mock_result = {'execution_failure_count': 0, 'execution_feedback': 'USER GUIDANCE: RETRY with this guidance', 'supervisor_feedback': 'User guidance applied to execution_feedback', 'supervisor_verdict': 'retry_generate_code'}

    def test_execution_response_order_matters(self, mock_state, mock_result):
        """Last value in ordered dict should be used for response."""
        # Create dict with specific ordering
        user_input = {}
        user_input["first"] = "STOP"
        user_input["second"] = "SKIP"
        user_input["third"] = "RETRY with this guidance"
        mock_result["execution_failure_count"] = 2
    
        trigger_handlers.handle_execution_failure_limit(mock_state, mock_result, user_input, "stage1")
    
        # Last response "RETRY with this guidance" should be used
        assert mock_result["execution_failure_count"] == 0
        assert mock_result["supervisor_verdict"] == "retry_generate_code"
>       assert "RETRY with this guidance" in mock_result["supervisor_feedback"]
E       AssertionError: assert 'RETRY with this guidance' in 'User guidance applied to execution_feedback'

tests/agents/trigger_handlers/test_execution_physics_limits.py:1546: AssertionError
_________ TestKeywordOnlyResponses.test_execution_exact_retry_keyword __________

self = <tests.agents.trigger_handlers.test_execution_physics_limits.TestKeywordOnlyResponses object at 0x132ac9810>
mock_state = {'backtrack_decision': {'target_stage_id': 'stage0'}, 'paper_text': 'Short paper text', 'pending_validated_materials':...': 'stage0'}, {'dependencies': ['stage0'], 'stage_id': 'stage1'}, {'dependencies': ['stage1'], 'stage_id': 'stage2'}]}}
mock_result = {'execution_failure_count': 0, 'execution_feedback': 'USER GUIDANCE: RETRY', 'supervisor_feedback': 'User guidance applied to execution_feedback', 'supervisor_verdict': 'retry_generate_code'}

    def test_execution_exact_retry_keyword(self, mock_state, mock_result):
        """Exact 'RETRY' keyword should work."""
        user_input = {"q1": "RETRY"}
        mock_result["execution_failure_count"] = 1
    
        trigger_handlers.handle_execution_failure_limit(mock_state, mock_result, user_input, "stage1")
    
        assert mock_result["execution_failure_count"] == 0
        assert mock_result["supervisor_verdict"] == "retry_generate_code"
        # Feedback should still be set
        assert "supervisor_feedback" in mock_result
>       assert mock_result["supervisor_feedback"].startswith("User guidance:")
E       AssertionError: assert False
E        +  where False = <built-in method startswith of str object at 0x127c46d30>('User guidance:')
E        +    where <built-in method startswith of str object at 0x127c46d30> = 'User guidance applied to execution_feedback'.startswith

tests/agents/trigger_handlers/test_execution_physics_limits.py:1572: AssertionError
_______ TestHandleClarification.test_handle_clarification_with_response ________

self = <tests.agents.trigger_handlers.test_misc_dispatcher.TestHandleClarification object at 0x131bc8190>
mock_state = {'backtrack_decision': {'target_stage_id': 'stage0'}, 'paper_text': 'Short paper text', 'pending_validated_materials':...': 'stage0'}, {'dependencies': ['stage0'], 'stage_id': 'stage1'}, {'dependencies': ['stage1'], 'stage_id': 'stage2'}]}}
mock_result = {'supervisor_feedback': 'User clarification recorded in user_context', 'supervisor_verdict': 'ok_continue', 'user_context': ['[2025-12-06 15:10] This is a clarification']}

    def test_handle_clarification_with_response(self, mock_state, mock_result):
        """Should process clarification response."""
        user_input = {"q1": "This is a clarification"}
    
        trigger_handlers.handle_clarification(mock_state, mock_result, user_input, "stage1")
    
        assert mock_result["supervisor_verdict"] == "ok_continue"
        assert "supervisor_feedback" in mock_result
        assert "clarification" in mock_result["supervisor_feedback"].lower()
>       assert "This is a clarification" in mock_result["supervisor_feedback"]
E       AssertionError: assert 'This is a clarification' in 'User clarification recorded in user_context'

tests/agents/trigger_handlers/test_misc_dispatcher.py:774: AssertionError
_____ TestHandleClarification.test_handle_clarification_multiple_responses _____

self = <tests.agents.trigger_handlers.test_misc_dispatcher.TestHandleClarification object at 0x132a44d60>
mock_state = {'backtrack_decision': {'target_stage_id': 'stage0'}, 'paper_text': 'Short paper text', 'pending_validated_materials':...': 'stage0'}, {'dependencies': ['stage0'], 'stage_id': 'stage1'}, {'dependencies': ['stage1'], 'stage_id': 'stage2'}]}}
mock_result = {'supervisor_feedback': 'User clarification recorded in user_context', 'supervisor_verdict': 'ok_continue', 'user_context': ['[2025-12-06 15:10] Last clarification']}

    def test_handle_clarification_multiple_responses(self, mock_state, mock_result):
        """Should use last response for clarification."""
        user_input = {"q1": "First", "q2": "Last clarification"}
    
        trigger_handlers.handle_clarification(mock_state, mock_result, user_input, "stage1")
    
>       assert "Last clarification" in mock_result["supervisor_feedback"]
E       AssertionError: assert 'Last clarification' in 'User clarification recorded in user_context'

tests/agents/trigger_handlers/test_misc_dispatcher.py:792: AssertionError
_ TestUserGuidanceIntegration.test_execution_retry_resets_counter_effectively __

self = <tests.integration.graph.test_graph_structure.TestUserGuidanceIntegration object at 0x131ef95b0>
mock_checkpoint = <MagicMock name='save_checkpoint' id='5179013312'>

    @patch('src.graph.save_checkpoint')
    def test_execution_retry_resets_counter_effectively(self, mock_checkpoint):
        """Test that RETRY for execution_failure_limit effectively resets counter.
    
        After user provides RETRY, the execution_failure_count should be 0,
        meaning the next failure won't immediately hit the limit again.
        """
        from src.agents.supervision.supervisor import supervisor_node
    
        # Simulate state where execution hit the limit
        state = {
            "ask_user_trigger": "execution_failure_limit",
            "user_responses": {"Q1": "RETRY with reduced grid resolution"},
            "pending_user_questions": ["Execution failed 3 times"],
            "execution_failure_count": MAX_EXECUTION_FAILURES,
            "current_stage_id": "stage1",
            "progress": {"stages": [], "user_interactions": []},
        }
    
        result = supervisor_node(state)
    
        # Verify counter is reset to 0
        assert result.get("execution_failure_count") == 0, (
            f"Counter should be reset to 0, got {result.get('execution_failure_count')}"
        )
    
        # Verify guidance is captured
        assert "supervisor_feedback" in result, "User guidance should be in supervisor_feedback"
>       assert "reduced grid" in result["supervisor_feedback"].lower() or "retry" in result["supervisor_feedback"].lower(), (
            f"User's guidance should be captured, got: {result.get('supervisor_feedback')}"
        )
E       AssertionError: User's guidance should be captured, got: User guidance applied to execution_feedback
E       assert ('reduced grid' in 'user guidance applied to execution_feedback' or 'retry' in 'user guidance applied to execution_feedback')
E        +  where 'user guidance applied to execution_feedback' = <built-in method lower of str object at 0x127c46d30>()
E        +    where <built-in method lower of str object at 0x127c46d30> = 'User guidance applied to execution_feedback'.lower
E        +  and   'user guidance applied to execution_feedback' = <built-in method lower of str object at 0x127c46d30>()
E        +    where <built-in method lower of str object at 0x127c46d30> = 'User guidance applied to execution_feedback'.lower

tests/integration/graph/test_graph_structure.py:2581: AssertionError
_ TestPlannerFeedback.test_plan_reviewer_sets_planner_feedback_on_llm_rejection _

self = <tests.integration.planning.test_plan_reviewer_rules.TestPlannerFeedback object at 0x133acf610>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_plan_reviewer_sets_planner_feedback_on_llm_rejection(self, base_state):
        """plan_reviewer should set planner_feedback from LLM response on rejection."""
        from src.agents.planning import plan_reviewer_node
    
        base_state["plan"] = {
            "paper_id": "test",
            "title": "Test Plan",
            "stages": [
                {
                    "stage_id": "s1",
                    "stage_type": "MATERIAL_VALIDATION",
                    "targets": ["Fig1"],
                    "dependencies": [],
                }
            ],
        }
        base_state["replan_count"] = 0
    
        feedback_text = "The plan needs better target coverage and clearer dependencies."
        mock_response = {
            "verdict": "needs_revision",
            "issues": [{"severity": "major", "description": "Incomplete"}],
            "summary": "Plan needs work",
            "feedback": feedback_text,
        }
    
        with patch(
            "src.agents.planning.call_agent_with_metrics", return_value=mock_response
        ):
            result = plan_reviewer_node(base_state)
    
>       assert result.get("planner_feedback") == feedback_text, (
            f"Expected planner_feedback='{feedback_text}', got '{result.get('planner_feedback')}'"
        )
E       AssertionError: Expected planner_feedback='The plan needs better target coverage and clearer dependencies.', got 'Plan needs work'
E       assert 'Plan needs work' == 'The plan nee...dependencies.'
E         
E         - The plan needs better target coverage and clearer dependencies.
E         + Plan needs work

tests/integration/planning/test_plan_reviewer_rules.py:1499: AssertionError
_______________ TestAdaptPromptsNode.test_adapt_prompts_success ________________

self = <tests.integration.planning.test_prompt_adaptation.TestAdaptPromptsNode object at 0x132f8d310>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_adapt_prompts_success(self, base_state):
        """adapt_prompts_node should call LLM and update state."""
        from src.agents.planning import adapt_prompts_node
    
        mock_response = {
            "adaptations": [
                {"agent": "planner", "adaptation": "Focus on materials"},
                {"agent": "designer", "adaptation": "Check boundaries"},
            ],
            "paper_domain": "metamaterials",
        }
    
        with patch(
            "src.agents.planning.call_agent_with_metrics", return_value=mock_response
        ) as mock_llm:
            result = adapt_prompts_node(base_state)
    
        # Verify exact workflow phase
        assert result["workflow_phase"] == "adapting_prompts"
    
        # Verify adaptations are stored exactly as returned
>       assert result["prompt_adaptations"] == mock_response["adaptations"]
E       AssertionError: assert [] == [{'adaptation...: 'designer'}]
E         
E         Right contains 2 more items, first extra item: {'adaptation': 'Focus on materials', 'agent': 'planner'}
E         Use -v to get more diff

tests/integration/planning/test_prompt_adaptation.py:34: AssertionError
__________ TestAdaptPromptsNode.test_adapt_prompts_node_updates_state __________

self = <tests.integration.planning.test_prompt_adaptation.TestAdaptPromptsNode object at 0x132fdc180>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_adapt_prompts_node_updates_state(self, base_state):
        """Ensure adaptations and domain bubble through the state."""
        from src.agents.planning import adapt_prompts_node
    
        mock_response = {
            "adaptations": ["Focus on plasmonics", "Use Johnson-Christy data"],
            "paper_domain": "plasmonics",
        }
    
        with patch(
            "src.agents.planning.call_agent_with_metrics", return_value=mock_response
        ) as mock_call:
            result = adapt_prompts_node(base_state)
    
        assert mock_call.called
        call_kwargs = mock_call.call_args.kwargs
        assert call_kwargs.get("agent_name") == "prompt_adaptor"
        assert result.get("workflow_phase") == "adapting_prompts"
        # Verify exact adaptations list
>       assert result["prompt_adaptations"] == mock_response["adaptations"]
E       AssertionError: assert [] == ['Focus on pl...Christy data']
E         
E         Right contains 2 more items, first extra item: 'Focus on plasmonics'
E         Use -v to get more diff

tests/integration/planning/test_prompt_adaptation.py:113: AssertionError
_______ TestAdaptPromptsNode.test_adapt_prompts_handles_none_adaptations _______

self = <tests.integration.planning.test_prompt_adaptation.TestAdaptPromptsNode object at 0x133308750>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_adapt_prompts_handles_none_adaptations(self, base_state):
        """adapt_prompts_node should handle None adaptations in response."""
        from src.agents.planning import adapt_prompts_node
    
        mock_response = {
            "adaptations": None,
            "paper_domain": "test",
        }
    
        with patch(
            "src.agents.planning.call_agent_with_metrics", return_value=mock_response
        ):
            result = adapt_prompts_node(base_state)
    
        assert result["workflow_phase"] == "adapting_prompts"
        # Should convert None to empty list
        assert result["prompt_adaptations"] == []
>       assert result["paper_domain"] == "test"
               ^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'paper_domain'

tests/integration/planning/test_prompt_adaptation.py:240: KeyError
___ TestAdaptPromptsNode.test_adapt_prompts_handles_missing_adaptations_key ____

self = <tests.integration.planning.test_prompt_adaptation.TestAdaptPromptsNode object at 0x133a9fb60>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_adapt_prompts_handles_missing_adaptations_key(self, base_state):
        """adapt_prompts_node should handle missing adaptations key."""
        from src.agents.planning import adapt_prompts_node
    
        mock_response = {
            "paper_domain": "test",
        }
    
        with patch(
            "src.agents.planning.call_agent_with_metrics", return_value=mock_response
        ):
            result = adapt_prompts_node(base_state)
    
        assert result["workflow_phase"] == "adapting_prompts"
        # Should default to empty list
        assert result["prompt_adaptations"] == []
>       assert result["paper_domain"] == "test"
               ^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'paper_domain'

tests/integration/planning/test_prompt_adaptation.py:258: KeyError
_____ TestAdaptPromptsNode.test_adapt_prompts_handles_non_list_adaptations _____

self = <tests.integration.planning.test_prompt_adaptation.TestAdaptPromptsNode object at 0x133a9fc50>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_adapt_prompts_handles_non_list_adaptations(self, base_state):
        """adapt_prompts_node should handle non-list adaptations."""
        from src.agents.planning import adapt_prompts_node
    
        mock_response = {
            "adaptations": "not a list",
            "paper_domain": "test",
        }
    
        with patch(
            "src.agents.planning.call_agent_with_metrics", return_value=mock_response
        ):
            result = adapt_prompts_node(base_state)
    
        assert result["workflow_phase"] == "adapting_prompts"
        # Should convert non-list to empty list
        assert result["prompt_adaptations"] == []
>       assert result["paper_domain"] == "test"
               ^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'paper_domain'

tests/integration/planning/test_prompt_adaptation.py:277: KeyError
____ TestAdaptPromptsNode.test_adapt_prompts_handles_empty_adaptations_list ____

self = <tests.integration.planning.test_prompt_adaptation.TestAdaptPromptsNode object at 0x132fda510>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_adapt_prompts_handles_empty_adaptations_list(self, base_state):
        """adapt_prompts_node should handle empty adaptations list."""
        from src.agents.planning import adapt_prompts_node
    
        mock_response = {
            "adaptations": [],
            "paper_domain": "test",
        }
    
        with patch(
            "src.agents.planning.call_agent_with_metrics", return_value=mock_response
        ):
            result = adapt_prompts_node(base_state)
    
        assert result["workflow_phase"] == "adapting_prompts"
        assert result["prompt_adaptations"] == []
>       assert result["paper_domain"] == "test"
               ^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'paper_domain'

tests/integration/planning/test_prompt_adaptation.py:313: KeyError
______ TestAdaptPromptsNode.test_adapt_prompts_handles_none_paper_domain _______

self = <tests.integration.planning.test_prompt_adaptation.TestAdaptPromptsNode object at 0x133a42d00>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_adapt_prompts_handles_none_paper_domain(self, base_state):
        """adapt_prompts_node should handle None paper_domain."""
        from src.agents.planning import adapt_prompts_node
    
        mock_response = {
            "adaptations": ["test"],
            "paper_domain": None,
        }
    
        with patch(
            "src.agents.planning.call_agent_with_metrics", return_value=mock_response
        ):
            result = adapt_prompts_node(base_state)
    
        assert result["workflow_phase"] == "adapting_prompts"
>       assert result["prompt_adaptations"] == ["test"]
E       AssertionError: assert [] == ['test']
E         
E         Right contains one more item: 'test'
E         Use -v to get more diff

tests/integration/planning/test_prompt_adaptation.py:330: AssertionError
___ TestAdaptPromptsNode.test_adapt_prompts_handles_missing_paper_domain_key ___

self = <tests.integration.planning.test_prompt_adaptation.TestAdaptPromptsNode object at 0x133ad6810>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_adapt_prompts_handles_missing_paper_domain_key(self, base_state):
        """adapt_prompts_node should handle missing paper_domain key."""
        from src.agents.planning import adapt_prompts_node
    
        mock_response = {
            "adaptations": ["test"],
        }
    
        with patch(
            "src.agents.planning.call_agent_with_metrics", return_value=mock_response
        ):
            result = adapt_prompts_node(base_state)
    
        assert result["workflow_phase"] == "adapting_prompts"
>       assert result["prompt_adaptations"] == ["test"]
E       AssertionError: assert [] == ['test']
E         
E         Right contains one more item: 'test'
E         Use -v to get more diff

tests/integration/planning/test_prompt_adaptation.py:348: AssertionError
______ TestAdaptPromptsNode.test_adapt_prompts_handles_empty_paper_domain ______

self = <tests.integration.planning.test_prompt_adaptation.TestAdaptPromptsNode object at 0x133ad6b10>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_adapt_prompts_handles_empty_paper_domain(self, base_state):
        """adapt_prompts_node should handle empty string paper_domain."""
        from src.agents.planning import adapt_prompts_node
    
        mock_response = {
            "adaptations": ["test"],
            "paper_domain": "",
        }
    
        with patch(
            "src.agents.planning.call_agent_with_metrics", return_value=mock_response
        ):
            result = adapt_prompts_node(base_state)
    
        assert result["workflow_phase"] == "adapting_prompts"
>       assert result["prompt_adaptations"] == ["test"]
E       AssertionError: assert [] == ['test']
E         
E         Right contains one more item: 'test'
E         Use -v to get more diff

tests/integration/planning/test_prompt_adaptation.py:367: AssertionError
___ TestAdaptPromptsNode.test_adapt_prompts_preserves_existing_paper_domain ____

self = <tests.integration.planning.test_prompt_adaptation.TestAdaptPromptsNode object at 0x131d022b0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_adapt_prompts_preserves_existing_paper_domain(self, base_state):
        """adapt_prompts_node should use paper_domain from state if available."""
        from src.agents.planning import adapt_prompts_node
    
        base_state["paper_domain"] = "existing_domain"
        mock_response = {
            "adaptations": ["test"],
            "paper_domain": "new_domain",
        }
    
        with patch(
            "src.agents.planning.call_agent_with_metrics", return_value=mock_response
        ) as mock_llm:
            result = adapt_prompts_node(base_state)
    
        # Should use new domain from LLM response
>       assert result["paper_domain"] == "new_domain"
               ^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'paper_domain'

tests/integration/planning/test_prompt_adaptation.py:387: KeyError
__ TestAdaptPromptsNode.test_adapt_prompts_handles_none_paper_domain_in_state __

self = <tests.integration.planning.test_prompt_adaptation.TestAdaptPromptsNode object at 0x131d003c0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_adapt_prompts_handles_none_paper_domain_in_state(self, base_state):
        """adapt_prompts_node should handle None paper_domain in state."""
        from src.agents.planning import adapt_prompts_node
    
        base_state["paper_domain"] = None
        mock_response = {
            "adaptations": ["test"],
            "paper_domain": "new_domain",
        }
    
        with patch(
            "src.agents.planning.call_agent_with_metrics", return_value=mock_response
        ) as mock_llm:
            result = adapt_prompts_node(base_state)
    
>       assert result["paper_domain"] == "new_domain"
               ^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'paper_domain'

tests/integration/planning/test_prompt_adaptation.py:408: KeyError
_____ TestAdaptPromptsNode.test_adapt_prompts_does_not_mutate_input_state ______

self = <tests.integration.planning.test_prompt_adaptation.TestAdaptPromptsNode object at 0x131d31bd0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_adapt_prompts_does_not_mutate_input_state(self, base_state):
        """adapt_prompts_node should not mutate input state."""
        from src.agents.planning import adapt_prompts_node
    
        original_state = deepcopy(base_state)
        original_prompt_adaptations = base_state.get("prompt_adaptations")
        original_workflow_phase = base_state.get("workflow_phase")
    
        mock_response = {
            "adaptations": ["test"],
            "paper_domain": "new_domain",
        }
    
        with patch(
            "src.agents.planning.call_agent_with_metrics", return_value=mock_response
        ):
            result = adapt_prompts_node(base_state)
    
        # Verify input state values are unchanged (may be mutated by call_agent_with_metrics for metrics)
        assert base_state.get("paper_text") == original_state.get("paper_text")
        assert base_state.get("paper_domain") == original_state.get("paper_domain")
        assert base_state.get("paper_id") == original_state.get("paper_id")
        # prompt_adaptations may exist in base_state from initialization, but should not be changed by function
        assert base_state.get("prompt_adaptations") == original_prompt_adaptations
        assert base_state.get("workflow_phase") == original_workflow_phase
    
        # Verify result is separate dict with new values
        assert result is not base_state
        assert result["workflow_phase"] == "adapting_prompts"
>       assert result["prompt_adaptations"] == ["test"]
E       AssertionError: assert [] == ['test']
E         
E         Right contains one more item: 'test'
E         Use -v to get more diff

tests/integration/planning/test_prompt_adaptation.py:443: AssertionError
_ TestAdaptPromptsNode.test_adapt_prompts_handles_context_check_returning_metrics _

self = <tests.integration.planning.test_prompt_adaptation.TestAdaptPromptsNode object at 0x132f766d0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_adapt_prompts_handles_context_check_returning_metrics(self, base_state):
        """adapt_prompts_node should continue when context check returns metrics only."""
        from src.agents.planning import adapt_prompts_node
    
        metrics_only = {
            "metrics": {"tokens_used": 1000},
        }
    
        mock_response = {
            "adaptations": ["test"],
            "paper_domain": "test",
        }
    
        with patch(
            "src.agents.base.check_context_or_escalate", return_value=metrics_only
        ), patch(
            "src.agents.planning.call_agent_with_metrics", return_value=mock_response
        ) as mock_llm:
            result = adapt_prompts_node(base_state)
    
        # Should continue and call LLM
        assert mock_llm.called
        assert result["workflow_phase"] == "adapting_prompts"
>       assert result["prompt_adaptations"] == ["test"]
E       AssertionError: assert [] == ['test']
E         
E         Right contains one more item: 'test'
E         Use -v to get more diff

tests/integration/planning/test_prompt_adaptation.py:573: AssertionError
____ TestAdaptPromptsNode.test_adapt_prompts_handles_large_adaptations_list ____

self = <tests.integration.planning.test_prompt_adaptation.TestAdaptPromptsNode object at 0x131d6fa10>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_adapt_prompts_handles_large_adaptations_list(self, base_state):
        """adapt_prompts_node should handle large adaptations list."""
        from src.agents.planning import adapt_prompts_node
    
        large_adaptations = [f"adaptation_{i}" for i in range(100)]
        mock_response = {
            "adaptations": large_adaptations,
            "paper_domain": "test",
        }
    
        with patch(
            "src.agents.planning.call_agent_with_metrics", return_value=mock_response
        ):
            result = adapt_prompts_node(base_state)
    
        assert result["workflow_phase"] == "adapting_prompts"
>       assert len(result["prompt_adaptations"]) == 100
E       assert 0 == 100
E        +  where 0 = len([])

tests/integration/planning/test_prompt_adaptation.py:591: AssertionError
__ TestAdaptPromptsNode.test_adapt_prompts_handles_complex_adaptation_objects __

self = <tests.integration.planning.test_prompt_adaptation.TestAdaptPromptsNode object at 0x133aec9b0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_adapt_prompts_handles_complex_adaptation_objects(self, base_state):
        """adapt_prompts_node should handle complex adaptation objects."""
        from src.agents.planning import adapt_prompts_node
    
        complex_adaptations = [
            {"agent": "planner", "adaptation": "Focus on materials", "priority": 1},
            {"agent": "designer", "adaptation": "Check boundaries", "priority": 2},
            {"nested": {"key": "value"}},
        ]
        mock_response = {
            "adaptations": complex_adaptations,
            "paper_domain": "test",
        }
    
        with patch(
            "src.agents.planning.call_agent_with_metrics", return_value=mock_response
        ):
            result = adapt_prompts_node(base_state)
    
        assert result["workflow_phase"] == "adapting_prompts"
>       assert result["prompt_adaptations"] == complex_adaptations
E       AssertionError: assert [] == [{'adaptation...y': 'value'}}]
E         
E         Right contains 3 more items, first extra item: {'adaptation': 'Focus on materials', 'agent': 'planner', 'priority': 1}
E         Use -v to get more diff

tests/integration/planning/test_prompt_adaptation.py:614: AssertionError
____ TestPromptAdaptations.test_replace_adaptation_replaces_all_occurrences ____

self = <tests.integration.real.test_prompts.TestPromptAdaptations object at 0x133219ef0>

    def test_replace_adaptation_replaces_all_occurrences(self):
        """Replace adaptation should replace all occurrences of the marker."""
        base_prompt = "Text with MARKER here and MARKER there."
        adaptations = [
            {
                "target_agent": "planner",
                "modification_type": "replace",
                "content": "REPLACED",
                "section_marker": "MARKER",
            }
        ]
    
        result = apply_prompt_adaptations(base_prompt, "planner", adaptations)
>       assert "MARKER" not in result, "All MARKER occurrences should be replaced"
E       AssertionError: All MARKER occurrences should be replaced
E       assert 'MARKER' not in 'Text with M...ARKER there.'
E         
E         'MARKER' is contained here:
E           Text with MARKER here and MARKER there.

tests/integration/real/test_prompts.py:869: AssertionError
___ TestPromptAdaptations.test_adaptation_with_special_characters_in_content ___

self = <tests.integration.real.test_prompts.TestPromptAdaptations object at 0x13321a190>

    def test_adaptation_with_special_characters_in_content(self):
        """Adaptation content with special characters should work."""
        base_prompt = "Original prompt."
        special_content = "Content with special chars: ${}[]()\\n\\t*+?^"
        adaptations = [
            {
                "target_agent": "planner",
                "modification_type": "append",
                "content": special_content,
            }
        ]
    
        result = apply_prompt_adaptations(base_prompt, "planner", adaptations)
>       assert special_content in result
E       AssertionError: assert 'Content with special chars: ${}[]()\\n\\t*+?^' in 'Original prompt.\n\n# Paper-Specific Adaptation\n'

tests/integration/real/test_prompts.py:885: AssertionError
_________ TestPromptAdaptations.test_adaptation_with_multiline_content _________

self = <tests.integration.real.test_prompts.TestPromptAdaptations object at 0x132f4f050>

        def test_adaptation_with_multiline_content(self):
            """Adaptation with multiline content should work."""
            base_prompt = "Original prompt."
            multiline_content = """Line 1
    Line 2
    Line 3"""
            adaptations = [
                {
                    "target_agent": "planner",
                    "modification_type": "append",
                    "content": multiline_content,
                }
            ]
    
            result = apply_prompt_adaptations(base_prompt, "planner", adaptations)
>           assert "Line 1" in result
E           AssertionError: assert 'Line 1' in 'Original prompt.\n\n# Paper-Specific Adaptation\n'

tests/integration/real/test_prompts.py:902: AssertionError
__ TestBuildAgentPromptOrder.test_placeholders_substituted_before_adaptations __

self = <tests.integration.real.test_prompts.TestBuildAgentPromptOrder object at 0x132f54910>

    def test_placeholders_substituted_before_adaptations(self):
        """Placeholders should be substituted before adaptations are applied."""
        # Create a state with adaptations that reference placeholder-like text
        state = create_initial_state(
            paper_id="test",
            paper_text="Test paper content for validation.",
            paper_domain="plasmonics",
        )
        state["prompt_adaptations"] = [
            {
                "target_agent": "planner",
                "modification_type": "append",
                "content": "Adaptation content with {THRESHOLDS_TABLE} reference",
            }
        ]
    
        prompt = build_agent_prompt("planner", state)
    
        # The placeholder in the adaptation should NOT be substituted
        # (adaptations are applied after placeholder substitution in global_rules/agent_prompt)
        # But if the adaptation content itself contains {THRESHOLDS_TABLE}, it won't be substituted
        # because adaptations are applied after substitution
        # This test verifies the order is correct
>       assert "Adaptation content" in prompt
E       assert 'Adaptation content' in "# Global Non-Negotiable Rules\n\nThese rules apply to ALL agents in the system. They must be prepended to every agent...onances significantly)\n- Sets validation stage (will be checked in Stage 0)\n```\n\n\n\n# Paper-Specific Adaptation\n"

tests/integration/real/test_prompts.py:954: AssertionError
_ TestErrorHandling.test_get_agent_prompt_cached_bypasses_cache_with_adaptations _

self = <tests.integration.real.test_prompts.TestErrorHandling object at 0x132fc1a90>

    def test_get_agent_prompt_cached_bypasses_cache_with_adaptations(self):
        """get_agent_prompt_cached should bypass cache when adaptations exist."""
        state = create_initial_state(
            paper_id="test",
            paper_text="Test paper content for validation.",
            paper_domain="plasmonics",
        )
        state["prompt_adaptations"] = [
            {
                "target_agent": "planner",
                "modification_type": "append",
                "content": "CACHE_BYPASS_TEST",
            }
        ]
        cache = {}
    
        # Should not use cache when adaptations exist
        prompt1 = get_agent_prompt_cached("planner", state, cache=cache)
>       assert "CACHE_BYPASS_TEST" in prompt1
E       assert 'CACHE_BYPASS_TEST' in "# Global Non-Negotiable Rules\n\nThese rules apply to ALL agents in the system. They must be prepended to every agent...onances significantly)\n- Sets validation stage (will be checked in Stage 0)\n```\n\n\n\n# Paper-Specific Adaptation\n"

tests/integration/real/test_prompts.py:1087: AssertionError
_____ TestRoutingReturnsValidValues.test_supervisor_verdicts_match_routing _____

self = <tests.integration.real.test_routing.TestRoutingReturnsValidValues object at 0x132f55d10>

    def test_supervisor_verdicts_match_routing(self):
        """Supervisor schema verdicts must match what routing expects."""
        schema_file = self.SCHEMAS_DIR / "supervisor_output_schema.json"
        with open(schema_file, encoding="utf-8") as file:
            schema = json.load(file)
    
        schema_verdicts = set(schema["properties"]["verdict"]["enum"])
        handled_verdicts = {
            "ok_continue",
            "change_priority",
            "replan_needed",
            "ask_user",
            "backtrack_to_stage",
            "all_complete",
        }
    
        unhandled = schema_verdicts - handled_verdicts
>       assert not unhandled, (
            "Supervisor schema allows verdicts that routing doesn't handle: "
            f"{unhandled}\nThis could cause routing errors at runtime!"
        )
E       AssertionError: Supervisor schema allows verdicts that routing doesn't handle: {'retry_plan_review', 'retry_code_review', 'retry_design_review', 'retry_design', 'replan_with_guidance', 'retry_analyze', 'retry_generate_code'}
E         This could cause routing errors at runtime!
E       assert not {'replan_with_guidance', 'retry_analyze', 'retry_code_review', 'retry_design', 'retry_design_review', 'retry_generate_code', ...}

tests/integration/real/test_routing.py:52: AssertionError
_ TestStateMutations.test_plan_reviewer_increments_replan_count_on_needs_revision _

self = <tests.integration.real.test_state_mutations.TestStateMutations object at 0x1335bc520>

    def test_plan_reviewer_increments_replan_count_on_needs_revision(self):
        """plan_reviewer_node must increment replan_count when verdict is needs_revision."""
        from src.agents.planning import plan_reviewer_node
    
        state = create_initial_state("test", "paper about nanorods" * 20, "plasmonics")
        state["plan"] = {
            "title": "Test Plan",
            "stages": [
                {
                    "stage_id": "stage_0",
                    "stage_type": "MATERIAL_VALIDATION",
                    "description": "Validate gold optical constants",
                    "targets": ["Fig1"],
                    "dependencies": [],
                }
            ],
            "targets": [{"figure_id": "Fig1", "description": "Test"}],
        }
        state["replan_count"] = 1
        # Set max_replans higher than current count to allow incrementing
        state["runtime_config"] = {"max_replans": 5}
        mock_response = {
            "verdict": "needs_revision",
            "issues": [{"severity": "major", "description": "Problem"}],
            "summary": "Needs work",
            "feedback": "Fix issues",
        }
    
        with patch(
            "src.agents.planning.call_agent_with_metrics",
            return_value=mock_response,
        ):
            result = plan_reviewer_node(state)
    
        assert result["last_plan_review_verdict"] == "needs_revision"
        assert "replan_count" in result
        # Should increment from 1 to 2 for LLM rejections
        assert result["replan_count"] == 2, (
            f"BUG: replan_count should be 2 (was 1), but got {result['replan_count']}. "
            "plan_reviewer_node is not incrementing replan_count for LLM rejections."
        )
        assert "planner_feedback" in result
>       assert result["planner_feedback"] == "Fix issues"
E       AssertionError: assert 'Needs work' == 'Fix issues'
E         
E         - Fix issues
E         + Needs work

tests/integration/real/test_state_mutations.py:466: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  src.agents.helpers.context:context.py:148 State validation issue for plan_review: PLAN_WARNING: Target Fig1 has precision_requirement='good' (5%) without digitized_data_path
  -> Fix: Digitized data is recommended for 'good' precision to enable quantitative metrics. Vision comparison may achieve this but with less certainty.
____________ TestStateMutations.test_counter_increments_on_revision ____________

self = <tests.integration.real.test_state_mutations.TestStateMutations object at 0x1335dd7c0>

    def test_counter_increments_on_revision(self):
        """Revision counters must increment when verdict is needs_revision."""
        from src.agents.design import design_reviewer_node
    
        state = create_initial_state("test", "paper text", "plasmonics")
        state["current_stage_id"] = "stage_0"
        state["design_description"] = {"stage_id": "stage_0", "design_description": "A valid FDTD simulation design for aluminum nanorods", "geometry": "nanorod"}
        state["design_revision_count"] = 0
        mock_response = {
            "verdict": "needs_revision",
            "issues": [{"severity": "major", "description": "problem"}],
            "summary": "Fix",
            "feedback": "Needs improvement",
        }
    
        with patch(
            "src.agents.design.call_agent_with_metrics",
            return_value=mock_response,
        ):
            result = design_reviewer_node(state)
    
        assert "design_revision_count" in result
        assert result["design_revision_count"] == 1, "Counter should increment on needs_revision"
        assert "last_design_review_verdict" in result
        assert result["last_design_review_verdict"] == "needs_revision"
        assert "reviewer_feedback" in result
>       assert result["reviewer_feedback"] == "Needs improvement"
E       AssertionError: assert 'Fix' == 'Needs improvement'
E         
E         - Needs improvement
E         + Fix

tests/integration/real/test_state_mutations.py:985: AssertionError
_ TestStateMutations.test_design_reviewer_counter_does_not_increment_on_approve _

self = <tests.integration.real.test_state_mutations.TestStateMutations object at 0x1335dda90>

    def test_design_reviewer_counter_does_not_increment_on_approve(self):
        """design_reviewer_node must not increment counter when verdict is approve."""
        from src.agents.design import design_reviewer_node
    
        state = create_initial_state("test", "paper text", "plasmonics")
        state["current_stage_id"] = "stage_0"
        state["design_description"] = {"stage_id": "stage_0", "design_description": "A valid FDTD simulation design for aluminum nanorods", "geometry": "nanorod"}
        state["design_revision_count"] = 5
        mock_response = {
            "verdict": "approve",
            "issues": [],
            "summary": "Good",
        }
    
        with patch(
            "src.agents.design.call_agent_with_metrics",
            return_value=mock_response,
        ):
            result = design_reviewer_node(state)
    
        assert result["design_revision_count"] == 5, "Counter should not increment on approve"
        assert result["last_design_review_verdict"] == "approve"
>       assert "reviewer_feedback" not in result
E       AssertionError: assert 'reviewer_feedback' not in {'design_revision_count': 5, 'last_design_review_verdict': 'approve', 'reviewer_feedback': None, 'reviewer_issues': [], ...}

tests/integration/real/test_state_mutations.py:1011: AssertionError
_ TestStateMutations.test_design_reviewer_normalizes_verdict_reject_to_needs_revision _

self = <tests.integration.real.test_state_mutations.TestStateMutations object at 0x1335ddc70>

    def test_design_reviewer_normalizes_verdict_reject_to_needs_revision(self):
        """design_reviewer_node must normalize 'reject' verdict to 'needs_revision'."""
        from src.agents.design import design_reviewer_node
    
        state = create_initial_state("test", "paper text", "plasmonics")
        state["current_stage_id"] = "stage_0"
        state["design_description"] = {"stage_id": "stage_0", "design_description": "A valid FDTD simulation design for aluminum nanorods", "geometry": "nanorod"}
        state["runtime_config"] = {"max_design_revisions": 5}
        mock_response = {"verdict": "reject", "issues": [], "summary": "Bad", "feedback": "Fix it"}
    
        with patch(
            "src.agents.design.call_agent_with_metrics",
            return_value=mock_response,
        ):
            result = design_reviewer_node(state)
    
        assert result["last_design_review_verdict"] == "needs_revision"
>       assert result["reviewer_feedback"] == "Fix it"
E       AssertionError: assert 'Bad' == 'Fix it'
E         
E         - Fix it
E         + Bad

tests/integration/real/test_state_mutations.py:1149: AssertionError
___________ TestStateMutations.test_code_generator_rejects_stub_code ___________

self = <tests.integration.real.test_state_mutations.TestStateMutations object at 0x1335de580>

    def test_code_generator_rejects_stub_code(self):
        """code_generator_node must reject stub code from LLM."""
        from src.agents.code import code_generator_node
    
        state = create_initial_state("test", "paper text " * 20, "plasmonics")
        state["current_stage_id"] = "stage_0"
        state["current_stage_type"] = "MATERIAL_VALIDATION"
        state["design_description"] = {
            "design_description": "A valid FDTD simulation design for aluminum nanorods", "geometry": "nanorod",
            "parameters": {"length": 100, "diameter": 40, "mesh_resolution": 10},
        }
        stub_code = "STUB: Code would be generated here"
    
        with patch(
            "src.agents.code.call_agent_with_metrics",
            return_value={"code": stub_code},
        ):
            with patch("src.agents.code.check_context_or_escalate", return_value=None):
                result = code_generator_node(state)
    
>       assert "code_revision_count" in result
E       AssertionError: assert 'code_revision_count' in {'code': 'STUB: Code would be generated here', 'reviewer_feedback': 'ERROR: Generated code is empty or contains stub m...roduce valid Meep simulation code. Please regenerate with proper implementation.', 'workflow_phase': 'code_generation'}

tests/integration/real/test_state_mutations.py:1773: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=True, empty=True). Code generation must produce valid simulation code.
____ TestStateMutations.test_code_generator_rejects_code_with_todo_at_start ____

self = <tests.integration.real.test_state_mutations.TestStateMutations object at 0x1335de5d0>

    def test_code_generator_rejects_code_with_todo_at_start(self):
        """code_generator_node must reject code starting with TODO marker."""
        from src.agents.code import code_generator_node
    
        state = create_initial_state("test", "paper text " * 20, "plasmonics")
        state["current_stage_id"] = "stage_0"
        state["current_stage_type"] = "MATERIAL_VALIDATION"
        state["design_description"] = {
            "design_description": "A valid FDTD simulation design for aluminum nanorods", "geometry": "nanorod",
            "parameters": {"length": 100, "diameter": 40, "mesh_resolution": 10},
        }
        stub_code = "# TODO: Implement this simulation"
    
        with patch(
            "src.agents.code.call_agent_with_metrics",
            return_value={"code": stub_code},
        ):
            with patch("src.agents.code.check_context_or_escalate", return_value=None):
                result = code_generator_node(state)
    
>       assert "code_revision_count" in result
E       AssertionError: assert 'code_revision_count' in {'code': '# TODO: Implement this simulation', 'reviewer_feedback': 'ERROR: Generated code is empty or contains stub ma...roduce valid Meep simulation code. Please regenerate with proper implementation.', 'workflow_phase': 'code_generation'}

tests/integration/real/test_state_mutations.py:1798: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=True, empty=True). Code generation must produce valid simulation code.
__________ TestStateMutations.test_code_generator_rejects_short_code ___________

self = <tests.integration.real.test_state_mutations.TestStateMutations object at 0x1335de670>

    def test_code_generator_rejects_short_code(self):
        """code_generator_node must reject code shorter than 50 chars."""
        from src.agents.code import code_generator_node
    
        state = create_initial_state("test", "paper text " * 20, "plasmonics")
        state["current_stage_id"] = "stage_0"
        state["current_stage_type"] = "MATERIAL_VALIDATION"
        state["design_description"] = {
            "design_description": "A valid FDTD simulation design for aluminum nanorods", "geometry": "nanorod",
            "parameters": {"length": 100, "diameter": 40, "mesh_resolution": 10},
        }
        short_code = "import meep"  # Too short
    
        with patch(
            "src.agents.code.call_agent_with_metrics",
            return_value={"code": short_code},
        ):
            with patch("src.agents.code.check_context_or_escalate", return_value=None):
                result = code_generator_node(state)
    
>       assert "code_revision_count" in result
E       AssertionError: assert 'code_revision_count' in {'code': 'import meep', 'reviewer_feedback': 'ERROR: Generated code is empty or contains stub markers. Code generation...roduce valid Meep simulation code. Please regenerate with proper implementation.', 'workflow_phase': 'code_generation'}

tests/integration/real/test_state_mutations.py:1860: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=False, empty=True). Code generation must produce valid simulation code.
_______ TestStateMutations.test_code_generator_uses_simulation_code_key ________

self = <tests.integration.real.test_state_mutations.TestStateMutations object at 0x1335de760>

    def test_code_generator_uses_simulation_code_key(self):
        """code_generator_node must handle simulation_code key from LLM."""
        from src.agents.code import code_generator_node
    
        state = create_initial_state("test", "paper text " * 20, "plasmonics")
        state["current_stage_id"] = "stage_0"
        state["current_stage_type"] = "MATERIAL_VALIDATION"
        state["design_description"] = {
            "design_description": "A valid FDTD simulation design for aluminum nanorods", "geometry": "nanorod",
            "parameters": {"length": 100, "diameter": 40, "mesh_resolution": 10},
        }
        mock_code = "import meep as mp\nimport numpy as np\n\n# Full simulation code with sufficient length for validation"
    
        with patch(
            "src.agents.code.call_agent_with_metrics",
            return_value={"simulation_code": mock_code},  # Different key
        ):
            with patch("src.agents.code.check_context_or_escalate", return_value=None):
                result = code_generator_node(state)
    
        assert "code" in result
>       assert result["code"] == mock_code
E       assert '{\n  "simula...alidation"\n}' == 'import meep ...or validation'
E         
E         + {
E         +   "simulation_code": "import meep as mp\nimport numpy as np\n\n# Full simulation code with sufficient length for validation"
E         + }
E         - import meep as mp
E         - import numpy as np
E         - 
E         - # Full simulation code with sufficient length for validation

tests/integration/real/test_state_mutations.py:1930: AssertionError
______ TestStateMutations.test_code_generator_respects_max_code_revisions ______

self = <tests.integration.real.test_state_mutations.TestStateMutations object at 0x1335de850>

    def test_code_generator_respects_max_code_revisions(self):
        """code_generator_node must respect max_code_revisions when rejecting stub."""
        from src.agents.code import code_generator_node
    
        state = create_initial_state("test", "paper text " * 20, "plasmonics")
        state["current_stage_id"] = "stage_0"
        state["current_stage_type"] = "MATERIAL_VALIDATION"
        state["design_description"] = {
            "design_description": "A valid FDTD simulation design for aluminum nanorods", "geometry": "nanorod",
            "parameters": {"length": 100, "diameter": 40, "mesh_resolution": 10},
        }
        state["code_revision_count"] = MAX_CODE_REVISIONS  # Already at max
        stub_code = "STUB: Code goes here"
    
        with patch(
            "src.agents.code.call_agent_with_metrics",
            return_value={"code": stub_code},
        ):
            with patch("src.agents.code.check_context_or_escalate", return_value=None):
                result = code_generator_node(state)
    
        # Should not exceed max
>       assert result["code_revision_count"] == MAX_CODE_REVISIONS
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'code_revision_count'

tests/integration/real/test_state_mutations.py:2008: KeyError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=True, empty=True). Code generation must produce valid simulation code.
___________ TestStateMutations.test_code_reviewer_sets_verdict_field ___________

self = <tests.integration.real.test_state_mutations.TestStateMutations object at 0x1335de8a0>

    def test_code_reviewer_sets_verdict_field(self):
        """code_reviewer_node must set last_code_review_verdict."""
        from src.agents.code import code_reviewer_node
    
        state = create_initial_state("test", "paper text " * 20, "plasmonics")
        state["current_stage_id"] = "stage_0"
        state["code"] = "import meep as mp\n# Code here"
        state["design_description"] = {"design_description": "A valid FDTD simulation design for aluminum nanorods", "geometry": "nanorod"}
        mock_response = {
            "verdict": "approve",
            "issues": [],
            "summary": "Good code",
        }
    
        with patch(
            "src.agents.code.call_agent_with_metrics",
            return_value=mock_response,
        ):
            result = code_reviewer_node(state)
    
        assert "last_code_review_verdict" in result
        assert result["last_code_review_verdict"] == "approve"
        assert "workflow_phase" in result
        assert result["workflow_phase"] == "code_review"
        # Should include reviewer_issues (empty list on approve)
        assert "reviewer_issues" in result
        assert result["reviewer_issues"] == []
        # Should not set reviewer_feedback on approve
>       assert "reviewer_feedback" not in result
E       AssertionError: assert 'reviewer_feedback' not in {'code_revision_count': 0, 'last_code_review_verdict': 'approve', 'reviewer_feedback': None, 'reviewer_issues': [], ...}

tests/integration/real/test_state_mutations.py:2042: AssertionError
__ TestStateMutations.test_code_reviewer_increments_counter_on_needs_revision __

self = <tests.integration.real.test_state_mutations.TestStateMutations object at 0x1335de8f0>

    def test_code_reviewer_increments_counter_on_needs_revision(self):
        """code_reviewer_node must increment code_revision_count on needs_revision."""
        from src.agents.code import code_reviewer_node
    
        state = create_initial_state("test", "paper text " * 20, "plasmonics")
        state["current_stage_id"] = "stage_0"
        state["code"] = "import meep as mp\n# Code here"
        state["design_description"] = {"design_description": "A valid FDTD simulation design for aluminum nanorods", "geometry": "nanorod"}
        state["code_revision_count"] = 1
        state["runtime_config"] = {"max_code_revisions": 5}
        mock_response = {
            "verdict": "needs_revision",
            "issues": [{"severity": "major", "description": "Problem"}],
            "summary": "Needs fixes",
            "feedback": "Fix issues",
        }
    
        with patch(
            "src.agents.code.call_agent_with_metrics",
            return_value=mock_response,
        ):
            result = code_reviewer_node(state)
    
        assert result["last_code_review_verdict"] == "needs_revision"
        assert result["code_revision_count"] == 2
        assert "reviewer_feedback" in result
>       assert result["reviewer_feedback"] == "Fix issues"
E       AssertionError: assert 'Needs fixes' == 'Fix issues'
E         
E         - Fix issues
E         + Needs fixes

tests/integration/real/test_state_mutations.py:2070: AssertionError
_ TestStateMutations.test_code_reviewer_normalizes_verdict_reject_to_needs_revision _

self = <tests.integration.real.test_state_mutations.TestStateMutations object at 0x1335dea80>

    def test_code_reviewer_normalizes_verdict_reject_to_needs_revision(self):
        """code_reviewer_node must normalize 'reject' verdict to 'needs_revision'."""
        from src.agents.code import code_reviewer_node
    
        state = create_initial_state("test", "paper text " * 20, "plasmonics")
        state["current_stage_id"] = "stage_0"
        state["code"] = "import meep as mp\n# Code here"
        state["design_description"] = {"design_description": "A valid FDTD simulation design for aluminum nanorods", "geometry": "nanorod"}
        state["runtime_config"] = {"max_code_revisions": 5}
        mock_response = {"verdict": "reject", "issues": [], "summary": "Bad", "feedback": "Fix it"}
    
        with patch(
            "src.agents.code.call_agent_with_metrics",
            return_value=mock_response,
        ):
            result = code_reviewer_node(state)
    
        assert result["last_code_review_verdict"] == "needs_revision"
>       assert result["reviewer_feedback"] == "Fix it"
E       AssertionError: assert 'Bad' == 'Fix it'
E         
E         - Fix it
E         + Bad

tests/integration/real/test_state_mutations.py:2190: AssertionError
_ TestSupervisorMaterialCheckpoint.test_supervisor_material_checkpoint_without_pending_materials_escalates _

self = <tests.integration.supervision.test_supervisor_triggers.TestSupervisorMaterialCheckpoint object at 0x1339462c0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': 'material_checkpoint', ...}

    def test_supervisor_material_checkpoint_without_pending_materials_escalates(
        self, base_state
    ):
        """User APPROVEs but no materials pending - should escalate."""
        from src.agents.supervision.supervisor import supervisor_node
    
        base_state["ask_user_trigger"] = "material_checkpoint"
        base_state["user_responses"] = {"Q1": "APPROVE"}
        base_state["pending_validated_materials"] = []
        base_state["pending_user_questions"] = ["Approve materials?"]
    
        result = supervisor_node(base_state)
    
        assert result.get("supervisor_verdict") == "ask_user"
        questions = result.get("pending_user_questions", [])
        assert questions and "No materials were extracted" in questions[0]
>       assert result.get("ask_user_trigger") is None
E       AssertionError: assert 'material_checkpoint' is None
E        +  where 'material_checkpoint' = <built-in method get of dict object at 0x13321c080>('ask_user_trigger')
E        +    where <built-in method get of dict object at 0x13321c080> = {'archive_errors': [], 'ask_user_trigger': 'material_checkpoint', 'awaiting_user_input': True, 'pending_user_questions...ials were extracted for validation. Please specify materials manually using CHANGE_MATERIAL or CHANGE_DATABASE.'], ...}.get

tests/integration/supervision/test_supervisor_triggers.py:86: AssertionError
_ TestSupervisorCodeReviewLimit.test_supervisor_handles_code_review_limit_with_hint _

self = <tests.integration.supervision.test_supervisor_triggers.TestSupervisorCodeReviewLimit object at 0x133901d10>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': 'code_review_limit', ...}

    def test_supervisor_handles_code_review_limit_with_hint(self, base_state):
        """User provides HINT - should reset counter and provide feedback."""
        from src.agents.supervision.supervisor import supervisor_node
    
        base_state["ask_user_trigger"] = "code_review_limit"
        base_state["user_responses"] = {
            "Q1": "PROVIDE_HINT: Try using mp.Medium instead"
        }
        base_state["pending_user_questions"] = ["Code review limit reached"]
        base_state["code_revision_count"] = 3
    
        result = supervisor_node(base_state)
    
        assert result.get("code_revision_count") == 0
>       assert result.get("supervisor_verdict") == "ok_continue"
E       AssertionError: assert 'retry_generate_code' == 'ok_continue'
E         
E         - ok_continue
E         + retry_generate_code

tests/integration/supervision/test_supervisor_triggers.py:218: AssertionError
_ TestSupervisorCodeReviewLimit.test_supervisor_handles_code_review_limit_hint_keyword_only _

self = <tests.integration.supervision.test_supervisor_triggers.TestSupervisorCodeReviewLimit object at 0x133901bd0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': 'code_review_limit', ...}

    def test_supervisor_handles_code_review_limit_hint_keyword_only(self, base_state):
        """Just HINT keyword (without PROVIDE_HINT) should also work."""
        from src.agents.supervision.supervisor import supervisor_node
    
        base_state["ask_user_trigger"] = "code_review_limit"
        base_state["user_responses"] = {"Q1": "HINT: Check the wavelength range"}
        base_state["pending_user_questions"] = ["Code review limit reached"]
        base_state["code_revision_count"] = 5
    
        result = supervisor_node(base_state)
    
        assert result.get("code_revision_count") == 0
>       assert result.get("supervisor_verdict") == "ok_continue"
E       AssertionError: assert 'retry_generate_code' == 'ok_continue'
E         
E         - ok_continue
E         + retry_generate_code

tests/integration/supervision/test_supervisor_triggers.py:237: AssertionError
_ TestSupervisorDesignReviewLimit.test_supervisor_handles_design_review_limit_with_hint _

self = <tests.integration.supervision.test_supervisor_triggers.TestSupervisorDesignReviewLimit object at 0x133902d50>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': 'design_review_limit', ...}

    def test_supervisor_handles_design_review_limit_with_hint(self, base_state):
        """User provides HINT - should reset counter and continue."""
        from src.agents.supervision.supervisor import supervisor_node
    
        base_state["ask_user_trigger"] = "design_review_limit"
        base_state["user_responses"] = {
            "Q1": "PROVIDE_HINT: Use a finer mesh near the surface"
        }
        base_state["pending_user_questions"] = ["Design review limit"]
        base_state["design_revision_count"] = 4
    
        result = supervisor_node(base_state)
    
        assert result.get("design_revision_count") == 0
>       assert result.get("supervisor_verdict") == "ok_continue"
E       AssertionError: assert 'retry_design' == 'ok_continue'
E         
E         - ok_continue
E         + retry_design

tests/integration/supervision/test_supervisor_triggers.py:325: AssertionError
_ TestSupervisorExecutionFailureLimit.test_supervisor_handles_execution_failure_with_retry _

self = <tests.integration.supervision.test_supervisor_triggers.TestSupervisorExecutionFailureLimit object at 0x133902ad0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': 'execution_failure_limit', ...}

    def test_supervisor_handles_execution_failure_with_retry(self, base_state):
        """User provides RETRY_WITH_GUIDANCE - should reset counter."""
        from src.agents.supervision.supervisor import supervisor_node
    
        base_state["ask_user_trigger"] = "execution_failure_limit"
        base_state["user_responses"] = {
            "Q1": "RETRY_WITH_GUIDANCE: Increase memory allocation"
        }
        base_state["pending_user_questions"] = ["Execution failed"]
        base_state["execution_failure_count"] = 2
    
        result = supervisor_node(base_state)
    
        assert result.get("execution_failure_count") == 0
>       assert result.get("supervisor_verdict") == "ok_continue"
E       AssertionError: assert 'retry_generate_code' == 'ok_continue'
E         
E         - ok_continue
E         + retry_generate_code

tests/integration/supervision/test_supervisor_triggers.py:377: AssertionError
_ TestSupervisorExecutionFailureLimit.test_supervisor_handles_execution_failure_with_just_retry _

self = <tests.integration.supervision.test_supervisor_triggers.TestSupervisorExecutionFailureLimit object at 0x133902850>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': 'execution_failure_limit', ...}

    def test_supervisor_handles_execution_failure_with_just_retry(self, base_state):
        """Just RETRY keyword should also work."""
        from src.agents.supervision.supervisor import supervisor_node
    
        base_state["ask_user_trigger"] = "execution_failure_limit"
        base_state["user_responses"] = {"Q1": "RETRY"}
        base_state["pending_user_questions"] = ["Execution failed"]
        base_state["execution_failure_count"] = 3
    
        result = supervisor_node(base_state)
    
        assert result.get("execution_failure_count") == 0
>       assert result.get("supervisor_verdict") == "ok_continue"
E       AssertionError: assert 'retry_generate_code' == 'ok_continue'
E         
E         - ok_continue
E         + retry_generate_code

tests/integration/supervision/test_supervisor_triggers.py:393: AssertionError
_ TestSupervisorExecutionFailureLimit.test_supervisor_handles_execution_failure_with_just_guidance _

self = <tests.integration.supervision.test_supervisor_triggers.TestSupervisorExecutionFailureLimit object at 0x1339468b0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': 'execution_failure_limit', ...}

    def test_supervisor_handles_execution_failure_with_just_guidance(self, base_state):
        """Just GUIDANCE keyword should also work."""
        from src.agents.supervision.supervisor import supervisor_node
    
        base_state["ask_user_trigger"] = "execution_failure_limit"
        base_state["user_responses"] = {"Q1": "GUIDANCE: try a smaller grid"}
        base_state["pending_user_questions"] = ["Execution failed"]
        base_state["execution_failure_count"] = 3
    
        result = supervisor_node(base_state)
    
        assert result.get("execution_failure_count") == 0
>       assert result.get("supervisor_verdict") == "ok_continue"
E       AssertionError: assert 'retry_generate_code' == 'ok_continue'
E         
E         - ok_continue
E         + retry_generate_code

tests/integration/supervision/test_supervisor_triggers.py:407: AssertionError
_ TestSupervisorPhysicsFailureLimit.test_supervisor_handles_physics_failure_retry _

self = <tests.integration.supervision.test_supervisor_triggers.TestSupervisorPhysicsFailureLimit object at 0x1339025d0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': 'physics_failure_limit', ...}

    def test_supervisor_handles_physics_failure_retry(self, base_state):
        """User requests RETRY - should reset counter."""
        from src.agents.supervision.supervisor import supervisor_node
    
        base_state["ask_user_trigger"] = "physics_failure_limit"
        base_state["user_responses"] = {"Q1": "RETRY with different parameters"}
        base_state["pending_user_questions"] = ["Physics check failed"]
        base_state["physics_failure_count"] = 2
    
>       result = supervisor_node(base_state)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/integration/supervision/test_supervisor_triggers.py:470: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/agents/supervision/supervisor.py:367: in supervisor_node
    handle_trigger(
src/agents/supervision/trigger_handlers.py:1227: in handle_trigger
    handler(state, result, user_responses, current_stage_id)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': 'physics_failure_limit', ...}
result = {'archive_errors': [], 'physics_failure_count': 0, 'workflow_phase': 'supervision'}
user_responses = {'Q1': 'RETRY with different parameters'}
current_stage_id = None

    def handle_physics_failure_limit(
        state: ReproState,
        result: Dict[str, Any],
        user_responses: Dict[str, str],
        current_stage_id: Optional[str],
    ) -> None:
        """
        Handle physics_failure_limit trigger response.
    
        Options defined in user_options.py: RETRY_WITH_GUIDANCE, ACCEPT_PARTIAL, SKIP_STAGE, STOP
        """
        response_text = parse_user_response(user_responses)
        matched = match_user_response("physics_failure_limit", response_text)
    
        if matched is None:
            result["supervisor_verdict"] = "ask_user"
            result["pending_user_questions"] = [get_clarification_message("physics_failure_limit")]
            return
    
        if matched.action == "retry_with_guidance":
            result["physics_failure_count"] = 0
            raw_response = list(user_responses.values())[-1] if user_responses else ""
            user_guidance = extract_guidance_text(raw_response)
            original_fb = state.get("physics_feedback", "")
            combined_fb = f"{original_fb}\n\nUSER GUIDANCE: {user_guidance}" if original_fb else f"USER GUIDANCE: {user_guidance}"
    
            # Route based on where the physics failure originated
            # Write to the feedback field that the destination node actually reads
            last_node = state.get("last_node_before_ask_user", "")
>           if last_node == "design_review" or "design" in last_node.lower():
                                                           ^^^^^^^^^^^^^^^
E           AttributeError: 'NoneType' object has no attribute 'lower'

src/agents/supervision/trigger_handlers.py:380: AttributeError
_ TestSupervisorPhysicsFailureLimit.test_supervisor_handles_physics_failure_accept_partial _

self = <tests.integration.supervision.test_supervisor_triggers.TestSupervisorPhysicsFailureLimit object at 0x133902350>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': 'physics_failure_limit', ...}

    def test_supervisor_handles_physics_failure_accept_partial(self, base_state):
        """User requests ACCEPT_PARTIAL - should mark as partial success."""
        from src.agents.supervision.supervisor import supervisor_node
    
        base_state["ask_user_trigger"] = "physics_failure_limit"
        base_state["user_responses"] = {"Q1": "ACCEPT_PARTIAL"}
        base_state["pending_user_questions"] = ["Physics check failed"]
        base_state["current_stage_id"] = "stage_0"
        base_state["progress"] = {
            "stages": [{"stage_id": "stage_0", "status": "in_progress"}]
        }
    
        result = supervisor_node(base_state)
    
>       assert result.get("supervisor_verdict") == "ok_continue"
E       AssertionError: assert 'retry_analyze' == 'ok_continue'
E         
E         - ok_continue
E         + retry_analyze

tests/integration/supervision/test_supervisor_triggers.py:489: AssertionError
_ TestSupervisorPhysicsFailureLimit.test_supervisor_handles_physics_failure_partial_keyword _

self = <tests.integration.supervision.test_supervisor_triggers.TestSupervisorPhysicsFailureLimit object at 0x133946b10>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': 'physics_failure_limit', ...}

    def test_supervisor_handles_physics_failure_partial_keyword(self, base_state):
        """Just PARTIAL keyword should also work."""
        from src.agents.supervision.supervisor import supervisor_node
    
        base_state["ask_user_trigger"] = "physics_failure_limit"
        base_state["user_responses"] = {"Q1": "PARTIAL is fine"}
        base_state["pending_user_questions"] = ["Physics check failed"]
        base_state["current_stage_id"] = "stage_0"
        base_state["progress"] = {
            "stages": [{"stage_id": "stage_0", "status": "in_progress"}]
        }
    
        result = supervisor_node(base_state)
    
>       assert result.get("supervisor_verdict") == "ok_continue"
E       AssertionError: assert 'retry_analyze' == 'ok_continue'
E         
E         - ok_continue
E         + retry_analyze

tests/integration/supervision/test_supervisor_triggers.py:505: AssertionError
_ TestSupervisorClarification.test_supervisor_handles_clarification_with_response _

self = <tests.integration.supervision.test_supervisor_triggers.TestSupervisorClarification object at 0x133903c50>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': 'clarification', ...}

    def test_supervisor_handles_clarification_with_response(self, base_state):
        """User provides clarification - should continue with feedback."""
        from src.agents.supervision.supervisor import supervisor_node
    
        base_state["ask_user_trigger"] = "clarification"
        base_state["user_responses"] = {
            "Q1": "The wavelength should be 550nm, not 650nm"
        }
        base_state["pending_user_questions"] = ["Please clarify the target wavelength"]
    
        result = supervisor_node(base_state)
    
        assert result.get("supervisor_verdict") == "ok_continue"
        assert "clarification" in result.get("supervisor_feedback", "").lower()
>       assert "550nm" in result.get("supervisor_feedback", "")
E       AssertionError: assert '550nm' in 'User clarification recorded in user_context'
E        +  where 'User clarification recorded in user_context' = <built-in method get of dict object at 0x1377679c0>('supervisor_feedback', '')
E        +    where <built-in method get of dict object at 0x1377679c0> = {'archive_errors': [], 'ask_user_trigger': None, 'awaiting_user_input': False, 'pending_user_questions': [], ...}.get

tests/integration/supervision/test_supervisor_triggers.py:994: AssertionError
_ TestCodeGeneratorAlternativeKeys.test_code_generator_uses_simulation_code_key _

self = <tests.integration.test_code_agents.TestCodeGeneratorAlternativeKeys object at 0x1349b39d0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}
valid_plan = {'extracted_parameters': [{'name': 'length', 'source': 'text', 'unit': 'nm', 'value': 100}], 'paper_id': 'test_integra... 'stage_0', 'stage_type': 'MATERIAL_VALIDATION', ...}], 'targets': [{'description': 'Test', 'figure_id': 'Fig1'}], ...}

    def test_code_generator_uses_simulation_code_key(self, base_state, valid_plan):
        """Should use 'simulation_code' key if 'code' is missing."""
        from src.agents.code import code_generator_node
    
        expected_code = "import meep as mp\nsim = mp.Simulation()"
        mock_response = {
            "simulation_code": expected_code,  # Alternative key
            "expected_outputs": [],
            "explanation": "Using simulation_code key",
        }
    
        base_state["plan"] = valid_plan
        base_state["current_stage_id"] = "stage_0"
        base_state["current_stage_type"] = "MATERIAL_VALIDATION"
        base_state["design_description"] = {
            "stage_id": "stage_0",
            "design_description": "Full design description for FDTD simulation of gold nanorod",
            "geometry": [{"type": "cylinder"}],
        }
        base_state["validated_materials"] = [{"material_id": "gold"}]
    
        with patch("src.agents.code.call_agent_with_metrics", return_value=mock_response):
            result = code_generator_node(base_state)
    
>       assert result.get("code") == expected_code, (
            f"Should use simulation_code. Expected: {expected_code}, Got: {result.get('code')}"
        )
E       AssertionError: Should use simulation_code. Expected: import meep as mp
E         sim = mp.Simulation(), Got: {
E           "simulation_code": "import meep as mp\nsim = mp.Simulation()",
E           "expected_outputs": [],
E           "explanation": "Using simulation_code key"
E         }
E       assert '{\n  "simula..._code key"\n}' == 'import meep ....Simulation()'
E         
E         - import meep as mp
E         - sim = mp.Simulation()
E         + {
E         +   "simulation_code": "import meep as mp\nsim = mp.Simulation()",
E         +   "expected_outputs": [],
E         +   "explanation": "Using simulation_code key"
E         + }

tests/integration/test_code_agents.py:1390: AssertionError
_ TestDesignReviewerVerdictHandling.test_design_reviewer_needs_revision_verdict_increments_counter _

self = <tests.integration.test_design_agents.TestDesignReviewerVerdictHandling object at 0x134930a50>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_design_reviewer_needs_revision_verdict_increments_counter(self, base_state):
        """Needs revision should increment counter and populate feedback."""
        from src.agents.design import design_reviewer_node
    
        mock_response = {
            "verdict": "needs_revision",
            "issues": [{"severity": "major", "description": "Missing wavelength"}],
            "summary": "Design needs improvements",
            "feedback": "Please add wavelength range specification",
        }
    
        base_state["current_stage_id"] = "stage_0"
        base_state["design_description"] = {"stage_id": "stage_0"}
        base_state["design_revision_count"] = 1
    
        with patch(
            "src.agents.design.call_agent_with_metrics", return_value=mock_response
        ):
            result = design_reviewer_node(base_state)
    
        assert result["last_design_review_verdict"] == "needs_revision"
        assert result["design_revision_count"] == 2  # Incremented from 1
        assert "reviewer_feedback" in result
        # Feedback should contain the actual feedback content
>       assert "wavelength" in result["reviewer_feedback"].lower()
E       AssertionError: assert 'wavelength' in 'design needs improvements'
E        +  where 'design needs improvements' = <built-in method lower of str object at 0x13389ffa0>()
E        +    where <built-in method lower of str object at 0x13389ffa0> = 'Design needs improvements'.lower

tests/integration/test_design_agents.py:83: AssertionError
_ TestDesignReviewerFeedback.test_design_reviewer_populates_feedback_from_feedback_field _

self = <tests.integration.test_design_agents.TestDesignReviewerFeedback object at 0x1349316d0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_design_reviewer_populates_feedback_from_feedback_field(self, base_state):
        """Feedback should be extracted from 'feedback' field when present."""
        from src.agents.design import design_reviewer_node
    
        mock_response = {
            "verdict": "needs_revision",
            "issues": [
                {"severity": "major", "description": "Missing wavelength range"},
            ],
            "summary": "Design needs several improvements",
            "feedback": "The simulation design is missing the wavelength specification. Please add wavelength_range to sources.",
        }
    
        base_state["current_stage_id"] = "stage_0"
        base_state["design_description"] = {"stage_id": "stage_0", "geometry": []}
        base_state["design_revision_count"] = 0
    
        with patch(
            "src.agents.design.call_agent_with_metrics", return_value=mock_response
        ):
            result = design_reviewer_node(base_state)
    
        assert "reviewer_feedback" in result
        # Should use feedback field, not summary
>       assert "wavelength" in result["reviewer_feedback"].lower()
E       AssertionError: assert 'wavelength' in 'design needs several improvements'
E        +  where 'design needs several improvements' = <built-in method lower of str object at 0x13490c260>()
E        +    where <built-in method lower of str object at 0x13490c260> = 'Design needs several improvements'.lower

tests/integration/test_design_agents.py:378: AssertionError
____ TestPromptAdaptorInvariants.test_adapt_prompts_preserves_paper_domain _____

self = <tests.integration.test_invariants.TestPromptAdaptorInvariants object at 0x1338a3650>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_adapt_prompts_preserves_paper_domain(self, base_state):
        """Test that adapt_prompts_node preserves paper domain from response."""
        from src.agents.planning import adapt_prompts_node
    
        mock_response = {"adaptations": [], "paper_domain": "quantum_optics"}
    
        with patch("src.agents.planning.call_agent_with_metrics", return_value=mock_response):
            result = adapt_prompts_node(base_state)
>           assert result.get("paper_domain") == "quantum_optics", \
                f"Expected paper_domain 'quantum_optics', got '{result.get('paper_domain')}'"
E           AssertionError: Expected paper_domain 'quantum_optics', got 'None'
E           assert None == 'quantum_optics'
E            +  where None = <built-in method get of dict object at 0x1374d7e80>('paper_domain')
E            +    where <built-in method get of dict object at 0x1374d7e80> = {'prompt_adaptations': [], 'workflow_phase': 'adapting_prompts'}.get

tests/integration/test_invariants.py:1269: AssertionError
______ TestBuildUserContentForPlanner.test_full_state_all_fields_present _______

self = <tests.llm_client.test_content_builders.TestBuildUserContentForPlanner object at 0x1338dbd90>

    def test_full_state_all_fields_present(self):
        """Test with all fields present - verify exact structure."""
        state = {
            "paper_text": "Full paper content",
            "paper_figures": [{"id": "fig1", "description": "A figure"}],
            "planner_feedback": "Please revise",
        }
        content = build_user_content_for_planner(state)
    
        # Verify exact structure with separators
        parts = content.split("\n\n---\n\n")
>       assert len(parts) == 3, f"Expected 3 parts separated by '---', got {len(parts)}"
E       AssertionError: Expected 3 parts separated by '---', got 5
E       assert 5 == 3
E        +  where 5 = len(['# PAPER TEXT\n\nFull paper content', '# AVAILABLE PAPER FIGURES', 'CRITICAL: Use these exact figure IDs in your plan... attached in order (first image = first ID listed).\n', '- **fig1**: A figure', '# REVISION FEEDBACK\n\nPlease revise'])

tests/llm_client/test_content_builders.py:33: AssertionError
_____________ TestBuildUserContentForPlanner.test_multiple_figures _____________

self = <tests.llm_client.test_content_builders.TestBuildUserContentForPlanner object at 0x1338dbc50>

    def test_multiple_figures(self):
        """Test with multiple figures - verify all are included."""
        state = {
            "paper_text": "Paper",
            "paper_figures": [
                {"id": "fig1", "description": "First figure"},
                {"id": "fig2", "description": "Second figure"},
                {"id": "fig3", "description": "Third figure"},
            ],
        }
        content = build_user_content_for_planner(state)
    
>       assert "- fig1: First figure" in content
E       AssertionError: assert '- fig1: First figure' in '# PAPER TEXT\n\nPaper\n\n---\n\n# AVAILABLE PAPER FIGURES\n\n---\n\nCRITICAL: Use these exact figure IDs in your plan...D listed).\n\n\n---\n\n- **fig1**: First figure\n\n---\n\n- **fig2**: Second figure\n\n---\n\n- **fig3**: Third figure'

tests/llm_client/test_content_builders.py:56: AssertionError
____________ TestBuildUserContentForPlanner.test_figure_without_id _____________

self = <tests.llm_client.test_content_builders.TestBuildUserContentForPlanner object at 0x13497fce0>

    def test_figure_without_id(self):
        """Test figure without id field - should use 'unknown'."""
        state = {
            "paper_text": "Paper",
            "paper_figures": [{"description": "A figure"}],
        }
        content = build_user_content_for_planner(state)
    
>       assert "- unknown: A figure" in content
E       AssertionError: assert '- unknown: A figure' in '# PAPER TEXT\n\nPaper\n\n---\n\n# AVAILABLE PAPER FIGURES\n\n---\n\nCRITICAL: Use these exact figure IDs in your plan targets. Images are attached in order (first image = first ID listed).\n\n\n---\n\n- **Fig1**: A figure'

tests/llm_client/test_content_builders.py:70: AssertionError
________ TestBuildUserContentForPlanner.test_figure_without_description ________

self = <tests.llm_client.test_content_builders.TestBuildUserContentForPlanner object at 0x1353b1220>

    def test_figure_without_description(self):
        """Test figure without description - should use 'No description'."""
        state = {
            "paper_text": "Paper",
            "paper_figures": [{"id": "fig1"}],
        }
        content = build_user_content_for_planner(state)
    
>       assert "- fig1: No description" in content
E       AssertionError: assert '- fig1: No description' in '# PAPER TEXT\n\nPaper\n\n---\n\n# AVAILABLE PAPER FIGURES\n\n---\n\nCRITICAL: Use these exact figure IDs in your plan targets. Images are attached in order (first image = first ID listed).\n\n\n---\n\n- **fig1**: See attached image'

tests/llm_client/test_content_builders.py:80: AssertionError
_____________ TestBuildUserContentForPlanner.test_empty_paper_text _____________

self = <tests.llm_client.test_content_builders.TestBuildUserContentForPlanner object at 0x134aee210>

    def test_empty_paper_text(self):
        """Test with empty paper text - should not include section."""
        state = {
            "paper_text": "",
            "paper_figures": [{"id": "fig1"}],
        }
        content = build_user_content_for_planner(state)
    
        assert "# PAPER TEXT" not in content
>       assert "# FIGURES" in content
E       AssertionError: assert '# FIGURES' in '# AVAILABLE PAPER FIGURES\n\n---\n\nCRITICAL: Use these exact figure IDs in your plan targets. Images are attached in order (first image = first ID listed).\n\n\n---\n\n- **fig1**: See attached image'

tests/llm_client/test_content_builders.py:91: AssertionError
__________ TestBuildUserContentForPlanner.test_missing_paper_text_key __________

self = <tests.llm_client.test_content_builders.TestBuildUserContentForPlanner object at 0x134966850>

    def test_missing_paper_text_key(self):
        """Test with missing paper_text key - should not include section."""
        state = {
            "paper_figures": [{"id": "fig1"}],
        }
        content = build_user_content_for_planner(state)
    
        assert "# PAPER TEXT" not in content
>       assert "# FIGURES" in content
E       AssertionError: assert '# FIGURES' in '# AVAILABLE PAPER FIGURES\n\n---\n\nCRITICAL: Use these exact figure IDs in your plan targets. Images are attached in order (first image = first ID listed).\n\n\n---\n\n- **fig1**: See attached image'

tests/llm_client/test_content_builders.py:123: AssertionError
________ TestBuildUserContentForPlanner.test_figure_with_empty_strings _________

self = <tests.llm_client.test_content_builders.TestBuildUserContentForPlanner object at 0x1338b3050>

    def test_figure_with_empty_strings(self):
        """Test figure with both id and description as empty strings."""
        state = {
            "paper_text": "Paper",
            "paper_figures": [{"id": "", "description": ""}],
        }
        content = build_user_content_for_planner(state)
    
        # Empty string id should show as empty, empty description should show as empty
>       assert "# FIGURES" in content
E       AssertionError: assert '# FIGURES' in '# PAPER TEXT\n\nPaper\n\n---\n\n# AVAILABLE PAPER FIGURES\n\n---\n\nCRITICAL: Use these exact figure IDs in your plan targets. Images are attached in order (first image = first ID listed).\n\n\n---\n\n- ****: '

tests/llm_client/test_content_builders.py:211: AssertionError
______ TestBuildUserContentForPlanner.test_figure_with_only_unknown_keys _______

self = <tests.llm_client.test_content_builders.TestBuildUserContentForPlanner object at 0x134ac4680>

    def test_figure_with_only_unknown_keys(self):
        """Test figure dict with no id or description keys."""
        state = {
            "paper_text": "Paper",
            "paper_figures": [{"unknown_key": "value"}],
        }
        content = build_user_content_for_planner(state)
    
>       assert "# FIGURES" in content
E       AssertionError: assert '# FIGURES' in '# PAPER TEXT\n\nPaper\n\n---\n\n# AVAILABLE PAPER FIGURES\n\n---\n\nCRITICAL: Use these exact figure IDs in your plan targets. Images are attached in order (first image = first ID listed).\n\n\n---\n\n- **Fig1**: See attached image'

tests/llm_client/test_content_builders.py:224: AssertionError
_________ TestBuildUserContentForPlanner.test_non_dict_figure_in_list __________

self = <tests.llm_client.test_content_builders.TestBuildUserContentForPlanner object at 0x134ac5860>

    def test_non_dict_figure_in_list(self):
        """Test figures list containing non-dict items - should handle gracefully or fail clearly."""
        state = {
            "paper_text": "Paper",
            "paper_figures": [
                {"id": "fig1", "description": "Valid figure"},
                "invalid_figure",  # Non-dict item
                {"id": "fig2", "description": "Another valid figure"},
            ],
        }
        # This tests that the code handles mixed types properly
        # If it crashes, that's a bug in the component
        try:
            content = build_user_content_for_planner(state)
            # If it doesn't crash, verify valid figures are included
>           assert "- fig1: Valid figure" in content
E           AssertionError: assert '- fig1: Valid figure' in '# PAPER TEXT\n\nPaper\n\n---\n\n# AVAILABLE PAPER FIGURES\n\n---\n\nCRITICAL: Use these exact figure IDs in your plan...order (first image = first ID listed).\n\n\n---\n\n- **fig1**: Valid figure\n\n---\n\n- **fig2**: Another valid figure'

tests/llm_client/test_content_builders.py:242: AssertionError
________ TestBuildUserContentForPlanner.test_sections_order_consistency ________

self = <tests.llm_client.test_content_builders.TestBuildUserContentForPlanner object at 0x135359bd0>

    def test_sections_order_consistency(self):
        """Test that sections appear in consistent order: paper_text, figures, feedback."""
        state = {
            "paper_text": "Paper text content",
            "paper_figures": [{"id": "fig1", "description": "Figure"}],
            "planner_feedback": "Feedback content",
        }
        content = build_user_content_for_planner(state)
    
        paper_idx = content.index("# PAPER TEXT")
>       figures_idx = content.index("# FIGURES")
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^
E       ValueError: substring not found

tests/llm_client/test_content_builders.py:273: ValueError
_____________ TestBuildUserContentForPlanner.test_separator_format _____________

self = <tests.llm_client.test_content_builders.TestBuildUserContentForPlanner object at 0x134ae5490>

    def test_separator_format(self):
        """Test that the separator between sections is exactly '---' surrounded by double newlines."""
        state = {
            "paper_text": "Paper",
            "paper_figures": [{"id": "fig1", "description": "Fig"}],
        }
        content = build_user_content_for_planner(state)
    
        # Verify the exact separator format
        assert "\n\n---\n\n" in content
        # Count separators - should be exactly 1 between 2 sections
>       assert content.count("\n\n---\n\n") == 1
E       AssertionError: assert 3 == 1
E        +  where 3 = <built-in method count of str object at 0x136fde830>('\n\n---\n\n')
E        +    where <built-in method count of str object at 0x136fde830> = '# PAPER TEXT\n\nPaper\n\n---\n\n# AVAILABLE PAPER FIGURES\n\n---\n\nCRITICAL: Use these exact figure IDs in your plan targets. Images are attached in order (first image = first ID listed).\n\n\n---\n\n- **fig1**: Fig'.count

tests/llm_client/test_content_builders.py:289: AssertionError
_________ TestBuildUserContentForPlanner.test_many_figures_performance _________

self = <tests.llm_client.test_content_builders.TestBuildUserContentForPlanner object at 0x1352dd050>

    def test_many_figures_performance(self):
        """Test with a large number of figures."""
        state = {
            "paper_text": "Paper",
            "paper_figures": [{"id": f"fig{i}", "description": f"Description {i}"} for i in range(100)],
        }
        content = build_user_content_for_planner(state)
    
        # All figures should be included
>       assert "- fig0: Description 0" in content
E       AssertionError: assert '- fig0: Description 0' in '# PAPER TEXT\n\nPaper\n\n---\n\n# AVAILABLE PAPER FIGURES\n\n---\n\nCRITICAL: Use these exact figure IDs in your plan...n 96\n\n---\n\n- **fig97**: Description 97\n\n---\n\n- **fig98**: Description 98\n\n---\n\n- **fig99**: Description 99'

tests/llm_client/test_content_builders.py:300: AssertionError
_______________ TestBuildUserContentForDesigner.test_empty_state _______________

self = <tests.llm_client.test_content_builders.TestBuildUserContentForDesigner object at 0x1353333f0>

    def test_empty_state(self):
        """Test with completely empty state."""
        state = {}
        content = build_user_content_for_designer(state)
    
        assert content.startswith("# CURRENT STAGE: unknown")
>       assert len(content.split("\n\n")) == 1  # Only stage header
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: assert 3 == 1
E        +  where 3 = len(['# CURRENT STAGE: unknown', '## Available Materials (from materials/index.json)\nUse these exact `material_id` values... constant n) | No |\n| constant_air | Air (vacuum approximation) | No |\n| constant_water | Water (constant n) | No |'])
E        +    where ['# CURRENT STAGE: unknown', '## Available Materials (from materials/index.json)\nUse these exact `material_id` values... constant n) | No |\n| constant_air | Air (vacuum approximation) | No |\n| constant_water | Water (constant n) | No |'] = <built-in method split of str object at 0x110a17000>('\n\n')
E        +      where <built-in method split of str object at 0x110a17000> = '# CURRENT STAGE: unknown\n\n## Available Materials (from materials/index.json)\nUse these exact `material_id` values ..., constant n) | No |\n| constant_air | Air (vacuum approximation) | No |\n| constant_water | Water (constant n) | No |'.split

tests/llm_client/test_content_builders.py:490: AssertionError
_____________ TestGetAgentSchema.test_get_agent_schema_supervisor ______________

self = <tests.llm_client.test_schema_loading.TestGetAgentSchema object at 0x13537b390>

    def test_get_agent_schema_supervisor(self):
        """Supervisor schema includes verdict property with correct structure."""
        schema = get_agent_schema("supervisor")
    
        assert schema is not None
        assert isinstance(schema, dict)
        assert schema.get("$id") == "supervisor_output_schema.json"
        assert "verdict" in schema.get("properties", {})
    
        # Verify verdict property structure with exact enum values
        verdict_prop = schema["properties"]["verdict"]
        assert "type" in verdict_prop
        assert verdict_prop["type"] == "string"
        assert "enum" in verdict_prop
        expected_verdicts = ["ok_continue", "replan_needed", "change_priority",
                            "ask_user", "backtrack_to_stage", "all_complete"]
>       assert verdict_prop["enum"] == expected_verdicts
E       AssertionError: assert ['ok_continue...o_stage', ...] == ['ok_continue...all_complete']
E         
E         At index 2 diff: 'replan_with_guidance' != 'change_priority'
E         Left contains 7 more items, first extra item: 'all_complete'
E         Use -v to get more diff

tests/llm_client/test_schema_loading.py:334: AssertionError
________ TestSchemaConsistency.test_supervisor_verdict_enum_is_complete ________

self = <tests.llm_client.test_schema_loading.TestSchemaConsistency object at 0x135398b00>

    def test_supervisor_verdict_enum_is_complete(self):
        """Supervisor verdict enum should contain all expected values."""
        schema = get_agent_schema("supervisor")
        verdict = schema["properties"]["verdict"]
    
        expected_verdicts = [
            "ok_continue",
            "replan_needed",
            "change_priority",
            "ask_user",
            "backtrack_to_stage",
            "all_complete"
        ]
    
>       assert verdict["enum"] == expected_verdicts, (
            f"Supervisor verdict enum mismatch. Expected {expected_verdicts}, got {verdict['enum']}"
        )
E       AssertionError: Supervisor verdict enum mismatch. Expected ['ok_continue', 'replan_needed', 'change_priority', 'ask_user', 'backtrack_to_stage', 'all_complete'], got ['ok_continue', 'replan_needed', 'replan_with_guidance', 'change_priority', 'ask_user', 'backtrack_to_stage', 'all_complete', 'retry_generate_code', 'retry_design', 'retry_code_review', 'retry_design_review', 'retry_plan_review', 'retry_analyze']
E       assert ['ok_continue...o_stage', ...] == ['ok_continue...all_complete']
E         
E         At index 2 diff: 'replan_with_guidance' != 'change_priority'
E         Left contains 7 more items, first extra item: 'all_complete'
E         Use -v to get more diff

tests/llm_client/test_schema_loading.py:1012: AssertionError
__________ TestCodePhase.test_code_review_limit_escalates_to_ask_user __________

self = <tests.state_machine.test_code.TestCodePhase object at 0x135080d60>
initial_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_code_review_limit_escalates_to_ask_user(self, initial_state):
        """
        Test: when code_revision_count hits limit, routes to ask_user.
    
        Sets max_code_revisions=1, so first rejection should escalate.
        """
        state = initial_state.copy()
        runtime_config = {**state.get("runtime_config", {})}
        runtime_config["max_code_revisions"] = 1
        state["runtime_config"] = runtime_config
    
        def mock_llm(*args, **kwargs):
            agent = kwargs.get("agent_name", "unknown")
            print(f"    [LLM] {agent}", flush=True)
    
            if agent == "code_reviewer":
                # Always reject to trigger limit
                return MockLLMResponses.code_reviewer_reject()
    
            responses = {
                "prompt_adaptor": MockLLMResponses.prompt_adaptor(),
                "planner": MockLLMResponses.planner(),
                "plan_reviewer": MockLLMResponses.plan_reviewer_approve(),
                "simulation_designer": MockLLMResponses.simulation_designer(),
                "design_reviewer": MockLLMResponses.design_reviewer_approve(),
                "code_generator": MockLLMResponses.code_generator(),
            }
            return responses.get(agent, {})
    
        mock_ask_user = create_mock_ask_user_node()
    
        with MultiPatch(LLM_PATCH_LOCATIONS, side_effect=mock_llm), MultiPatch(
            CHECKPOINT_PATCH_LOCATIONS, return_value="/tmp/cp.json"
        ), patch("src.agents.user_interaction.ask_user_node", side_effect=mock_ask_user), patch(
            "src.graph.ask_user_node", side_effect=mock_ask_user
        ):
            print("\n" + "=" * 60, flush=True)
            print("TEST: Code Review Limit Escalation", flush=True)
            print("=" * 60, flush=True)
    
            graph = create_repro_graph()
            config = {"configurable": {"thread_id": unique_thread_id("code_limit")}}
    
            print("\n--- Running graph ---", flush=True)
            nodes_visited = []
>           for event in graph.stream(state, config):
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/state_machine/test_code.py:297: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <langgraph.graph.state.CompiledStateGraph object at 0x127df3020>
input = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}
config = {'callbacks': None, 'configurable': {'__pregel_runtime': Runtime(context=None, store=None, stream_writer=<function Pre...ad_id': 'code_limit_8c90b7e8'}, 'metadata': ChainMap({'thread_id': 'code_limit_8c90b7e8'}), 'recursion_limit': 25, ...}
context = None, stream_mode = 'updates', print_mode = ()
output_keys = ['paper_id', 'paper_domain', 'paper_text', 'paper_title', 'paper_citation', 'run_output_dir', ...]
interrupt_before = None, interrupt_after = None, durability = None
subgraphs = False

    def stream(
        self,
        input: InputT | Command | None,
        config: RunnableConfig | None = None,
        *,
        context: ContextT | None = None,
        stream_mode: StreamMode | Sequence[StreamMode] | None = None,
        print_mode: StreamMode | Sequence[StreamMode] = (),
        output_keys: str | Sequence[str] | None = None,
        interrupt_before: All | Sequence[str] | None = None,
        interrupt_after: All | Sequence[str] | None = None,
        durability: Durability | None = None,
        subgraphs: bool = False,
        debug: bool | None = None,
        **kwargs: Unpack[DeprecatedKwargs],
    ) -> Iterator[dict[str, Any] | Any]:
        """Stream graph steps for a single input.
    
        Args:
            input: The input to the graph.
            config: The configuration to use for the run.
            context: The static context to use for the run.
                !!! version-added "Added in version 0.6.0"
            stream_mode: The mode to stream output, defaults to `self.stream_mode`.
    
                Options are:
    
                - `"values"`: Emit all values in the state after each step, including interrupts.
                    When used with functional API, values are emitted once at the end of the workflow.
                - `"updates"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.
                    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.
                - `"custom"`: Emit custom data from inside nodes or tasks using `StreamWriter`.
                - `"messages"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.
                    - Will be emitted as 2-tuples `(LLM token, metadata)`.
                - `"checkpoints"`: Emit an event when a checkpoint is created, in the same format as returned by `get_state()`.
                - `"tasks"`: Emit events when tasks start and finish, including their results and errors.
                - `"debug"`: Emit debug events with as much information as possible for each step.
    
                You can pass a list as the `stream_mode` parameter to stream multiple modes at once.
                The streamed outputs will be tuples of `(mode, data)`.
    
                See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.
            print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes.
    
                Does not affect the output of the graph in any way.
            output_keys: The keys to stream, defaults to all non-context channels.
            interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
            interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
            durability: The durability mode for the graph execution, defaults to `"async"`.
    
                Options are:
    
                - `"sync"`: Changes are persisted synchronously before the next step starts.
                - `"async"`: Changes are persisted asynchronously while the next step executes.
                - `"exit"`: Changes are persisted only when the graph exits.
            subgraphs: Whether to stream events from inside subgraphs, defaults to `False`.
    
                If `True`, the events will be emitted as tuples `(namespace, data)`,
                or `(namespace, mode, data)` if `stream_mode` is a list,
                where `namespace` is a tuple with the path to the node where a subgraph is invoked,
                e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.
    
                See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.
    
        Yields:
            The output of each step in the graph. The output shape depends on the `stream_mode`.
        """
        if (checkpoint_during := kwargs.get("checkpoint_during")) is not None:
            warnings.warn(
                "`checkpoint_during` is deprecated and will be removed. Please use `durability` instead.",
                category=LangGraphDeprecatedSinceV10,
                stacklevel=2,
            )
            if durability is not None:
                raise ValueError(
                    "Cannot use both `checkpoint_during` and `durability` parameters. Please use `durability` instead."
                )
            durability = "async" if checkpoint_during else "exit"
    
        if stream_mode is None:
            # if being called as a node in another graph, default to values mode
            # but don't overwrite stream_mode arg if provided
            stream_mode = (
                "values"
                if config is not None and CONFIG_KEY_TASK_ID in config.get(CONF, {})
                else self.stream_mode
            )
        if debug or self.debug:
            print_mode = ["updates", "values"]
    
        stream = SyncQueue()
    
        config = ensure_config(self.config, config)
        callback_manager = get_callback_manager_for_config(config)
        run_manager = callback_manager.on_chain_start(
            None,
            input,
            name=config.get("run_name", self.get_name()),
            run_id=config.get("run_id"),
        )
        try:
            # assign defaults
            (
                stream_modes,
                output_keys,
                interrupt_before_,
                interrupt_after_,
                checkpointer,
                store,
                cache,
                durability_,
            ) = self._defaults(
                config,
                stream_mode=stream_mode,
                print_mode=print_mode,
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                durability=durability,
            )
            if checkpointer is None and durability is not None:
                warnings.warn(
                    "`durability` has no effect when no checkpointer is present.",
                )
            # set up subgraph checkpointing
            if self.checkpointer is True:
                ns = cast(str, config[CONF][CONFIG_KEY_CHECKPOINT_NS])
                config[CONF][CONFIG_KEY_CHECKPOINT_NS] = recast_checkpoint_ns(ns)
            # set up messages stream mode
            if "messages" in stream_modes:
                ns_ = cast(str | None, config[CONF].get(CONFIG_KEY_CHECKPOINT_NS))
                run_manager.inheritable_handlers.append(
                    StreamMessagesHandler(
                        stream.put,
                        subgraphs,
                        parent_ns=tuple(ns_.split(NS_SEP)) if ns_ else None,
                    )
                )
    
            # set up custom stream mode
            if "custom" in stream_modes:
    
                def stream_writer(c: Any) -> None:
                    stream.put(
                        (
                            tuple(
                                get_config()[CONF][CONFIG_KEY_CHECKPOINT_NS].split(
                                    NS_SEP
                                )[:-1]
                            ),
                            "custom",
                            c,
                        )
                    )
            elif CONFIG_KEY_STREAM in config[CONF]:
                stream_writer = config[CONF][CONFIG_KEY_RUNTIME].stream_writer
            else:
    
                def stream_writer(c: Any) -> None:
                    pass
    
            # set durability mode for subgraphs
            if durability is not None:
                config[CONF][CONFIG_KEY_DURABILITY] = durability_
    
            runtime = Runtime(
                context=_coerce_context(self.context_schema, context),
                store=store,
                stream_writer=stream_writer,
                previous=None,
            )
            parent_runtime = config[CONF].get(CONFIG_KEY_RUNTIME, DEFAULT_RUNTIME)
            runtime = parent_runtime.merge(runtime)
            config[CONF][CONFIG_KEY_RUNTIME] = runtime
    
            with SyncPregelLoop(
                input,
                stream=StreamProtocol(stream.put, stream_modes),
                config=config,
                store=store,
                cache=cache,
                checkpointer=checkpointer,
                nodes=self.nodes,
                specs=self.channels,
                output_keys=output_keys,
                input_keys=self.input_channels,
                stream_keys=self.stream_channels_asis,
                interrupt_before=interrupt_before_,
                interrupt_after=interrupt_after_,
                manager=run_manager,
                durability=durability_,
                trigger_to_nodes=self.trigger_to_nodes,
                migrate_checkpoint=self._migrate_checkpoint,
                retry_policy=self.retry_policy,
                cache_policy=self.cache_policy,
            ) as loop:
                # create runner
                runner = PregelRunner(
                    submit=config[CONF].get(
                        CONFIG_KEY_RUNNER_SUBMIT, weakref.WeakMethod(loop.submit)
                    ),
                    put_writes=weakref.WeakMethod(loop.put_writes),
                    node_finished=config[CONF].get(CONFIG_KEY_NODE_FINISHED),
                )
                # enable subgraph streaming
                if subgraphs:
                    loop.config[CONF][CONFIG_KEY_STREAM] = loop.stream
                # enable concurrent streaming
                get_waiter: Callable[[], concurrent.futures.Future[None]] | None = None
                if (
                    self.stream_eager
                    or subgraphs
                    or "messages" in stream_modes
                    or "custom" in stream_modes
                ):
                    # we are careful to have a single waiter live at any one time
                    # because on exit we increment semaphore count by exactly 1
                    waiter: concurrent.futures.Future | None = None
                    # because sync futures cannot be cancelled, we instead
                    # release the stream semaphore on exit, which will cause
                    # a pending waiter to return immediately
                    loop.stack.callback(stream._count.release)
    
                    def get_waiter() -> concurrent.futures.Future[None]:
                        nonlocal waiter
                        if waiter is None or waiter.done():
                            waiter = loop.submit(stream.wait)
                            return waiter
                        else:
                            return waiter
    
                # Similarly to Bulk Synchronous Parallel / Pregel model
                # computation proceeds in steps, while there are channel updates.
                # Channel updates from step N are only visible in step N+1
                # channels are guaranteed to be immutable for the duration of the step,
                # with channel updates applied only at the transition between steps.
                while loop.tick():
                    for task in loop.match_cached_writes():
                        loop.output_writes(task.id, task.writes, cached=True)
                    for _ in runner.tick(
                        [t for t in loop.tasks.values() if not t.writes],
                        timeout=self.step_timeout,
                        get_waiter=get_waiter,
                        schedule_task=loop.accept_push,
                    ):
                        # emit output
                        yield from _output(
                            stream_mode, print_mode, subgraphs, stream.get, queue.Empty
                        )
                    loop.after_tick()
                    # wait for checkpoint
                    if durability_ == "sync":
                        loop._put_checkpoint_fut.result()
            # emit output
            yield from _output(
                stream_mode, print_mode, subgraphs, stream.get, queue.Empty
            )
            # handle exit
            if loop.status == "out_of_steps":
                msg = create_error_message(
                    message=(
                        f"Recursion limit of {config['recursion_limit']} reached "
                        "without hitting a stop condition. You can increase the "
                        "limit by setting the `recursion_limit` config key."
                    ),
                    error_code=ErrorCode.GRAPH_RECURSION_LIMIT,
                )
>               raise GraphRecursionError(msg)
E               langgraph.errors.GraphRecursionError: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.
E               For troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/GRAPH_RECURSION_LIMIT

/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/langgraph/pregel/main.py:2671: GraphRecursionError
----------------------------- Captured stdout call -----------------------------

============================================================
TEST: Code Review Limit Escalation
============================================================

--- Running graph ---
    [LLM] prompt_adaptor
     adapt_prompts
    [LLM] planner
     planning
    [LLM] plan_reviewer
     plan_review
     select_stage
    [LLM] simulation_designer
     design
    [LLM] design_reviewer
     design_review
    [LLM] code_generator
     generate_code
    [LLM] code_reviewer
     code_review
    [MOCK ask_user] trigger=code_review_limit, questions=1, response='APPROVE'
     ask_user
     supervisor
    [MOCK ask_user] trigger=code_review_limit, questions=1, response='APPROVE'
     ask_user
     supervisor
    [MOCK ask_user] trigger=code_review_limit, questions=1, response='APPROVE'
     ask_user
     supervisor
    [MOCK ask_user] trigger=code_review_limit, questions=1, response='APPROVE'
     ask_user
     supervisor
    [MOCK ask_user] trigger=code_review_limit, questions=1, response='APPROVE'
     ask_user
     supervisor
    [MOCK ask_user] trigger=code_review_limit, questions=1, response='APPROVE'
     ask_user
     supervisor
    [MOCK ask_user] trigger=code_review_limit, questions=1, response='APPROVE'
     ask_user
     supervisor
    [MOCK ask_user] trigger=code_review_limit, questions=1, response='APPROVE'
     ask_user
     supervisor
    [MOCK ask_user] trigger=code_review_limit, questions=1, response='APPROVE'
     ask_user
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=False, empty=True). Code generation must produce valid simulation code.
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
___ TestEdgeCases.test_code_generator_extracts_code_from_simulation_code_key ___

self = <tests.state_machine.test_code.TestEdgeCases object at 0x135028c00>

        def test_code_generator_extracts_code_from_simulation_code_key(self):
            """Code generator should extract code from 'simulation_code' key too."""
            state = {
                "paper_id": "test",
                "paper_text": "Test paper text." * 10,
                "current_stage_id": "stage_0_materials",
                "current_stage_type": "MATERIAL_VALIDATION",
                "design_description": MockLLMResponses.simulation_designer(),
                "validated_materials": [],
                "runtime_config": {},
                "code_revision_count": 0,
            }
    
            long_code = """
    import meep as mp
    import numpy as np
    # Full simulation code here...
    sim = mp.Simulation(resolution=20, cell_size=mp.Vector3(10,10,10))
    sim.run(until=100)
    """ * 3  # Make it long enough
    
            with patch("src.agents.code.call_agent_with_metrics") as mock_llm:
                # Return with 'simulation_code' key instead of 'code'
                mock_llm.return_value = {"simulation_code": long_code, "expected_outputs": []}
                result = code_generator_node(state)
    
            # Should extract from simulation_code
>           assert result.get("code") == long_code, (
                f"Should extract code from simulation_code key"
            )
E           AssertionError: Should extract code from simulation_code key
E           assert '{\n  "simula...tputs": []\n}' == '\nimport mee...(until=100)\n'
E             
E             + {
E             +   "simulation_code": "\nimport meep as mp\nimport numpy as np\n# Full simulation code here...\nsim = mp.Simulation(resolution=20, cell_size=mp.Vector3(10,10,10))\nsim.run(until=100)\n\nimport meep as mp\nimport numpy as np\n# Full simulation code here...\nsim = mp.Simulation(resolution=20, cell_size=mp.Vector3(10,10,10))\nsim.run(until=100)\n\nimport meep as mp\nimport numpy as np\n# Full simulation code here...\nsim = mp.Simulation(resolution=20, cell_size=mp.Vector3(10,10,10))\nsim.run(until=100)\n",
E             +   "expected_outputs": []
E             + }
E             - 
E             - import meep as mp...
E             
E             ...Full output truncated (16 lines hidden), use '-vv' to show

tests/state_machine/test_code.py:949: AssertionError
________ TestFullSingleStageHappyPath.test_full_single_stage_happy_path ________

self = <tests.state_machine.test_full_flow.TestFullSingleStageHappyPath object at 0x13507fd90>
initial_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_full_single_stage_happy_path(self, initial_state):
        """
        Full Happy Path:
        Planning  Design  Code  Execution  Analysis  Supervisor  Report
    
        This test verifies:
        1. Correct node traversal sequence
        2. State values at critical checkpoints
        3. Verdict values set by agents
        4. Progress tracking through stages
        """
        visited_nodes = []
        llm_calls = []
        state_snapshots = {}
    
        def mock_llm(*args, **kwargs):
            agent = kwargs.get("agent_name", "unknown")
            llm_calls.append(f"LLM:{agent}")
            print(f"    [LLM] {agent}")
    
            responses = {
                "prompt_adaptor": MockLLMResponses.prompt_adaptor(),
                "planner": MockLLMResponses.planner(),
                "plan_reviewer": MockLLMResponses.plan_reviewer_approve(),
                "simulation_designer": MockLLMResponses.simulation_designer(),
                "design_reviewer": MockLLMResponses.design_reviewer_approve(),
                "code_generator": MockLLMResponses.code_generator(),
                "code_reviewer": MockLLMResponses.code_reviewer_approve(),
                "execution_validator": MockLLMResponses.execution_validator_pass(),
                "physics_sanity": MockLLMResponses.physics_sanity_pass(),
                "results_analyzer": MockLLMResponses.results_analyzer(),
                "comparison_validator": MockLLMResponses.comparison_validator(),
                "supervisor": MockLLMResponses.supervisor_continue(),
                "report_generator": MockLLMResponses.report_generator(),
            }
    
            if agent == "supervisor":
                supervisor_calls = sum(1 for c in llm_calls if "supervisor" in c)
                print(f"    [DEBUG] Supervisor called {supervisor_calls} times")
                if supervisor_calls == 1:
                    print("    [DEBUG] Returning supervisor_continue (trigger checkpoint)")
                    return MockLLMResponses.supervisor_continue()
                print("    [DEBUG] Returning all_complete")
                return {"verdict": "all_complete", "reasoning": "User approved, done."}
    
            return responses.get(agent, {})
    
        mock_ask_user = create_mock_ask_user_node()
    
        with MultiPatch(LLM_PATCH_LOCATIONS, side_effect=mock_llm), MultiPatch(
            CHECKPOINT_PATCH_LOCATIONS, return_value="/tmp/cp.json"
        ), patch(
            "src.code_runner.run_code_node",
            return_value={"workflow_phase": "running_code", "execution_output": "Success"},
        ), patch("src.agents.user_interaction.ask_user_node", side_effect=mock_ask_user), patch(
            "src.graph.ask_user_node", side_effect=mock_ask_user
        ):
            print("\n" + "=" * 60)
            print("TEST: Full Single-Stage Happy Path")
            print("=" * 60)
    
            graph = create_repro_graph()
            config = {"configurable": {"thread_id": unique_thread_id("happy_path")}}
    
            print("\n--- Running graph ---")
    
            steps = 0
            max_steps = 40
            pending_state = initial_state
            completed = False
    
            while steps < max_steps and not completed:
                interrupted = False
>               for event in graph.stream(pending_state, config):
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/state_machine/test_full_flow.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <langgraph.graph.state.CompiledStateGraph object at 0x134bede10>
input = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}
config = {'callbacks': None, 'configurable': {'__pregel_runtime': Runtime(context=None, store=None, stream_writer=<function Pre...ad_id': 'happy_path_fbdefdba'}, 'metadata': ChainMap({'thread_id': 'happy_path_fbdefdba'}), 'recursion_limit': 25, ...}
context = None, stream_mode = 'updates', print_mode = ()
output_keys = ['paper_id', 'paper_domain', 'paper_text', 'paper_title', 'paper_citation', 'run_output_dir', ...]
interrupt_before = None, interrupt_after = None, durability = None
subgraphs = False

    def stream(
        self,
        input: InputT | Command | None,
        config: RunnableConfig | None = None,
        *,
        context: ContextT | None = None,
        stream_mode: StreamMode | Sequence[StreamMode] | None = None,
        print_mode: StreamMode | Sequence[StreamMode] = (),
        output_keys: str | Sequence[str] | None = None,
        interrupt_before: All | Sequence[str] | None = None,
        interrupt_after: All | Sequence[str] | None = None,
        durability: Durability | None = None,
        subgraphs: bool = False,
        debug: bool | None = None,
        **kwargs: Unpack[DeprecatedKwargs],
    ) -> Iterator[dict[str, Any] | Any]:
        """Stream graph steps for a single input.
    
        Args:
            input: The input to the graph.
            config: The configuration to use for the run.
            context: The static context to use for the run.
                !!! version-added "Added in version 0.6.0"
            stream_mode: The mode to stream output, defaults to `self.stream_mode`.
    
                Options are:
    
                - `"values"`: Emit all values in the state after each step, including interrupts.
                    When used with functional API, values are emitted once at the end of the workflow.
                - `"updates"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.
                    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.
                - `"custom"`: Emit custom data from inside nodes or tasks using `StreamWriter`.
                - `"messages"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.
                    - Will be emitted as 2-tuples `(LLM token, metadata)`.
                - `"checkpoints"`: Emit an event when a checkpoint is created, in the same format as returned by `get_state()`.
                - `"tasks"`: Emit events when tasks start and finish, including their results and errors.
                - `"debug"`: Emit debug events with as much information as possible for each step.
    
                You can pass a list as the `stream_mode` parameter to stream multiple modes at once.
                The streamed outputs will be tuples of `(mode, data)`.
    
                See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.
            print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes.
    
                Does not affect the output of the graph in any way.
            output_keys: The keys to stream, defaults to all non-context channels.
            interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
            interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
            durability: The durability mode for the graph execution, defaults to `"async"`.
    
                Options are:
    
                - `"sync"`: Changes are persisted synchronously before the next step starts.
                - `"async"`: Changes are persisted asynchronously while the next step executes.
                - `"exit"`: Changes are persisted only when the graph exits.
            subgraphs: Whether to stream events from inside subgraphs, defaults to `False`.
    
                If `True`, the events will be emitted as tuples `(namespace, data)`,
                or `(namespace, mode, data)` if `stream_mode` is a list,
                where `namespace` is a tuple with the path to the node where a subgraph is invoked,
                e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.
    
                See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.
    
        Yields:
            The output of each step in the graph. The output shape depends on the `stream_mode`.
        """
        if (checkpoint_during := kwargs.get("checkpoint_during")) is not None:
            warnings.warn(
                "`checkpoint_during` is deprecated and will be removed. Please use `durability` instead.",
                category=LangGraphDeprecatedSinceV10,
                stacklevel=2,
            )
            if durability is not None:
                raise ValueError(
                    "Cannot use both `checkpoint_during` and `durability` parameters. Please use `durability` instead."
                )
            durability = "async" if checkpoint_during else "exit"
    
        if stream_mode is None:
            # if being called as a node in another graph, default to values mode
            # but don't overwrite stream_mode arg if provided
            stream_mode = (
                "values"
                if config is not None and CONFIG_KEY_TASK_ID in config.get(CONF, {})
                else self.stream_mode
            )
        if debug or self.debug:
            print_mode = ["updates", "values"]
    
        stream = SyncQueue()
    
        config = ensure_config(self.config, config)
        callback_manager = get_callback_manager_for_config(config)
        run_manager = callback_manager.on_chain_start(
            None,
            input,
            name=config.get("run_name", self.get_name()),
            run_id=config.get("run_id"),
        )
        try:
            # assign defaults
            (
                stream_modes,
                output_keys,
                interrupt_before_,
                interrupt_after_,
                checkpointer,
                store,
                cache,
                durability_,
            ) = self._defaults(
                config,
                stream_mode=stream_mode,
                print_mode=print_mode,
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                durability=durability,
            )
            if checkpointer is None and durability is not None:
                warnings.warn(
                    "`durability` has no effect when no checkpointer is present.",
                )
            # set up subgraph checkpointing
            if self.checkpointer is True:
                ns = cast(str, config[CONF][CONFIG_KEY_CHECKPOINT_NS])
                config[CONF][CONFIG_KEY_CHECKPOINT_NS] = recast_checkpoint_ns(ns)
            # set up messages stream mode
            if "messages" in stream_modes:
                ns_ = cast(str | None, config[CONF].get(CONFIG_KEY_CHECKPOINT_NS))
                run_manager.inheritable_handlers.append(
                    StreamMessagesHandler(
                        stream.put,
                        subgraphs,
                        parent_ns=tuple(ns_.split(NS_SEP)) if ns_ else None,
                    )
                )
    
            # set up custom stream mode
            if "custom" in stream_modes:
    
                def stream_writer(c: Any) -> None:
                    stream.put(
                        (
                            tuple(
                                get_config()[CONF][CONFIG_KEY_CHECKPOINT_NS].split(
                                    NS_SEP
                                )[:-1]
                            ),
                            "custom",
                            c,
                        )
                    )
            elif CONFIG_KEY_STREAM in config[CONF]:
                stream_writer = config[CONF][CONFIG_KEY_RUNTIME].stream_writer
            else:
    
                def stream_writer(c: Any) -> None:
                    pass
    
            # set durability mode for subgraphs
            if durability is not None:
                config[CONF][CONFIG_KEY_DURABILITY] = durability_
    
            runtime = Runtime(
                context=_coerce_context(self.context_schema, context),
                store=store,
                stream_writer=stream_writer,
                previous=None,
            )
            parent_runtime = config[CONF].get(CONFIG_KEY_RUNTIME, DEFAULT_RUNTIME)
            runtime = parent_runtime.merge(runtime)
            config[CONF][CONFIG_KEY_RUNTIME] = runtime
    
            with SyncPregelLoop(
                input,
                stream=StreamProtocol(stream.put, stream_modes),
                config=config,
                store=store,
                cache=cache,
                checkpointer=checkpointer,
                nodes=self.nodes,
                specs=self.channels,
                output_keys=output_keys,
                input_keys=self.input_channels,
                stream_keys=self.stream_channels_asis,
                interrupt_before=interrupt_before_,
                interrupt_after=interrupt_after_,
                manager=run_manager,
                durability=durability_,
                trigger_to_nodes=self.trigger_to_nodes,
                migrate_checkpoint=self._migrate_checkpoint,
                retry_policy=self.retry_policy,
                cache_policy=self.cache_policy,
            ) as loop:
                # create runner
                runner = PregelRunner(
                    submit=config[CONF].get(
                        CONFIG_KEY_RUNNER_SUBMIT, weakref.WeakMethod(loop.submit)
                    ),
                    put_writes=weakref.WeakMethod(loop.put_writes),
                    node_finished=config[CONF].get(CONFIG_KEY_NODE_FINISHED),
                )
                # enable subgraph streaming
                if subgraphs:
                    loop.config[CONF][CONFIG_KEY_STREAM] = loop.stream
                # enable concurrent streaming
                get_waiter: Callable[[], concurrent.futures.Future[None]] | None = None
                if (
                    self.stream_eager
                    or subgraphs
                    or "messages" in stream_modes
                    or "custom" in stream_modes
                ):
                    # we are careful to have a single waiter live at any one time
                    # because on exit we increment semaphore count by exactly 1
                    waiter: concurrent.futures.Future | None = None
                    # because sync futures cannot be cancelled, we instead
                    # release the stream semaphore on exit, which will cause
                    # a pending waiter to return immediately
                    loop.stack.callback(stream._count.release)
    
                    def get_waiter() -> concurrent.futures.Future[None]:
                        nonlocal waiter
                        if waiter is None or waiter.done():
                            waiter = loop.submit(stream.wait)
                            return waiter
                        else:
                            return waiter
    
                # Similarly to Bulk Synchronous Parallel / Pregel model
                # computation proceeds in steps, while there are channel updates.
                # Channel updates from step N are only visible in step N+1
                # channels are guaranteed to be immutable for the duration of the step,
                # with channel updates applied only at the transition between steps.
                while loop.tick():
                    for task in loop.match_cached_writes():
                        loop.output_writes(task.id, task.writes, cached=True)
                    for _ in runner.tick(
                        [t for t in loop.tasks.values() if not t.writes],
                        timeout=self.step_timeout,
                        get_waiter=get_waiter,
                        schedule_task=loop.accept_push,
                    ):
                        # emit output
                        yield from _output(
                            stream_mode, print_mode, subgraphs, stream.get, queue.Empty
                        )
                    loop.after_tick()
                    # wait for checkpoint
                    if durability_ == "sync":
                        loop._put_checkpoint_fut.result()
            # emit output
            yield from _output(
                stream_mode, print_mode, subgraphs, stream.get, queue.Empty
            )
            # handle exit
            if loop.status == "out_of_steps":
                msg = create_error_message(
                    message=(
                        f"Recursion limit of {config['recursion_limit']} reached "
                        "without hitting a stop condition. You can increase the "
                        "limit by setting the `recursion_limit` config key."
                    ),
                    error_code=ErrorCode.GRAPH_RECURSION_LIMIT,
                )
>               raise GraphRecursionError(msg)
E               langgraph.errors.GraphRecursionError: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.
E               For troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/GRAPH_RECURSION_LIMIT

/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/langgraph/pregel/main.py:2671: GraphRecursionError
----------------------------- Captured stdout call -----------------------------

============================================================
TEST: Full Single-Stage Happy Path
============================================================

--- Running graph ---
    [LLM] prompt_adaptor
  [1]  adapt_prompts
    [LLM] planner
  [2]  planning
    [LLM] plan_reviewer
  [3]  plan_review
  [4]  select_stage
    [LLM] simulation_designer
  [5]  design
    [LLM] design_reviewer
  [6]  design_review
    [LLM] code_generator
  [7]  generate_code
    [LLM] code_reviewer
  [8]  code_review
Warning: Could not set resource limits: current limit exceeds maximum limit
  [9]  run_code
    [LLM] execution_validator
  [10]  execution_check
    [LLM] physics_sanity
  [11]  physics_check
  [12]  analyze
  [13]  comparison_check
  [14]  analyze
  [15]  comparison_check
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
  [16]  ask_user
  [17]  supervisor
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
  [18]  ask_user
  [19]  supervisor
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
  [20]  ask_user
  [21]  supervisor
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
  [22]  ask_user
  [23]  supervisor
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
  [24]  ask_user
  [25]  supervisor
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=False, empty=True). Code generation must produce valid simulation code.
ERROR    src.agents.analysis:analysis.py:164 Stage outputs are empty or missing for stage stage_0_materials. Cannot proceed with analysis without simulation outputs.
ERROR    src.agents.analysis:analysis.py:164 Stage outputs are empty or missing for stage stage_0_materials. Cannot proceed with analysis without simulation outputs.
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
__ TestCodeRevisionLimitEscalation.test_code_revision_limit_triggers_ask_user __

self = <tests.state_machine.test_full_flow.TestCodeRevisionLimitEscalation object at 0x1330282d0>
initial_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_code_revision_limit_triggers_ask_user(self, initial_state):
        """
        When code_revision_count reaches max_code_revisions,
        routing should escalate to ask_user interrupt.
    
        The limit check happens in routing AFTER code_review sets the verdict.
        With max_code_revisions=1:
        - First rejection: count goes 01, but routing checks count >= max, so it escalates
    
        With max_code_revisions=2:
        - First rejection: count goes 01, which is < 2, so continue to generate_code
        - Second rejection: count goes 12, which is >= 2, so escalate
    
        We use max=1 to test the basic escalation flow with minimal loops.
        """
        # Configure limit = 1 for testing (escalate after first rejection)
        state = initial_state.copy()
        state["runtime_config"] = {
            **(state.get("runtime_config") or {}),
            "max_code_revisions": 1,
        }
    
        visited_nodes = []
        code_review_count = 0
    
        def mock_llm(*args, **kwargs):
            nonlocal code_review_count
            agent = kwargs.get("agent_name", "unknown")
    
            if agent == "code_reviewer":
                code_review_count += 1
                # Always reject to hit the limit
                return MockLLMResponses.code_reviewer_reject()
    
            responses = {
                "prompt_adaptor": MockLLMResponses.prompt_adaptor(),
                "planner": MockLLMResponses.planner(),
                "plan_reviewer": MockLLMResponses.plan_reviewer_approve(),
                "simulation_designer": MockLLMResponses.simulation_designer(),
                "design_reviewer": MockLLMResponses.design_reviewer_approve(),
                "code_generator": MockLLMResponses.code_generator(),
            }
            return responses.get(agent, {})
    
        mock_ask_user = create_mock_ask_user_node()
    
        with MultiPatch(LLM_PATCH_LOCATIONS, side_effect=mock_llm), MultiPatch(
            CHECKPOINT_PATCH_LOCATIONS, return_value="/tmp/cp.json"
        ), patch("src.agents.user_interaction.ask_user_node", side_effect=mock_ask_user), patch(
            "src.graph.ask_user_node", side_effect=mock_ask_user
        ):
            print("\n" + "=" * 60)
            print("TEST: Code Revision Limit Triggers ask_user")
            print("=" * 60)
    
            graph = create_repro_graph()
            config = {"configurable": {"thread_id": unique_thread_id("code_limit")}}
    
>           for event in graph.stream(state, config):
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/state_machine/test_full_flow.py:428: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <langgraph.graph.state.CompiledStateGraph object at 0x127df3020>
input = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}
config = {'callbacks': None, 'configurable': {'__pregel_runtime': Runtime(context=None, store=None, stream_writer=<function Pre...ad_id': 'code_limit_b654cb87'}, 'metadata': ChainMap({'thread_id': 'code_limit_b654cb87'}), 'recursion_limit': 25, ...}
context = None, stream_mode = 'updates', print_mode = ()
output_keys = ['paper_id', 'paper_domain', 'paper_text', 'paper_title', 'paper_citation', 'run_output_dir', ...]
interrupt_before = None, interrupt_after = None, durability = None
subgraphs = False

    def stream(
        self,
        input: InputT | Command | None,
        config: RunnableConfig | None = None,
        *,
        context: ContextT | None = None,
        stream_mode: StreamMode | Sequence[StreamMode] | None = None,
        print_mode: StreamMode | Sequence[StreamMode] = (),
        output_keys: str | Sequence[str] | None = None,
        interrupt_before: All | Sequence[str] | None = None,
        interrupt_after: All | Sequence[str] | None = None,
        durability: Durability | None = None,
        subgraphs: bool = False,
        debug: bool | None = None,
        **kwargs: Unpack[DeprecatedKwargs],
    ) -> Iterator[dict[str, Any] | Any]:
        """Stream graph steps for a single input.
    
        Args:
            input: The input to the graph.
            config: The configuration to use for the run.
            context: The static context to use for the run.
                !!! version-added "Added in version 0.6.0"
            stream_mode: The mode to stream output, defaults to `self.stream_mode`.
    
                Options are:
    
                - `"values"`: Emit all values in the state after each step, including interrupts.
                    When used with functional API, values are emitted once at the end of the workflow.
                - `"updates"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.
                    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.
                - `"custom"`: Emit custom data from inside nodes or tasks using `StreamWriter`.
                - `"messages"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.
                    - Will be emitted as 2-tuples `(LLM token, metadata)`.
                - `"checkpoints"`: Emit an event when a checkpoint is created, in the same format as returned by `get_state()`.
                - `"tasks"`: Emit events when tasks start and finish, including their results and errors.
                - `"debug"`: Emit debug events with as much information as possible for each step.
    
                You can pass a list as the `stream_mode` parameter to stream multiple modes at once.
                The streamed outputs will be tuples of `(mode, data)`.
    
                See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.
            print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes.
    
                Does not affect the output of the graph in any way.
            output_keys: The keys to stream, defaults to all non-context channels.
            interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
            interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
            durability: The durability mode for the graph execution, defaults to `"async"`.
    
                Options are:
    
                - `"sync"`: Changes are persisted synchronously before the next step starts.
                - `"async"`: Changes are persisted asynchronously while the next step executes.
                - `"exit"`: Changes are persisted only when the graph exits.
            subgraphs: Whether to stream events from inside subgraphs, defaults to `False`.
    
                If `True`, the events will be emitted as tuples `(namespace, data)`,
                or `(namespace, mode, data)` if `stream_mode` is a list,
                where `namespace` is a tuple with the path to the node where a subgraph is invoked,
                e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.
    
                See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.
    
        Yields:
            The output of each step in the graph. The output shape depends on the `stream_mode`.
        """
        if (checkpoint_during := kwargs.get("checkpoint_during")) is not None:
            warnings.warn(
                "`checkpoint_during` is deprecated and will be removed. Please use `durability` instead.",
                category=LangGraphDeprecatedSinceV10,
                stacklevel=2,
            )
            if durability is not None:
                raise ValueError(
                    "Cannot use both `checkpoint_during` and `durability` parameters. Please use `durability` instead."
                )
            durability = "async" if checkpoint_during else "exit"
    
        if stream_mode is None:
            # if being called as a node in another graph, default to values mode
            # but don't overwrite stream_mode arg if provided
            stream_mode = (
                "values"
                if config is not None and CONFIG_KEY_TASK_ID in config.get(CONF, {})
                else self.stream_mode
            )
        if debug or self.debug:
            print_mode = ["updates", "values"]
    
        stream = SyncQueue()
    
        config = ensure_config(self.config, config)
        callback_manager = get_callback_manager_for_config(config)
        run_manager = callback_manager.on_chain_start(
            None,
            input,
            name=config.get("run_name", self.get_name()),
            run_id=config.get("run_id"),
        )
        try:
            # assign defaults
            (
                stream_modes,
                output_keys,
                interrupt_before_,
                interrupt_after_,
                checkpointer,
                store,
                cache,
                durability_,
            ) = self._defaults(
                config,
                stream_mode=stream_mode,
                print_mode=print_mode,
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                durability=durability,
            )
            if checkpointer is None and durability is not None:
                warnings.warn(
                    "`durability` has no effect when no checkpointer is present.",
                )
            # set up subgraph checkpointing
            if self.checkpointer is True:
                ns = cast(str, config[CONF][CONFIG_KEY_CHECKPOINT_NS])
                config[CONF][CONFIG_KEY_CHECKPOINT_NS] = recast_checkpoint_ns(ns)
            # set up messages stream mode
            if "messages" in stream_modes:
                ns_ = cast(str | None, config[CONF].get(CONFIG_KEY_CHECKPOINT_NS))
                run_manager.inheritable_handlers.append(
                    StreamMessagesHandler(
                        stream.put,
                        subgraphs,
                        parent_ns=tuple(ns_.split(NS_SEP)) if ns_ else None,
                    )
                )
    
            # set up custom stream mode
            if "custom" in stream_modes:
    
                def stream_writer(c: Any) -> None:
                    stream.put(
                        (
                            tuple(
                                get_config()[CONF][CONFIG_KEY_CHECKPOINT_NS].split(
                                    NS_SEP
                                )[:-1]
                            ),
                            "custom",
                            c,
                        )
                    )
            elif CONFIG_KEY_STREAM in config[CONF]:
                stream_writer = config[CONF][CONFIG_KEY_RUNTIME].stream_writer
            else:
    
                def stream_writer(c: Any) -> None:
                    pass
    
            # set durability mode for subgraphs
            if durability is not None:
                config[CONF][CONFIG_KEY_DURABILITY] = durability_
    
            runtime = Runtime(
                context=_coerce_context(self.context_schema, context),
                store=store,
                stream_writer=stream_writer,
                previous=None,
            )
            parent_runtime = config[CONF].get(CONFIG_KEY_RUNTIME, DEFAULT_RUNTIME)
            runtime = parent_runtime.merge(runtime)
            config[CONF][CONFIG_KEY_RUNTIME] = runtime
    
            with SyncPregelLoop(
                input,
                stream=StreamProtocol(stream.put, stream_modes),
                config=config,
                store=store,
                cache=cache,
                checkpointer=checkpointer,
                nodes=self.nodes,
                specs=self.channels,
                output_keys=output_keys,
                input_keys=self.input_channels,
                stream_keys=self.stream_channels_asis,
                interrupt_before=interrupt_before_,
                interrupt_after=interrupt_after_,
                manager=run_manager,
                durability=durability_,
                trigger_to_nodes=self.trigger_to_nodes,
                migrate_checkpoint=self._migrate_checkpoint,
                retry_policy=self.retry_policy,
                cache_policy=self.cache_policy,
            ) as loop:
                # create runner
                runner = PregelRunner(
                    submit=config[CONF].get(
                        CONFIG_KEY_RUNNER_SUBMIT, weakref.WeakMethod(loop.submit)
                    ),
                    put_writes=weakref.WeakMethod(loop.put_writes),
                    node_finished=config[CONF].get(CONFIG_KEY_NODE_FINISHED),
                )
                # enable subgraph streaming
                if subgraphs:
                    loop.config[CONF][CONFIG_KEY_STREAM] = loop.stream
                # enable concurrent streaming
                get_waiter: Callable[[], concurrent.futures.Future[None]] | None = None
                if (
                    self.stream_eager
                    or subgraphs
                    or "messages" in stream_modes
                    or "custom" in stream_modes
                ):
                    # we are careful to have a single waiter live at any one time
                    # because on exit we increment semaphore count by exactly 1
                    waiter: concurrent.futures.Future | None = None
                    # because sync futures cannot be cancelled, we instead
                    # release the stream semaphore on exit, which will cause
                    # a pending waiter to return immediately
                    loop.stack.callback(stream._count.release)
    
                    def get_waiter() -> concurrent.futures.Future[None]:
                        nonlocal waiter
                        if waiter is None or waiter.done():
                            waiter = loop.submit(stream.wait)
                            return waiter
                        else:
                            return waiter
    
                # Similarly to Bulk Synchronous Parallel / Pregel model
                # computation proceeds in steps, while there are channel updates.
                # Channel updates from step N are only visible in step N+1
                # channels are guaranteed to be immutable for the duration of the step,
                # with channel updates applied only at the transition between steps.
                while loop.tick():
                    for task in loop.match_cached_writes():
                        loop.output_writes(task.id, task.writes, cached=True)
                    for _ in runner.tick(
                        [t for t in loop.tasks.values() if not t.writes],
                        timeout=self.step_timeout,
                        get_waiter=get_waiter,
                        schedule_task=loop.accept_push,
                    ):
                        # emit output
                        yield from _output(
                            stream_mode, print_mode, subgraphs, stream.get, queue.Empty
                        )
                    loop.after_tick()
                    # wait for checkpoint
                    if durability_ == "sync":
                        loop._put_checkpoint_fut.result()
            # emit output
            yield from _output(
                stream_mode, print_mode, subgraphs, stream.get, queue.Empty
            )
            # handle exit
            if loop.status == "out_of_steps":
                msg = create_error_message(
                    message=(
                        f"Recursion limit of {config['recursion_limit']} reached "
                        "without hitting a stop condition. You can increase the "
                        "limit by setting the `recursion_limit` config key."
                    ),
                    error_code=ErrorCode.GRAPH_RECURSION_LIMIT,
                )
>               raise GraphRecursionError(msg)
E               langgraph.errors.GraphRecursionError: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.
E               For troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/GRAPH_RECURSION_LIMIT

/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/langgraph/pregel/main.py:2671: GraphRecursionError
----------------------------- Captured stdout call -----------------------------

============================================================
TEST: Code Revision Limit Triggers ask_user
============================================================
   adapt_prompts
   planning
   plan_review
   select_stage
   design
   design_review
   generate_code
   code_review
    [MOCK ask_user] trigger=code_review_limit, questions=1, response='APPROVE'
   ask_user
   supervisor
    [MOCK ask_user] trigger=code_review_limit, questions=1, response='APPROVE'
   ask_user
   supervisor
    [MOCK ask_user] trigger=code_review_limit, questions=1, response='APPROVE'
   ask_user
   supervisor
    [MOCK ask_user] trigger=code_review_limit, questions=1, response='APPROVE'
   ask_user
   supervisor
    [MOCK ask_user] trigger=code_review_limit, questions=1, response='APPROVE'
   ask_user
   supervisor
    [MOCK ask_user] trigger=code_review_limit, questions=1, response='APPROVE'
   ask_user
   supervisor
    [MOCK ask_user] trigger=code_review_limit, questions=1, response='APPROVE'
   ask_user
   supervisor
    [MOCK ask_user] trigger=code_review_limit, questions=1, response='APPROVE'
   ask_user
   supervisor
    [MOCK ask_user] trigger=code_review_limit, questions=1, response='APPROVE'
   ask_user
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=False, empty=True). Code generation must produce valid simulation code.
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
________ TestSupervisorReplanFlow.test_supervisor_replan_routes_to_plan ________

self = <tests.state_machine.test_full_flow.TestSupervisorReplanFlow object at 0x1330287d0>
initial_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_supervisor_replan_routes_to_plan(self, initial_state):
        """
        Supervisor replan_needed verdict should route to plan node.
        """
        visited_nodes = []
        supervisor_count = 0
    
        def mock_llm(*args, **kwargs):
            nonlocal supervisor_count
            agent = kwargs.get("agent_name", "unknown")
    
            if agent == "supervisor":
                supervisor_count += 1
                if supervisor_count == 1:
                    return MockLLMResponses.supervisor_replan()
                return MockLLMResponses.supervisor_continue()
    
            responses = {
                "prompt_adaptor": MockLLMResponses.prompt_adaptor(),
                "planner": MockLLMResponses.planner(),
                "plan_reviewer": MockLLMResponses.plan_reviewer_approve(),
                "simulation_designer": MockLLMResponses.simulation_designer(),
                "design_reviewer": MockLLMResponses.design_reviewer_approve(),
                "code_generator": MockLLMResponses.code_generator(),
                "code_reviewer": MockLLMResponses.code_reviewer_approve(),
                "execution_validator": MockLLMResponses.execution_validator_pass(),
                "physics_sanity": MockLLMResponses.physics_sanity_pass(),
                "results_analyzer": MockLLMResponses.results_analyzer(),
                "comparison_validator": MockLLMResponses.comparison_validator(),
            }
            return responses.get(agent, {})
    
        mock_ask_user = create_mock_ask_user_node()
    
        with MultiPatch(LLM_PATCH_LOCATIONS, side_effect=mock_llm), MultiPatch(
            CHECKPOINT_PATCH_LOCATIONS, return_value="/tmp/cp.json"
        ), patch(
            "src.code_runner.run_code_node",
            return_value={"workflow_phase": "running_code", "execution_output": "Output"},
        ), patch("src.agents.user_interaction.ask_user_node", side_effect=mock_ask_user), patch(
            "src.graph.ask_user_node", side_effect=mock_ask_user
        ):
            print("\n" + "=" * 60)
            print("TEST: Supervisor Replan Routes to Plan")
            print("=" * 60)
    
            graph = create_repro_graph()
            config = {"configurable": {"thread_id": unique_thread_id("replan")}}
    
            steps = 0
            max_steps = 60
            pending_state = initial_state
            found_replan = False
    
            while steps < max_steps and not found_replan:
>               for event in graph.stream(pending_state, config):
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/state_machine/test_full_flow.py:655: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <langgraph.graph.state.CompiledStateGraph object at 0x127df3020>
input = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}
config = {'callbacks': None, 'configurable': {'__pregel_runtime': Runtime(context=None, store=None, stream_writer=<function Pre...), 'thread_id': 'replan_ebb3260b'}, 'metadata': ChainMap({'thread_id': 'replan_ebb3260b'}), 'recursion_limit': 25, ...}
context = None, stream_mode = 'updates', print_mode = ()
output_keys = ['paper_id', 'paper_domain', 'paper_text', 'paper_title', 'paper_citation', 'run_output_dir', ...]
interrupt_before = None, interrupt_after = None, durability = None
subgraphs = False

    def stream(
        self,
        input: InputT | Command | None,
        config: RunnableConfig | None = None,
        *,
        context: ContextT | None = None,
        stream_mode: StreamMode | Sequence[StreamMode] | None = None,
        print_mode: StreamMode | Sequence[StreamMode] = (),
        output_keys: str | Sequence[str] | None = None,
        interrupt_before: All | Sequence[str] | None = None,
        interrupt_after: All | Sequence[str] | None = None,
        durability: Durability | None = None,
        subgraphs: bool = False,
        debug: bool | None = None,
        **kwargs: Unpack[DeprecatedKwargs],
    ) -> Iterator[dict[str, Any] | Any]:
        """Stream graph steps for a single input.
    
        Args:
            input: The input to the graph.
            config: The configuration to use for the run.
            context: The static context to use for the run.
                !!! version-added "Added in version 0.6.0"
            stream_mode: The mode to stream output, defaults to `self.stream_mode`.
    
                Options are:
    
                - `"values"`: Emit all values in the state after each step, including interrupts.
                    When used with functional API, values are emitted once at the end of the workflow.
                - `"updates"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.
                    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.
                - `"custom"`: Emit custom data from inside nodes or tasks using `StreamWriter`.
                - `"messages"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.
                    - Will be emitted as 2-tuples `(LLM token, metadata)`.
                - `"checkpoints"`: Emit an event when a checkpoint is created, in the same format as returned by `get_state()`.
                - `"tasks"`: Emit events when tasks start and finish, including their results and errors.
                - `"debug"`: Emit debug events with as much information as possible for each step.
    
                You can pass a list as the `stream_mode` parameter to stream multiple modes at once.
                The streamed outputs will be tuples of `(mode, data)`.
    
                See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.
            print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes.
    
                Does not affect the output of the graph in any way.
            output_keys: The keys to stream, defaults to all non-context channels.
            interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
            interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
            durability: The durability mode for the graph execution, defaults to `"async"`.
    
                Options are:
    
                - `"sync"`: Changes are persisted synchronously before the next step starts.
                - `"async"`: Changes are persisted asynchronously while the next step executes.
                - `"exit"`: Changes are persisted only when the graph exits.
            subgraphs: Whether to stream events from inside subgraphs, defaults to `False`.
    
                If `True`, the events will be emitted as tuples `(namespace, data)`,
                or `(namespace, mode, data)` if `stream_mode` is a list,
                where `namespace` is a tuple with the path to the node where a subgraph is invoked,
                e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.
    
                See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.
    
        Yields:
            The output of each step in the graph. The output shape depends on the `stream_mode`.
        """
        if (checkpoint_during := kwargs.get("checkpoint_during")) is not None:
            warnings.warn(
                "`checkpoint_during` is deprecated and will be removed. Please use `durability` instead.",
                category=LangGraphDeprecatedSinceV10,
                stacklevel=2,
            )
            if durability is not None:
                raise ValueError(
                    "Cannot use both `checkpoint_during` and `durability` parameters. Please use `durability` instead."
                )
            durability = "async" if checkpoint_during else "exit"
    
        if stream_mode is None:
            # if being called as a node in another graph, default to values mode
            # but don't overwrite stream_mode arg if provided
            stream_mode = (
                "values"
                if config is not None and CONFIG_KEY_TASK_ID in config.get(CONF, {})
                else self.stream_mode
            )
        if debug or self.debug:
            print_mode = ["updates", "values"]
    
        stream = SyncQueue()
    
        config = ensure_config(self.config, config)
        callback_manager = get_callback_manager_for_config(config)
        run_manager = callback_manager.on_chain_start(
            None,
            input,
            name=config.get("run_name", self.get_name()),
            run_id=config.get("run_id"),
        )
        try:
            # assign defaults
            (
                stream_modes,
                output_keys,
                interrupt_before_,
                interrupt_after_,
                checkpointer,
                store,
                cache,
                durability_,
            ) = self._defaults(
                config,
                stream_mode=stream_mode,
                print_mode=print_mode,
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                durability=durability,
            )
            if checkpointer is None and durability is not None:
                warnings.warn(
                    "`durability` has no effect when no checkpointer is present.",
                )
            # set up subgraph checkpointing
            if self.checkpointer is True:
                ns = cast(str, config[CONF][CONFIG_KEY_CHECKPOINT_NS])
                config[CONF][CONFIG_KEY_CHECKPOINT_NS] = recast_checkpoint_ns(ns)
            # set up messages stream mode
            if "messages" in stream_modes:
                ns_ = cast(str | None, config[CONF].get(CONFIG_KEY_CHECKPOINT_NS))
                run_manager.inheritable_handlers.append(
                    StreamMessagesHandler(
                        stream.put,
                        subgraphs,
                        parent_ns=tuple(ns_.split(NS_SEP)) if ns_ else None,
                    )
                )
    
            # set up custom stream mode
            if "custom" in stream_modes:
    
                def stream_writer(c: Any) -> None:
                    stream.put(
                        (
                            tuple(
                                get_config()[CONF][CONFIG_KEY_CHECKPOINT_NS].split(
                                    NS_SEP
                                )[:-1]
                            ),
                            "custom",
                            c,
                        )
                    )
            elif CONFIG_KEY_STREAM in config[CONF]:
                stream_writer = config[CONF][CONFIG_KEY_RUNTIME].stream_writer
            else:
    
                def stream_writer(c: Any) -> None:
                    pass
    
            # set durability mode for subgraphs
            if durability is not None:
                config[CONF][CONFIG_KEY_DURABILITY] = durability_
    
            runtime = Runtime(
                context=_coerce_context(self.context_schema, context),
                store=store,
                stream_writer=stream_writer,
                previous=None,
            )
            parent_runtime = config[CONF].get(CONFIG_KEY_RUNTIME, DEFAULT_RUNTIME)
            runtime = parent_runtime.merge(runtime)
            config[CONF][CONFIG_KEY_RUNTIME] = runtime
    
            with SyncPregelLoop(
                input,
                stream=StreamProtocol(stream.put, stream_modes),
                config=config,
                store=store,
                cache=cache,
                checkpointer=checkpointer,
                nodes=self.nodes,
                specs=self.channels,
                output_keys=output_keys,
                input_keys=self.input_channels,
                stream_keys=self.stream_channels_asis,
                interrupt_before=interrupt_before_,
                interrupt_after=interrupt_after_,
                manager=run_manager,
                durability=durability_,
                trigger_to_nodes=self.trigger_to_nodes,
                migrate_checkpoint=self._migrate_checkpoint,
                retry_policy=self.retry_policy,
                cache_policy=self.cache_policy,
            ) as loop:
                # create runner
                runner = PregelRunner(
                    submit=config[CONF].get(
                        CONFIG_KEY_RUNNER_SUBMIT, weakref.WeakMethod(loop.submit)
                    ),
                    put_writes=weakref.WeakMethod(loop.put_writes),
                    node_finished=config[CONF].get(CONFIG_KEY_NODE_FINISHED),
                )
                # enable subgraph streaming
                if subgraphs:
                    loop.config[CONF][CONFIG_KEY_STREAM] = loop.stream
                # enable concurrent streaming
                get_waiter: Callable[[], concurrent.futures.Future[None]] | None = None
                if (
                    self.stream_eager
                    or subgraphs
                    or "messages" in stream_modes
                    or "custom" in stream_modes
                ):
                    # we are careful to have a single waiter live at any one time
                    # because on exit we increment semaphore count by exactly 1
                    waiter: concurrent.futures.Future | None = None
                    # because sync futures cannot be cancelled, we instead
                    # release the stream semaphore on exit, which will cause
                    # a pending waiter to return immediately
                    loop.stack.callback(stream._count.release)
    
                    def get_waiter() -> concurrent.futures.Future[None]:
                        nonlocal waiter
                        if waiter is None or waiter.done():
                            waiter = loop.submit(stream.wait)
                            return waiter
                        else:
                            return waiter
    
                # Similarly to Bulk Synchronous Parallel / Pregel model
                # computation proceeds in steps, while there are channel updates.
                # Channel updates from step N are only visible in step N+1
                # channels are guaranteed to be immutable for the duration of the step,
                # with channel updates applied only at the transition between steps.
                while loop.tick():
                    for task in loop.match_cached_writes():
                        loop.output_writes(task.id, task.writes, cached=True)
                    for _ in runner.tick(
                        [t for t in loop.tasks.values() if not t.writes],
                        timeout=self.step_timeout,
                        get_waiter=get_waiter,
                        schedule_task=loop.accept_push,
                    ):
                        # emit output
                        yield from _output(
                            stream_mode, print_mode, subgraphs, stream.get, queue.Empty
                        )
                    loop.after_tick()
                    # wait for checkpoint
                    if durability_ == "sync":
                        loop._put_checkpoint_fut.result()
            # emit output
            yield from _output(
                stream_mode, print_mode, subgraphs, stream.get, queue.Empty
            )
            # handle exit
            if loop.status == "out_of_steps":
                msg = create_error_message(
                    message=(
                        f"Recursion limit of {config['recursion_limit']} reached "
                        "without hitting a stop condition. You can increase the "
                        "limit by setting the `recursion_limit` config key."
                    ),
                    error_code=ErrorCode.GRAPH_RECURSION_LIMIT,
                )
>               raise GraphRecursionError(msg)
E               langgraph.errors.GraphRecursionError: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.
E               For troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/GRAPH_RECURSION_LIMIT

/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/langgraph/pregel/main.py:2671: GraphRecursionError
----------------------------- Captured stdout call -----------------------------

============================================================
TEST: Supervisor Replan Routes to Plan
============================================================
  [1]  adapt_prompts
  [2]  planning
  [3]  plan_review
  [4]  select_stage
  [5]  design
  [6]  design_review
  [7]  generate_code
  [8]  code_review
Warning: Could not set resource limits: current limit exceeds maximum limit
  [9]  run_code
  [10]  execution_check
  [11]  physics_check
  [12]  analyze
  [13]  comparison_check
  [14]  analyze
  [15]  comparison_check
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
  [16]  ask_user
  [17]  supervisor
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
  [18]  ask_user
  [19]  supervisor
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
  [20]  ask_user
  [21]  supervisor
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
  [22]  ask_user
  [23]  supervisor
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
  [24]  ask_user
  [25]  supervisor
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=False, empty=True). Code generation must produce valid simulation code.
ERROR    src.agents.analysis:analysis.py:164 Stage outputs are empty or missing for stage stage_0_materials. Cannot proceed with analysis without simulation outputs.
ERROR    src.agents.analysis:analysis.py:164 Stage outputs are empty or missing for stage stage_0_materials. Cannot proceed with analysis without simulation outputs.
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
_________ TestMultiStageCompletion.test_all_complete_routes_to_report __________

self = <tests.state_machine.test_full_flow.TestMultiStageCompletion object at 0x133028cd0>
initial_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_all_complete_routes_to_report(self, initial_state):
        """
        When supervisor returns all_complete verdict, should route to generate_report.
    
        This test verifies the final routing behavior when supervisor decides
        all stages are complete. The key assertion is that the LAST supervisor
        call (with all_complete verdict) routes to generate_report.
        """
        visited_nodes = []
        supervisor_verdicts = []
    
        def mock_llm(*args, **kwargs):
            agent = kwargs.get("agent_name", "unknown")
    
            if agent == "supervisor":
                # Always return all_complete to trigger the routing
                supervisor_verdicts.append("all_complete")
                return {"verdict": "all_complete", "reasoning": "All stages completed successfully"}
    
            responses = {
                "prompt_adaptor": MockLLMResponses.prompt_adaptor(),
                "planner": MockLLMResponses.planner(),
                "plan_reviewer": MockLLMResponses.plan_reviewer_approve(),
                "simulation_designer": MockLLMResponses.simulation_designer(),
                "design_reviewer": MockLLMResponses.design_reviewer_approve(),
                "code_generator": MockLLMResponses.code_generator(),
                "code_reviewer": MockLLMResponses.code_reviewer_approve(),
                "execution_validator": MockLLMResponses.execution_validator_pass(),
                "physics_sanity": MockLLMResponses.physics_sanity_pass(),
                "results_analyzer": MockLLMResponses.results_analyzer(),
                "comparison_validator": MockLLMResponses.comparison_validator(),
                "report_generator": MockLLMResponses.report_generator(),
            }
            return responses.get(agent, {})
    
        # Use mock_ask_user to handle any interrupts
        mock_ask_user = create_mock_ask_user_node()
    
        with MultiPatch(LLM_PATCH_LOCATIONS, side_effect=mock_llm), MultiPatch(
            CHECKPOINT_PATCH_LOCATIONS, return_value="/tmp/cp.json"
        ), patch(
            "src.code_runner.run_code_node",
            return_value={"workflow_phase": "running_code", "execution_output": "Success"},
        ), patch("src.agents.user_interaction.ask_user_node", side_effect=mock_ask_user), patch(
            "src.graph.ask_user_node", side_effect=mock_ask_user
        ):
            print("\n" + "=" * 60)
            print("TEST: All Complete Routes to Report")
            print("=" * 60)
    
            graph = create_repro_graph()
            config = {"configurable": {"thread_id": unique_thread_id("all_complete")}}
    
>           for event in graph.stream(initial_state, config):
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/state_machine/test_full_flow.py:985: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <langgraph.graph.state.CompiledStateGraph object at 0x127df3020>
input = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}
config = {'callbacks': None, 'configurable': {'__pregel_runtime': Runtime(context=None, store=None, stream_writer=<function Pre...d': 'all_complete_66fee3a5'}, 'metadata': ChainMap({'thread_id': 'all_complete_66fee3a5'}), 'recursion_limit': 25, ...}
context = None, stream_mode = 'updates', print_mode = ()
output_keys = ['paper_id', 'paper_domain', 'paper_text', 'paper_title', 'paper_citation', 'run_output_dir', ...]
interrupt_before = None, interrupt_after = None, durability = None
subgraphs = False

    def stream(
        self,
        input: InputT | Command | None,
        config: RunnableConfig | None = None,
        *,
        context: ContextT | None = None,
        stream_mode: StreamMode | Sequence[StreamMode] | None = None,
        print_mode: StreamMode | Sequence[StreamMode] = (),
        output_keys: str | Sequence[str] | None = None,
        interrupt_before: All | Sequence[str] | None = None,
        interrupt_after: All | Sequence[str] | None = None,
        durability: Durability | None = None,
        subgraphs: bool = False,
        debug: bool | None = None,
        **kwargs: Unpack[DeprecatedKwargs],
    ) -> Iterator[dict[str, Any] | Any]:
        """Stream graph steps for a single input.
    
        Args:
            input: The input to the graph.
            config: The configuration to use for the run.
            context: The static context to use for the run.
                !!! version-added "Added in version 0.6.0"
            stream_mode: The mode to stream output, defaults to `self.stream_mode`.
    
                Options are:
    
                - `"values"`: Emit all values in the state after each step, including interrupts.
                    When used with functional API, values are emitted once at the end of the workflow.
                - `"updates"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.
                    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.
                - `"custom"`: Emit custom data from inside nodes or tasks using `StreamWriter`.
                - `"messages"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.
                    - Will be emitted as 2-tuples `(LLM token, metadata)`.
                - `"checkpoints"`: Emit an event when a checkpoint is created, in the same format as returned by `get_state()`.
                - `"tasks"`: Emit events when tasks start and finish, including their results and errors.
                - `"debug"`: Emit debug events with as much information as possible for each step.
    
                You can pass a list as the `stream_mode` parameter to stream multiple modes at once.
                The streamed outputs will be tuples of `(mode, data)`.
    
                See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.
            print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes.
    
                Does not affect the output of the graph in any way.
            output_keys: The keys to stream, defaults to all non-context channels.
            interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
            interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
            durability: The durability mode for the graph execution, defaults to `"async"`.
    
                Options are:
    
                - `"sync"`: Changes are persisted synchronously before the next step starts.
                - `"async"`: Changes are persisted asynchronously while the next step executes.
                - `"exit"`: Changes are persisted only when the graph exits.
            subgraphs: Whether to stream events from inside subgraphs, defaults to `False`.
    
                If `True`, the events will be emitted as tuples `(namespace, data)`,
                or `(namespace, mode, data)` if `stream_mode` is a list,
                where `namespace` is a tuple with the path to the node where a subgraph is invoked,
                e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.
    
                See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.
    
        Yields:
            The output of each step in the graph. The output shape depends on the `stream_mode`.
        """
        if (checkpoint_during := kwargs.get("checkpoint_during")) is not None:
            warnings.warn(
                "`checkpoint_during` is deprecated and will be removed. Please use `durability` instead.",
                category=LangGraphDeprecatedSinceV10,
                stacklevel=2,
            )
            if durability is not None:
                raise ValueError(
                    "Cannot use both `checkpoint_during` and `durability` parameters. Please use `durability` instead."
                )
            durability = "async" if checkpoint_during else "exit"
    
        if stream_mode is None:
            # if being called as a node in another graph, default to values mode
            # but don't overwrite stream_mode arg if provided
            stream_mode = (
                "values"
                if config is not None and CONFIG_KEY_TASK_ID in config.get(CONF, {})
                else self.stream_mode
            )
        if debug or self.debug:
            print_mode = ["updates", "values"]
    
        stream = SyncQueue()
    
        config = ensure_config(self.config, config)
        callback_manager = get_callback_manager_for_config(config)
        run_manager = callback_manager.on_chain_start(
            None,
            input,
            name=config.get("run_name", self.get_name()),
            run_id=config.get("run_id"),
        )
        try:
            # assign defaults
            (
                stream_modes,
                output_keys,
                interrupt_before_,
                interrupt_after_,
                checkpointer,
                store,
                cache,
                durability_,
            ) = self._defaults(
                config,
                stream_mode=stream_mode,
                print_mode=print_mode,
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                durability=durability,
            )
            if checkpointer is None and durability is not None:
                warnings.warn(
                    "`durability` has no effect when no checkpointer is present.",
                )
            # set up subgraph checkpointing
            if self.checkpointer is True:
                ns = cast(str, config[CONF][CONFIG_KEY_CHECKPOINT_NS])
                config[CONF][CONFIG_KEY_CHECKPOINT_NS] = recast_checkpoint_ns(ns)
            # set up messages stream mode
            if "messages" in stream_modes:
                ns_ = cast(str | None, config[CONF].get(CONFIG_KEY_CHECKPOINT_NS))
                run_manager.inheritable_handlers.append(
                    StreamMessagesHandler(
                        stream.put,
                        subgraphs,
                        parent_ns=tuple(ns_.split(NS_SEP)) if ns_ else None,
                    )
                )
    
            # set up custom stream mode
            if "custom" in stream_modes:
    
                def stream_writer(c: Any) -> None:
                    stream.put(
                        (
                            tuple(
                                get_config()[CONF][CONFIG_KEY_CHECKPOINT_NS].split(
                                    NS_SEP
                                )[:-1]
                            ),
                            "custom",
                            c,
                        )
                    )
            elif CONFIG_KEY_STREAM in config[CONF]:
                stream_writer = config[CONF][CONFIG_KEY_RUNTIME].stream_writer
            else:
    
                def stream_writer(c: Any) -> None:
                    pass
    
            # set durability mode for subgraphs
            if durability is not None:
                config[CONF][CONFIG_KEY_DURABILITY] = durability_
    
            runtime = Runtime(
                context=_coerce_context(self.context_schema, context),
                store=store,
                stream_writer=stream_writer,
                previous=None,
            )
            parent_runtime = config[CONF].get(CONFIG_KEY_RUNTIME, DEFAULT_RUNTIME)
            runtime = parent_runtime.merge(runtime)
            config[CONF][CONFIG_KEY_RUNTIME] = runtime
    
            with SyncPregelLoop(
                input,
                stream=StreamProtocol(stream.put, stream_modes),
                config=config,
                store=store,
                cache=cache,
                checkpointer=checkpointer,
                nodes=self.nodes,
                specs=self.channels,
                output_keys=output_keys,
                input_keys=self.input_channels,
                stream_keys=self.stream_channels_asis,
                interrupt_before=interrupt_before_,
                interrupt_after=interrupt_after_,
                manager=run_manager,
                durability=durability_,
                trigger_to_nodes=self.trigger_to_nodes,
                migrate_checkpoint=self._migrate_checkpoint,
                retry_policy=self.retry_policy,
                cache_policy=self.cache_policy,
            ) as loop:
                # create runner
                runner = PregelRunner(
                    submit=config[CONF].get(
                        CONFIG_KEY_RUNNER_SUBMIT, weakref.WeakMethod(loop.submit)
                    ),
                    put_writes=weakref.WeakMethod(loop.put_writes),
                    node_finished=config[CONF].get(CONFIG_KEY_NODE_FINISHED),
                )
                # enable subgraph streaming
                if subgraphs:
                    loop.config[CONF][CONFIG_KEY_STREAM] = loop.stream
                # enable concurrent streaming
                get_waiter: Callable[[], concurrent.futures.Future[None]] | None = None
                if (
                    self.stream_eager
                    or subgraphs
                    or "messages" in stream_modes
                    or "custom" in stream_modes
                ):
                    # we are careful to have a single waiter live at any one time
                    # because on exit we increment semaphore count by exactly 1
                    waiter: concurrent.futures.Future | None = None
                    # because sync futures cannot be cancelled, we instead
                    # release the stream semaphore on exit, which will cause
                    # a pending waiter to return immediately
                    loop.stack.callback(stream._count.release)
    
                    def get_waiter() -> concurrent.futures.Future[None]:
                        nonlocal waiter
                        if waiter is None or waiter.done():
                            waiter = loop.submit(stream.wait)
                            return waiter
                        else:
                            return waiter
    
                # Similarly to Bulk Synchronous Parallel / Pregel model
                # computation proceeds in steps, while there are channel updates.
                # Channel updates from step N are only visible in step N+1
                # channels are guaranteed to be immutable for the duration of the step,
                # with channel updates applied only at the transition between steps.
                while loop.tick():
                    for task in loop.match_cached_writes():
                        loop.output_writes(task.id, task.writes, cached=True)
                    for _ in runner.tick(
                        [t for t in loop.tasks.values() if not t.writes],
                        timeout=self.step_timeout,
                        get_waiter=get_waiter,
                        schedule_task=loop.accept_push,
                    ):
                        # emit output
                        yield from _output(
                            stream_mode, print_mode, subgraphs, stream.get, queue.Empty
                        )
                    loop.after_tick()
                    # wait for checkpoint
                    if durability_ == "sync":
                        loop._put_checkpoint_fut.result()
            # emit output
            yield from _output(
                stream_mode, print_mode, subgraphs, stream.get, queue.Empty
            )
            # handle exit
            if loop.status == "out_of_steps":
                msg = create_error_message(
                    message=(
                        f"Recursion limit of {config['recursion_limit']} reached "
                        "without hitting a stop condition. You can increase the "
                        "limit by setting the `recursion_limit` config key."
                    ),
                    error_code=ErrorCode.GRAPH_RECURSION_LIMIT,
                )
>               raise GraphRecursionError(msg)
E               langgraph.errors.GraphRecursionError: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.
E               For troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/GRAPH_RECURSION_LIMIT

/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/langgraph/pregel/main.py:2671: GraphRecursionError
----------------------------- Captured stdout call -----------------------------

============================================================
TEST: All Complete Routes to Report
============================================================
   adapt_prompts
   planning
   plan_review
   select_stage
   design
   design_review
   generate_code
   code_review
Warning: Could not set resource limits: current limit exceeds maximum limit
   run_code
   execution_check
   physics_check
   analyze
   comparison_check
   analyze
   comparison_check
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
   ask_user
   supervisor
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
   ask_user
   supervisor
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
   ask_user
   supervisor
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
   ask_user
   supervisor
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
   ask_user
   supervisor
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=False, empty=True). Code generation must produce valid simulation code.
ERROR    src.agents.analysis:analysis.py:164 Stage outputs are empty or missing for stage stage_0_materials. Cannot proceed with analysis without simulation outputs.
ERROR    src.agents.analysis:analysis.py:164 Stage outputs are empty or missing for stage stage_0_materials. Cannot proceed with analysis without simulation outputs.
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
___ TestInterruptResumeFlow.test_invoke_pattern_handles_interrupt_correctly ____

self = <tests.state_machine.test_full_flow.TestInterruptResumeFlow object at 0x133028f50>
initial_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_invoke_pattern_handles_interrupt_correctly(self, initial_state):
        """
        Test the invoke() + Command(resume=...) pattern used in runner scripts.
    
        This validates the recommended pattern for handling interrupts with interrupt():
        1. Call invoke() - returns when graph pauses at interrupt() inside ask_user
        2. Check get_state().tasks for interrupt payload
        3. Resume with invoke(Command(resume=user_response))
    
        This pattern is what runner.py uses for human-in-the-loop.
        """
        state = initial_state.copy()
        # Set low limit to trigger interrupt quickly
        state["runtime_config"] = {
            **(state.get("runtime_config") or {}),
            "max_replans": 1,
        }
    
        # Track checkpoint for detecting that graph progressed
        initial_checkpoint_id = {"value": None}
    
        def mock_llm(*args, **kwargs):
            agent = kwargs.get("agent_name", "unknown")
    
            if agent == "plan_reviewer":
                # Always reject to trigger replan limit
                return MockLLMResponses.plan_reviewer_reject()
    
            if agent == "supervisor":
                return MockLLMResponses.supervisor_continue()
    
            responses = {
                "prompt_adaptor": MockLLMResponses.prompt_adaptor(),
                "planner": MockLLMResponses.planner(),
            }
            return responses.get(agent, {})
    
        # Don't mock ask_user_node - we need the real interrupt() to fire
        with MultiPatch(LLM_PATCH_LOCATIONS, side_effect=mock_llm), MultiPatch(
            CHECKPOINT_PATCH_LOCATIONS, return_value="/tmp/cp.json"
        ):
            print("\n" + "=" * 60)
            print("TEST: Invoke Pattern Handles Interrupt (Runner Script Pattern)")
            print("=" * 60)
    
            graph = create_repro_graph()
            config = {"configurable": {"thread_id": unique_thread_id("invoke_pattern")}}
    
            # ============================================================
            # Step 1: Initial invoke - should pause at interrupt() inside ask_user
            # ============================================================
            print("\n--- Step 1: Initial invoke() ---")
            result = graph.invoke(state, config)
    
            # ============================================================
            # Step 2: Check for interrupt payload (the runner.py pattern)
            # With interrupt() inside the node, we check snapshot.tasks for payload
            # ============================================================
            print("\n--- Step 2: Check for interrupt payload ---")
            snapshot = graph.get_state(config)
    
            # Check for interrupt payload from ask_user's interrupt() call
            interrupt_payload = None
            if snapshot.tasks:
                for task in snapshot.tasks:
                    if hasattr(task, 'interrupts') and task.interrupts:
                        interrupt_payload = task.interrupts[0].value
                        break
    
            assert interrupt_payload is not None, (
                f"Graph should have interrupt payload from ask_user's interrupt() call. "
                f"snapshot.next={snapshot.next}"
            )
    
            trigger = interrupt_payload.get("trigger", "unknown")
            questions = interrupt_payload.get("questions", [])
            print(f"  trigger = {trigger}")
            print(f"  questions = {len(questions)}")
    
            # Save checkpoint ID to verify progress
            initial_checkpoint_id["value"] = snapshot.config.get("configurable", {}).get("checkpoint_id")
            print(f"  checkpoint_id = {initial_checkpoint_id['value']}")
    
            # ============================================================
            # Step 3: Resume with user response using Command(resume=...)
            # ============================================================
            print("\n--- Step 3: Resume with user response ---")
            resume_result = graph.invoke(
                Command(resume="APPROVE_PLAN"),
                config
            )
    
            # ============================================================
            # Step 4: Verify the graph progressed (checkpoint changed)
            # ============================================================
            print("\n--- Step 4: Verify resume worked ---")
            post_resume_snapshot = graph.get_state(config)
            post_checkpoint_id = post_resume_snapshot.config.get("configurable", {}).get("checkpoint_id")
    
            # The checkpoint ID should have changed, indicating the graph progressed
            assert post_checkpoint_id != initial_checkpoint_id["value"], (
                "Checkpoint ID should change after resume, indicating graph progressed. "
                f"Before: {initial_checkpoint_id['value']}, After: {post_checkpoint_id}"
            )
            print(f"  checkpoint_id changed: {initial_checkpoint_id['value'][:20]}...  {post_checkpoint_id[:20]}...")
    
            # The user response should be recorded in state
            # Note: With the interrupt() pattern, responses are keyed by the question text
            post_state = post_resume_snapshot.values
            user_responses = post_state.get("user_responses", {})
>           assert len(user_responses) > 0, \
                f"User response should be recorded in state. Got: {user_responses}"
E           AssertionError: User response should be recorded in state. Got: {}
E           assert 0 > 0
E            +  where 0 = len({})

tests/state_machine/test_full_flow.py:1228: AssertionError
----------------------------- Captured stdout call -----------------------------

============================================================
TEST: Invoke Pattern Handles Interrupt (Runner Script Pattern)
============================================================

--- Step 1: Initial invoke() ---

--- Step 2: Check for interrupt payload ---
  trigger = replan_limit
  questions = 1
  checkpoint_id = 1f0d2a4e-d6b4-6cf0-8003-93f0925b34c9

--- Step 3: Resume with user response ---

--- Step 4: Verify resume worked ---
  checkpoint_id changed: 1f0d2a4e-d6b4-6cf0-8...  1f0d2a4e-d6d0-6b58-8...
------------------------------ Captured log call -------------------------------
WARNING  src.agents.design:design.py:222 Design reviewer output missing 'verdict'. Defaulting to 'needs_revision'.
WARNING  src.agents.design:design.py:222 Design reviewer output missing 'verdict'. Defaulting to 'needs_revision'.
WARNING  src.agents.design:design.py:222 Design reviewer output missing 'verdict'. Defaulting to 'needs_revision'.
_ TestSupervisorBacktrackFlow.test_supervisor_backtrack_routes_to_handle_backtrack _

self = <tests.state_machine.test_full_flow.TestSupervisorBacktrackFlow object at 0x133029090>
initial_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_supervisor_backtrack_routes_to_handle_backtrack(self, initial_state):
        """
        Supervisor backtrack_to_stage verdict should route to handle_backtrack node,
        then to select_stage to pick up the backtrack target stage.
        """
        visited_nodes = []
        supervisor_count = 0
    
        def mock_llm(*args, **kwargs):
            nonlocal supervisor_count
            agent = kwargs.get("agent_name", "unknown")
    
            if agent == "supervisor":
                supervisor_count += 1
                if supervisor_count == 1:
                    # First call: request backtrack to same stage
                    return {
                        "verdict": "backtrack_to_stage",
                        "backtrack_target": "stage_0_materials",
                        "reasoning": "Results indicate fundamental parameter error"
                    }
                # Subsequent calls: continue normally
                return MockLLMResponses.supervisor_continue()
    
            responses = {
                "prompt_adaptor": MockLLMResponses.prompt_adaptor(),
                "planner": MockLLMResponses.planner(),
                "plan_reviewer": MockLLMResponses.plan_reviewer_approve(),
                "simulation_designer": MockLLMResponses.simulation_designer(),
                "design_reviewer": MockLLMResponses.design_reviewer_approve(),
                "code_generator": MockLLMResponses.code_generator(),
                "code_reviewer": MockLLMResponses.code_reviewer_approve(),
                "execution_validator": MockLLMResponses.execution_validator_pass(),
                "physics_sanity": MockLLMResponses.physics_sanity_pass(),
                "results_analyzer": MockLLMResponses.results_analyzer(),
                "comparison_validator": MockLLMResponses.comparison_validator(),
            }
            return responses.get(agent, {})
    
        mock_ask_user = create_mock_ask_user_node()
    
        with MultiPatch(LLM_PATCH_LOCATIONS, side_effect=mock_llm), MultiPatch(
            CHECKPOINT_PATCH_LOCATIONS, return_value="/tmp/cp.json"
        ), patch(
            "src.code_runner.run_code_node",
            return_value={"workflow_phase": "running_code", "execution_output": "Output"},
        ), patch("src.agents.user_interaction.ask_user_node", side_effect=mock_ask_user), patch(
            "src.graph.ask_user_node", side_effect=mock_ask_user
        ):
            print("\n" + "=" * 60)
            print("TEST: Supervisor Backtrack Routes to handle_backtrack")
            print("=" * 60)
    
            graph = create_repro_graph()
            config = {"configurable": {"thread_id": unique_thread_id("backtrack")}}
    
            steps = 0
            max_steps = 50
            pending_state = initial_state
            found_backtrack = False
    
            while steps < max_steps:
                interrupted = False
>               for event in graph.stream(pending_state, config):
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/state_machine/test_full_flow.py:1309: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <langgraph.graph.state.CompiledStateGraph object at 0x127df3020>
input = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}
config = {'callbacks': None, 'configurable': {'__pregel_runtime': Runtime(context=None, store=None, stream_writer=<function Pre...read_id': 'backtrack_2678ff4d'}, 'metadata': ChainMap({'thread_id': 'backtrack_2678ff4d'}), 'recursion_limit': 25, ...}
context = None, stream_mode = 'updates', print_mode = ()
output_keys = ['paper_id', 'paper_domain', 'paper_text', 'paper_title', 'paper_citation', 'run_output_dir', ...]
interrupt_before = None, interrupt_after = None, durability = None
subgraphs = False

    def stream(
        self,
        input: InputT | Command | None,
        config: RunnableConfig | None = None,
        *,
        context: ContextT | None = None,
        stream_mode: StreamMode | Sequence[StreamMode] | None = None,
        print_mode: StreamMode | Sequence[StreamMode] = (),
        output_keys: str | Sequence[str] | None = None,
        interrupt_before: All | Sequence[str] | None = None,
        interrupt_after: All | Sequence[str] | None = None,
        durability: Durability | None = None,
        subgraphs: bool = False,
        debug: bool | None = None,
        **kwargs: Unpack[DeprecatedKwargs],
    ) -> Iterator[dict[str, Any] | Any]:
        """Stream graph steps for a single input.
    
        Args:
            input: The input to the graph.
            config: The configuration to use for the run.
            context: The static context to use for the run.
                !!! version-added "Added in version 0.6.0"
            stream_mode: The mode to stream output, defaults to `self.stream_mode`.
    
                Options are:
    
                - `"values"`: Emit all values in the state after each step, including interrupts.
                    When used with functional API, values are emitted once at the end of the workflow.
                - `"updates"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.
                    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.
                - `"custom"`: Emit custom data from inside nodes or tasks using `StreamWriter`.
                - `"messages"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.
                    - Will be emitted as 2-tuples `(LLM token, metadata)`.
                - `"checkpoints"`: Emit an event when a checkpoint is created, in the same format as returned by `get_state()`.
                - `"tasks"`: Emit events when tasks start and finish, including their results and errors.
                - `"debug"`: Emit debug events with as much information as possible for each step.
    
                You can pass a list as the `stream_mode` parameter to stream multiple modes at once.
                The streamed outputs will be tuples of `(mode, data)`.
    
                See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.
            print_mode: Accepts the same values as `stream_mode`, but only prints the output to the console, for debugging purposes.
    
                Does not affect the output of the graph in any way.
            output_keys: The keys to stream, defaults to all non-context channels.
            interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
            interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
            durability: The durability mode for the graph execution, defaults to `"async"`.
    
                Options are:
    
                - `"sync"`: Changes are persisted synchronously before the next step starts.
                - `"async"`: Changes are persisted asynchronously while the next step executes.
                - `"exit"`: Changes are persisted only when the graph exits.
            subgraphs: Whether to stream events from inside subgraphs, defaults to `False`.
    
                If `True`, the events will be emitted as tuples `(namespace, data)`,
                or `(namespace, mode, data)` if `stream_mode` is a list,
                where `namespace` is a tuple with the path to the node where a subgraph is invoked,
                e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.
    
                See [LangGraph streaming guide](https://docs.langchain.com/oss/python/langgraph/streaming) for more details.
    
        Yields:
            The output of each step in the graph. The output shape depends on the `stream_mode`.
        """
        if (checkpoint_during := kwargs.get("checkpoint_during")) is not None:
            warnings.warn(
                "`checkpoint_during` is deprecated and will be removed. Please use `durability` instead.",
                category=LangGraphDeprecatedSinceV10,
                stacklevel=2,
            )
            if durability is not None:
                raise ValueError(
                    "Cannot use both `checkpoint_during` and `durability` parameters. Please use `durability` instead."
                )
            durability = "async" if checkpoint_during else "exit"
    
        if stream_mode is None:
            # if being called as a node in another graph, default to values mode
            # but don't overwrite stream_mode arg if provided
            stream_mode = (
                "values"
                if config is not None and CONFIG_KEY_TASK_ID in config.get(CONF, {})
                else self.stream_mode
            )
        if debug or self.debug:
            print_mode = ["updates", "values"]
    
        stream = SyncQueue()
    
        config = ensure_config(self.config, config)
        callback_manager = get_callback_manager_for_config(config)
        run_manager = callback_manager.on_chain_start(
            None,
            input,
            name=config.get("run_name", self.get_name()),
            run_id=config.get("run_id"),
        )
        try:
            # assign defaults
            (
                stream_modes,
                output_keys,
                interrupt_before_,
                interrupt_after_,
                checkpointer,
                store,
                cache,
                durability_,
            ) = self._defaults(
                config,
                stream_mode=stream_mode,
                print_mode=print_mode,
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                durability=durability,
            )
            if checkpointer is None and durability is not None:
                warnings.warn(
                    "`durability` has no effect when no checkpointer is present.",
                )
            # set up subgraph checkpointing
            if self.checkpointer is True:
                ns = cast(str, config[CONF][CONFIG_KEY_CHECKPOINT_NS])
                config[CONF][CONFIG_KEY_CHECKPOINT_NS] = recast_checkpoint_ns(ns)
            # set up messages stream mode
            if "messages" in stream_modes:
                ns_ = cast(str | None, config[CONF].get(CONFIG_KEY_CHECKPOINT_NS))
                run_manager.inheritable_handlers.append(
                    StreamMessagesHandler(
                        stream.put,
                        subgraphs,
                        parent_ns=tuple(ns_.split(NS_SEP)) if ns_ else None,
                    )
                )
    
            # set up custom stream mode
            if "custom" in stream_modes:
    
                def stream_writer(c: Any) -> None:
                    stream.put(
                        (
                            tuple(
                                get_config()[CONF][CONFIG_KEY_CHECKPOINT_NS].split(
                                    NS_SEP
                                )[:-1]
                            ),
                            "custom",
                            c,
                        )
                    )
            elif CONFIG_KEY_STREAM in config[CONF]:
                stream_writer = config[CONF][CONFIG_KEY_RUNTIME].stream_writer
            else:
    
                def stream_writer(c: Any) -> None:
                    pass
    
            # set durability mode for subgraphs
            if durability is not None:
                config[CONF][CONFIG_KEY_DURABILITY] = durability_
    
            runtime = Runtime(
                context=_coerce_context(self.context_schema, context),
                store=store,
                stream_writer=stream_writer,
                previous=None,
            )
            parent_runtime = config[CONF].get(CONFIG_KEY_RUNTIME, DEFAULT_RUNTIME)
            runtime = parent_runtime.merge(runtime)
            config[CONF][CONFIG_KEY_RUNTIME] = runtime
    
            with SyncPregelLoop(
                input,
                stream=StreamProtocol(stream.put, stream_modes),
                config=config,
                store=store,
                cache=cache,
                checkpointer=checkpointer,
                nodes=self.nodes,
                specs=self.channels,
                output_keys=output_keys,
                input_keys=self.input_channels,
                stream_keys=self.stream_channels_asis,
                interrupt_before=interrupt_before_,
                interrupt_after=interrupt_after_,
                manager=run_manager,
                durability=durability_,
                trigger_to_nodes=self.trigger_to_nodes,
                migrate_checkpoint=self._migrate_checkpoint,
                retry_policy=self.retry_policy,
                cache_policy=self.cache_policy,
            ) as loop:
                # create runner
                runner = PregelRunner(
                    submit=config[CONF].get(
                        CONFIG_KEY_RUNNER_SUBMIT, weakref.WeakMethod(loop.submit)
                    ),
                    put_writes=weakref.WeakMethod(loop.put_writes),
                    node_finished=config[CONF].get(CONFIG_KEY_NODE_FINISHED),
                )
                # enable subgraph streaming
                if subgraphs:
                    loop.config[CONF][CONFIG_KEY_STREAM] = loop.stream
                # enable concurrent streaming
                get_waiter: Callable[[], concurrent.futures.Future[None]] | None = None
                if (
                    self.stream_eager
                    or subgraphs
                    or "messages" in stream_modes
                    or "custom" in stream_modes
                ):
                    # we are careful to have a single waiter live at any one time
                    # because on exit we increment semaphore count by exactly 1
                    waiter: concurrent.futures.Future | None = None
                    # because sync futures cannot be cancelled, we instead
                    # release the stream semaphore on exit, which will cause
                    # a pending waiter to return immediately
                    loop.stack.callback(stream._count.release)
    
                    def get_waiter() -> concurrent.futures.Future[None]:
                        nonlocal waiter
                        if waiter is None or waiter.done():
                            waiter = loop.submit(stream.wait)
                            return waiter
                        else:
                            return waiter
    
                # Similarly to Bulk Synchronous Parallel / Pregel model
                # computation proceeds in steps, while there are channel updates.
                # Channel updates from step N are only visible in step N+1
                # channels are guaranteed to be immutable for the duration of the step,
                # with channel updates applied only at the transition between steps.
                while loop.tick():
                    for task in loop.match_cached_writes():
                        loop.output_writes(task.id, task.writes, cached=True)
                    for _ in runner.tick(
                        [t for t in loop.tasks.values() if not t.writes],
                        timeout=self.step_timeout,
                        get_waiter=get_waiter,
                        schedule_task=loop.accept_push,
                    ):
                        # emit output
                        yield from _output(
                            stream_mode, print_mode, subgraphs, stream.get, queue.Empty
                        )
                    loop.after_tick()
                    # wait for checkpoint
                    if durability_ == "sync":
                        loop._put_checkpoint_fut.result()
            # emit output
            yield from _output(
                stream_mode, print_mode, subgraphs, stream.get, queue.Empty
            )
            # handle exit
            if loop.status == "out_of_steps":
                msg = create_error_message(
                    message=(
                        f"Recursion limit of {config['recursion_limit']} reached "
                        "without hitting a stop condition. You can increase the "
                        "limit by setting the `recursion_limit` config key."
                    ),
                    error_code=ErrorCode.GRAPH_RECURSION_LIMIT,
                )
>               raise GraphRecursionError(msg)
E               langgraph.errors.GraphRecursionError: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.
E               For troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/GRAPH_RECURSION_LIMIT

/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/langgraph/pregel/main.py:2671: GraphRecursionError
----------------------------- Captured stdout call -----------------------------

============================================================
TEST: Supervisor Backtrack Routes to handle_backtrack
============================================================
  [1]  adapt_prompts
  [2]  planning
  [3]  plan_review
  [4]  select_stage
  [5]  design
  [6]  design_review
  [7]  generate_code
  [8]  code_review
Warning: Could not set resource limits: current limit exceeds maximum limit
  [9]  run_code
  [10]  execution_check
  [11]  physics_check
  [12]  analyze
  [13]  comparison_check
  [14]  analyze
  [15]  comparison_check
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
  [16]  ask_user
  [17]  supervisor
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
  [18]  ask_user
  [19]  supervisor
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
  [20]  ask_user
  [21]  supervisor
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
  [22]  ask_user
  [23]  supervisor
    [MOCK ask_user] trigger=analysis_limit, questions=1, response='APPROVE'
  [24]  ask_user
  [25]  supervisor
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=False, empty=True). Code generation must produce valid simulation code.
ERROR    src.agents.analysis:analysis.py:164 Stage outputs are empty or missing for stage stage_0_materials. Cannot proceed with analysis without simulation outputs.
ERROR    src.agents.analysis:analysis.py:164 Stage outputs are empty or missing for stage stage_0_materials. Cannot proceed with analysis without simulation outputs.
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
WARNING  src.agents.user_options:user_options.py:652  ollama not installed, skipping LLM classification (pip install ollama)
__________ TestAdaptPromptsNode.test_adapt_prompts_stores_adaptations __________

self = <tests.state_machine.test_planning.TestAdaptPromptsNode object at 0x13302b610>
initial_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_adapt_prompts_stores_adaptations(self, initial_state):
        """Test: adapt_prompts stores adaptations in state.
    
        Verifies:
        - prompt_adaptations is set in state
        - paper_domain can be updated by prompt_adaptor
        """
    
        def mock_llm(*args, **kwargs):
            agent = kwargs.get("agent_name", "unknown")
    
            if agent == "prompt_adaptor":
                return {
                    "adaptations": [
                        {"type": "domain_specific", "content": "Focus on plasmonic resonance"},
                        {"type": "method_specific", "content": "Use FDTD for near-field"},
                    ],
                    "paper_domain": "plasmonics",
                }
            if agent == "planner":
                return MockLLMResponses.planner()
            if agent == "plan_reviewer":
                return MockLLMResponses.plan_reviewer_approve()
            return {}
    
        with MultiPatch(LLM_PATCH_LOCATIONS, side_effect=mock_llm), MultiPatch(
            CHECKPOINT_PATCH_LOCATIONS, return_value="/tmp/cp.json"
        ):
            print("\n" + "=" * 60)
            print("TEST: Adapt Prompts stores adaptations")
            print("=" * 60)
    
            graph = create_repro_graph()
            config = {"configurable": {"thread_id": unique_thread_id("adapt_prompts")}}
    
            # Stream until after adapt_prompts (wait for plan node)
            adapt_prompts_state, nodes_visited, _ = stream_until_node(
                graph, initial_state, config, "adapt_prompts", break_on_target=False
            )
    
            for node in nodes_visited:
                print(f"   {node}")
    
            # Verify adaptations were stored
            assert adapt_prompts_state is not None
            assert "prompt_adaptations" in adapt_prompts_state, \
                "prompt_adaptations should be in state"
    
            adaptations = adapt_prompts_state["prompt_adaptations"]
            assert isinstance(adaptations, list), "adaptations should be a list"
>           assert len(adaptations) == 2, f"Should have 2 adaptations, got {len(adaptations)}"
E           AssertionError: Should have 2 adaptations, got 0
E           assert 0 == 2
E            +  where 0 = len([])

tests/state_machine/test_planning.py:1306: AssertionError
----------------------------- Captured stdout call -----------------------------

============================================================
TEST: Adapt Prompts stores adaptations
============================================================
   adapt_prompts
   planning
______ TestWhitelistValidation.test_agent_files_use_whitelisted_patterns _______

self = <tests.test_schema_code_consistency.TestWhitelistValidation object at 0x133edee90>

    def test_agent_files_use_whitelisted_patterns(self):
        """All agent files should use only whitelisted access patterns."""
        result = validate_agent_files()
    
        # Filter to only whitelist violations (not schema mismatches)
        whitelist_violations = [
>           v for v in result.violations
                      ^^^^^^^^^^^^^^^^^
            if v.type in (ViolationType.PATTERN_NOT_WHITELISTED, ViolationType.DYNAMIC_KEY)
        ]
E       AttributeError: 'tuple' object has no attribute 'violations'

tests/test_schema_code_consistency.py:58: AttributeError
____ TestWhitelistValidation.test_prompts_module_uses_whitelisted_patterns _____

self = <tests.test_schema_code_consistency.TestWhitelistValidation object at 0x133ede710>

    def test_prompts_module_uses_whitelisted_patterns(self):
        """Prompts module should use only whitelisted patterns for adaptation access."""
        result = validate_prompts_module()
    
        whitelist_violations = [
>           v for v in result.violations
                      ^^^^^^^^^^^^^^^^^
            if v.type in (ViolationType.PATTERN_NOT_WHITELISTED, ViolationType.DYNAMIC_KEY)
        ]
E       AttributeError: 'tuple' object has no attribute 'violations'

tests/test_schema_code_consistency.py:75: AttributeError
_____ TestSchemaFieldConsistency.test_agent_output_fields_exist_in_schema ______

self = <tests.test_schema_code_consistency.TestSchemaFieldConsistency object at 0x133edf250>

    def test_agent_output_fields_exist_in_schema(self):
        """All agent_output field accesses should reference valid schema fields."""
        result = validate_agent_files()
    
        # Filter to only schema field violations
        schema_violations = [
>           v for v in result.violations
                      ^^^^^^^^^^^^^^^^^
            if v.type == ViolationType.FIELD_NOT_IN_SCHEMA
        ]
E       AttributeError: 'tuple' object has no attribute 'violations'

tests/test_schema_code_consistency.py:95: AttributeError
______ TestSchemaFieldConsistency.test_adaptation_fields_exist_in_schema _______

self = <tests.test_schema_code_consistency.TestSchemaFieldConsistency object at 0x133edf390>

    def test_adaptation_fields_exist_in_schema(self):
        """Fields accessed on adaptation objects should exist in schema."""
        result = validate_prompts_module()
    
        schema_violations = [
>           v for v in result.violations
                      ^^^^^^^^^^^^^^^^^
            if v.type == ViolationType.FIELD_NOT_IN_SCHEMA
        ]
E       AttributeError: 'tuple' object has no attribute 'violations'

tests/test_schema_code_consistency.py:112: AttributeError
______________ TestValidatorFunctionality.test_detects_static_get ______________

self = <tests.test_schema_code_consistency.TestValidatorFunctionality object at 0x133edf610>

        def test_detects_static_get(self):
            """Should detect and allow static .get() calls."""
            code = '''
    def test_func():
        value = agent_output.get("verdict")
    '''
            result = extract_field_accesses(code, "test.py")
    
>           assert len(result.field_accesses) == 1
                       ^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'tuple' object has no attribute 'field_accesses'

tests/test_schema_code_consistency.py:134: AttributeError
_______ TestValidatorFunctionality.test_detects_static_get_with_default ________

self = <tests.test_schema_code_consistency.TestValidatorFunctionality object at 0x133edf4d0>

        def test_detects_static_get_with_default(self):
            """Should detect and allow static .get() with default."""
            code = '''
    def test_func():
        value = agent_output.get("verdict", "default")
    '''
            result = extract_field_accesses(code, "test.py")
    
>           assert len(result.field_accesses) == 1
                       ^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'tuple' object has no attribute 'field_accesses'

tests/test_schema_code_consistency.py:149: AttributeError
___________ TestValidatorFunctionality.test_detects_static_in_check ____________

self = <tests.test_schema_code_consistency.TestValidatorFunctionality object at 0x133e22060>

        def test_detects_static_in_check(self):
            """Should detect and allow static 'in' checks."""
            code = '''
    def test_func():
        if "verdict" in agent_output:
            pass
    '''
            result = extract_field_accesses(code, "test.py")
    
>           assert len(result.field_accesses) == 1
                       ^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'tuple' object has no attribute 'field_accesses'

tests/test_schema_code_consistency.py:165: AttributeError
______________ TestValidatorFunctionality.test_flags_dynamic_get _______________

self = <tests.test_schema_code_consistency.TestValidatorFunctionality object at 0x133e222c0>

        def test_flags_dynamic_get(self):
            """Should flag dynamic .get() calls."""
            code = '''
    def test_func():
        key = "verdict"
        value = agent_output.get(key)
    '''
            result = extract_field_accesses(code, "test.py")
    
>           assert len(result.violations) == 1
                       ^^^^^^^^^^^^^^^^^
E           AttributeError: 'tuple' object has no attribute 'violations'

tests/test_schema_code_consistency.py:181: AttributeError
____________ TestValidatorFunctionality.test_flags_subscript_access ____________

self = <tests.test_schema_code_consistency.TestValidatorFunctionality object at 0x133df1490>

        def test_flags_subscript_access(self):
            """Should flag subscript access."""
            code = '''
    def test_func():
        value = agent_output["verdict"]
    '''
            result = extract_field_accesses(code, "test.py")
    
>           assert len(result.violations) == 1
                       ^^^^^^^^^^^^^^^^^
E           AttributeError: 'tuple' object has no attribute 'violations'

tests/test_schema_code_consistency.py:192: AttributeError
_______________ TestValidatorFunctionality.test_flags_iteration ________________

self = <tests.test_schema_code_consistency.TestValidatorFunctionality object at 0x133e3a030>

        def test_flags_iteration(self):
            """Should flag iteration over tracked variables."""
            code = '''
    def test_func():
        for key in agent_output:
            pass
    '''
            result = extract_field_accesses(code, "test.py")
    
>           assert len(result.violations) == 1
                       ^^^^^^^^^^^^^^^^^
E           AttributeError: 'tuple' object has no attribute 'violations'

tests/test_schema_code_consistency.py:205: AttributeError
______________ TestValidatorFunctionality.test_flags_keys_method _______________

self = <tests.test_schema_code_consistency.TestValidatorFunctionality object at 0x133e3a140>

        def test_flags_keys_method(self):
            """Should flag .keys() method."""
            code = '''
    def test_func():
        keys = agent_output.keys()
    '''
            result = extract_field_accesses(code, "test.py")
    
>           assert len(result.violations) == 1
                       ^^^^^^^^^^^^^^^^^
E           AttributeError: 'tuple' object has no attribute 'violations'

tests/test_schema_code_consistency.py:217: AttributeError
______________ TestValidatorFunctionality.test_flags_items_method ______________

self = <tests.test_schema_code_consistency.TestValidatorFunctionality object at 0x133066550>

        def test_flags_items_method(self):
            """Should flag .items() method."""
            code = '''
    def test_func():
        for k, v in agent_output.items():
            pass
    '''
            result = extract_field_accesses(code, "test.py")
    
            # Should have iteration violation AND .items() violation
>           assert len(result.violations) >= 1
                       ^^^^^^^^^^^^^^^^^
E           AttributeError: 'tuple' object has no attribute 'violations'

tests/test_schema_code_consistency.py:231: AttributeError
________________ TestValidatorFunctionality.test_flags_getattr _________________

self = <tests.test_schema_code_consistency.TestValidatorFunctionality object at 0x133066750>

        def test_flags_getattr(self):
            """Should flag getattr() calls."""
            code = '''
    def test_func():
        value = getattr(agent_output, "verdict")
    '''
            result = extract_field_accesses(code, "test.py")
    
>           assert len(result.violations) == 1
                       ^^^^^^^^^^^^^^^^^
E           AttributeError: 'tuple' object has no attribute 'violations'

tests/test_schema_code_consistency.py:242: AttributeError
________ TestValidatorFunctionality.test_ignores_non_tracked_variables _________

self = <tests.test_schema_code_consistency.TestValidatorFunctionality object at 0x1330ba120>

        def test_ignores_non_tracked_variables(self):
            """Should ignore access patterns on non-tracked variables."""
            code = '''
    def test_func():
        other_dict = {"a": 1}
        value = other_dict["key"]
        keys = other_dict.keys()
        for k in other_dict:
            pass
    '''
            result = extract_field_accesses(code, "test.py")
    
            # No violations because other_dict is not tracked
>           assert len(result.violations) == 0
                       ^^^^^^^^^^^^^^^^^
E           AttributeError: 'tuple' object has no attribute 'violations'

tests/test_schema_code_consistency.py:259: AttributeError
___________ TestValidatorFunctionality.test_tracks_function_context ____________

self = <tests.test_schema_code_consistency.TestValidatorFunctionality object at 0x1330baa80>

        def test_tracks_function_context(self):
            """Should track which function the access is in."""
            code = '''
    def my_function():
        value = agent_output.get("verdict")
    
    def other_function():
        value = agent_output.get("summary")
    '''
            result = extract_field_accesses(code, "test.py")
    
>           assert len(result.field_accesses) == 2
                       ^^^^^^^^^^^^^^^^^^^^^
E           AttributeError: 'tuple' object has no attribute 'field_accesses'

tests/test_schema_code_consistency.py:273: AttributeError
__________________ TestIntegration.test_all_schemas_loadable ___________________

self = <tests.test_schema_code_consistency.TestIntegration object at 0x133edf9d0>

    def test_all_schemas_loadable(self):
        """All referenced schemas should be loadable."""
        from tools.validate_schema_access import AGENT_OUTPUT_SCHEMA_MAPPING
    
        for file_schemas in AGENT_OUTPUT_SCHEMA_MAPPING.values():
            for schema_file in file_schemas.values():
>               schema_path = SCHEMAS_DIR / schema_file
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
E               TypeError: unsupported operand type(s) for /: 'PosixPath' and 'AgentSchemaInfo'

tests/test_schema_code_consistency.py:337: TypeError
___________ TestIntegration.test_full_validation_runs_without_error ____________

self = <tests.test_schema_code_consistency.TestIntegration object at 0x133e235c0>

    def test_full_validation_runs_without_error(self):
        """Full validation should run without crashing."""
        result = validate_agent_files()
    
>       assert result.files_scanned > 0, "Should have scanned at least one file"
               ^^^^^^^^^^^^^^^^^^^^
E       AttributeError: 'tuple' object has no attribute 'files_scanned'

tests/test_schema_code_consistency.py:354: AttributeError
_________________ TestPlanReviewPhase.test_plan_review_approve _________________

self = <test_full_workflow_success.TestPlanReviewPhase object at 0x134c78f50>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_plan_review_approve(self, base_state):
        """Test plan review approval with proper verdict."""
        base_state["plan"] = MockResponseFactory.planner_response()
    
        with patch("src.agents.planning.call_agent_with_metrics") as mock_llm:
            mock_llm.return_value = MockResponseFactory.reviewer_approve()
    
            result = plan_reviewer_node(base_state)
    
>           assert result["last_plan_review_verdict"] == "approve"
E           AssertionError: assert 'needs_revision' == 'approve'
E             
E             - approve
E             + needs_revision

tests/workflow/test_full_workflow_success.py:158: AssertionError
__________ TestPlanReviewPhase.test_plan_review_verdict_normalization __________

self = <test_full_workflow_success.TestPlanReviewPhase object at 0x13310eea0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_plan_review_verdict_normalization(self, base_state):
        """Test that various verdict strings are normalized correctly."""
        base_state["plan"] = MockResponseFactory.planner_response()
    
        # Test "pass" -> "approve"
        with patch("src.agents.planning.call_agent_with_metrics") as mock_llm:
            mock_llm.return_value = {"verdict": "pass", "issues": [], "summary": "ok"}
            result = plan_reviewer_node(base_state)
>           assert result["last_plan_review_verdict"] == "approve"
E           AssertionError: assert 'needs_revision' == 'approve'
E             
E             - approve
E             + needs_revision

tests/workflow/test_full_workflow_success.py:192: AssertionError
_____ TestCodeGenerator.test_code_generator_stub_code_increments_revision ______

self = <test_full_workflow_success.TestCodeGenerator object at 0x133176150>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_code_generator_stub_code_increments_revision(self, base_state):
        """Test that stub code output increments revision counter."""
        base_state["current_stage_id"] = "stage_1_extinction"
        base_state["current_stage_type"] = "SINGLE_STRUCTURE"
        base_state["design_description"] = MockResponseFactory.designer_response()
        base_state["validated_materials"] = [
            {"material_id": "gold", "path": "/materials/Au.csv"}
        ]
        base_state["code_revision_count"] = 0
    
        with patch("src.agents.code.call_agent_with_metrics") as mock_llm:
            mock_llm.return_value = {"code": "# STUB - Code would go here"}
    
            result = code_generator_node(base_state)
    
>           assert result.get("code_revision_count", 0) >= 1
E           AssertionError: assert 0 >= 1
E            +  where 0 = <built-in method get of dict object at 0x137e42e00>('code_revision_count', 0)
E            +    where <built-in method get of dict object at 0x137e42e00> = {'code': '# STUB - Code would go here', 'reviewer_feedback': 'ERROR: Generated code is empty or contains stub markers....roduce valid Meep simulation code. Please regenerate with proper implementation.', 'workflow_phase': 'code_generation'}.get

tests/workflow/test_full_workflow_success.py:741: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=True, empty=True). Code generation must produce valid simulation code.
_______________ TestFullWorkflowSuccess.test_plan_review_approve _______________

self = <test_full_workflow_success.TestFullWorkflowSuccess object at 0x134c79f90>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_plan_review_approve(self, base_state):
        """Test plan review approval."""
        # Set up state with plan
        base_state["plan"] = MockResponseFactory.planner_response()
    
        with patch("src.agents.planning.call_agent_with_metrics") as mock_llm:
            mock_llm.return_value = MockResponseFactory.reviewer_approve()
    
            result = plan_reviewer_node(base_state)
    
>           assert result["last_plan_review_verdict"] == "approve"
E           AssertionError: assert 'needs_revision' == 'approve'
E             
E             - approve
E             + needs_revision

tests/workflow/test_full_workflow_success.py:865: AssertionError
________ TestSupervisorTriggerHandling.test_code_review_limit_with_hint ________

self = <test_multi_stage_and_supervisor.TestSupervisorTriggerHandling object at 0x134ccd6e0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': 'code_review_limit', ...}

    def test_code_review_limit_with_hint(self, base_state):
        """Test code_review_limit trigger with user hint."""
        plan = MockResponseFactory.planner_response()
        base_state["plan"] = plan
        base_state["progress"] = deepcopy(plan["progress"])
        base_state["ask_user_trigger"] = "code_review_limit"
        base_state["user_responses"] = {"q1": "PROVIDE_HINT: try using meep.Vector3"}
        base_state["code_revision_count"] = 5  # At limit
    
        result = supervisor_node(base_state)
    
>       assert result["supervisor_verdict"] == "ok_continue"
E       AssertionError: assert 'retry_generate_code' == 'ok_continue'
E         
E         - ok_continue
E         + retry_generate_code

tests/workflow/test_multi_stage_and_supervisor.py:498: AssertionError
_______ TestSupervisorTriggerHandling.test_design_review_limit_with_hint _______

self = <test_multi_stage_and_supervisor.TestSupervisorTriggerHandling object at 0x1331af8a0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': 'design_review_limit', ...}

    def test_design_review_limit_with_hint(self, base_state):
        """Test design_review_limit trigger with hint."""
        base_state["ask_user_trigger"] = "design_review_limit"
        base_state["user_responses"] = {"q1": "HINT: increase resolution to 4nm"}
    
        result = supervisor_node(base_state)
    
>       assert result["supervisor_verdict"] == "ok_continue"
E       AssertionError: assert 'retry_design' == 'ok_continue'
E         
E         - ok_continue
E         + retry_design

tests/workflow/test_multi_stage_and_supervisor.py:536: AssertionError
_______ TestSupervisorTriggerHandling.test_execution_failure_limit_retry _______

self = <test_multi_stage_and_supervisor.TestSupervisorTriggerHandling object at 0x133176550>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': 'execution_failure_limit', ...}

    def test_execution_failure_limit_retry(self, base_state):
        """Test execution_failure_limit trigger with retry."""
        base_state["ask_user_trigger"] = "execution_failure_limit"
        base_state["user_responses"] = {"q1": "RETRY_WITH_GUIDANCE: check memory allocation"}
        base_state["execution_failure_count"] = 3
    
        result = supervisor_node(base_state)
    
>       assert result["supervisor_verdict"] == "ok_continue"
E       AssertionError: assert 'retry_generate_code' == 'ok_continue'
E         
E         - ok_continue
E         + retry_generate_code

tests/workflow/test_multi_stage_and_supervisor.py:547: AssertionError
___ TestSupervisorTriggerHandling.test_physics_failure_limit_accept_partial ____

self = <test_multi_stage_and_supervisor.TestSupervisorTriggerHandling object at 0x133176750>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': 'physics_failure_limit', ...}

    def test_physics_failure_limit_accept_partial(self, base_state):
        """Test physics_failure_limit trigger with accept partial."""
        plan = MockResponseFactory.planner_response()
        base_state["plan"] = plan
        base_state["progress"] = deepcopy(plan["progress"])
        base_state["ask_user_trigger"] = "physics_failure_limit"
        base_state["user_responses"] = {"q1": "ACCEPT_PARTIAL - results are good enough"}
        base_state["current_stage_id"] = "stage_0_materials"
    
        result = supervisor_node(base_state)
    
>       assert result["supervisor_verdict"] == "ok_continue"
E       AssertionError: assert 'retry_analyze' == 'ok_continue'
E         
E         - ok_continue
E         + retry_analyze

tests/workflow/test_multi_stage_and_supervisor.py:561: AssertionError
_____ TestSupervisorEdgeCases.test_supervisor_backtrack_decision_from_llm ______

self = <test_multi_stage_and_supervisor.TestSupervisorEdgeCases object at 0x1331afac0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_supervisor_backtrack_decision_from_llm(self, base_state):
        """Test supervisor sets backtrack_decision from LLM response."""
        plan = MockResponseFactory.planner_response()
        base_state["plan"] = plan
        base_state["progress"] = deepcopy(plan["progress"])
        base_state["current_stage_id"] = "stage_1_extinction"
    
        with patch("src.agents.supervision.supervisor.call_agent_with_metrics") as mock_llm:
            mock_llm.return_value = {
                "verdict": "backtrack_to_stage",
                "backtrack_target": "stage_0_materials",
                "reasoning": "Material properties incorrect",
            }
    
            result = supervisor_node(base_state)
    
        assert result["supervisor_verdict"] == "backtrack_to_stage"
>       assert result.get("backtrack_decision") is not None
E       AssertionError: assert None is not None
E        +  where None = <built-in method get of dict object at 0x1674305c0>('backtrack_decision')
E        +    where <built-in method get of dict object at 0x1674305c0> = {'archive_errors': [], 'supervisor_feedback': '', 'supervisor_verdict': 'backtrack_to_stage', 'workflow_phase': 'supervision'}.get

tests/workflow/test_multi_stage_and_supervisor.py:795: AssertionError
________ TestPlanReviewerStructureValidation.test_plan_with_none_stages ________

self = <test_planner_validation.TestPlanReviewerStructureValidation object at 0x134c37770>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_plan_with_none_stages(self, base_state):
        """Test detection of plan with None stages."""
        base_state["plan"] = {"stages": None}
    
>       result = plan_reviewer_node(base_state)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/workflow/test_planner_validation.py:263: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/agents/base.py:53: in wrapper
    return func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/agents/planning.py:519: in plan_reviewer_node
    target_id_issues = validate_target_ids_exist(plan, paper_figures)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

plan = {'stages': None}
paper_figures = [{'description': 'Extinction spectrum of gold nanorod from 400-900 nm', 'digitized_data_path': 'papers/test_gold_nanor...resonance wavelength', 'id': 'Fig2', 'image_path': 'papers/test_gold_nanorod/fig2_nearfield.png', 'type': 'field_map'}]

    def validate_target_ids_exist(plan: Dict[str, Any], paper_figures: List[Dict[str, Any]]) -> List[str]:
        """
        Validate that all target figure IDs in the plan reference actual paper_figures.
    
        This catches cases where the planner invented semantic IDs like "Fig_spectrum"
        instead of using the actual IDs from paper_figures (e.g., "Fig1", "Fig2").
    
        Args:
            plan: The reproduction plan dictionary
            paper_figures: List of figure dictionaries with 'id' keys
    
        Returns:
            List of issue strings (empty if all targets are valid)
        """
        # Build set of valid figure IDs from paper_figures
        valid_ids = {fig.get("id") for fig in paper_figures if isinstance(fig, dict) and fig.get("id")}
    
        if not valid_ids:
            # No paper figures to validate against
            return []
    
        issues = []
    
        # Check stage-level targets
>       for stage in plan.get("stages", []):
                     ^^^^^^^^^^^^^^^^^^^^^^
E       TypeError: 'NoneType' object is not iterable

src/agents/planning.py:313: TypeError
_ TestPlanReviewerVerdictNormalization.test_verdict_pass_normalized_to_approve _

self = <test_planner_validation.TestPlanReviewerVerdictNormalization object at 0x134c86e90>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_verdict_pass_normalized_to_approve(self, base_state):
        """Test that 'pass' verdict is normalized to 'approve'."""
        plan = MockResponseFactory.planner_response()
        base_state["plan"] = plan
    
        with patch("src.agents.planning.call_agent_with_metrics") as mock_llm:
            mock_llm.return_value = {"verdict": "pass", "summary": "OK"}
            result = plan_reviewer_node(base_state)
    
>       assert result["last_plan_review_verdict"] == "approve"
E       AssertionError: assert 'needs_revision' == 'approve'
E         
E         - approve
E         + needs_revision

tests/workflow/test_planner_validation.py:486: AssertionError
_ TestPlanReviewerVerdictNormalization.test_verdict_approved_normalized_to_approve _

self = <test_planner_validation.TestPlanReviewerVerdictNormalization object at 0x134cceea0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_verdict_approved_normalized_to_approve(self, base_state):
        """Test that 'approved' verdict is normalized to 'approve'."""
        plan = MockResponseFactory.planner_response()
        base_state["plan"] = plan
    
        with patch("src.agents.planning.call_agent_with_metrics") as mock_llm:
            mock_llm.return_value = {"verdict": "approved", "summary": "OK"}
            result = plan_reviewer_node(base_state)
    
>       assert result["last_plan_review_verdict"] == "approve"
E       AssertionError: assert 'needs_revision' == 'approve'
E         
E         - approve
E         + needs_revision

tests/workflow/test_planner_validation.py:508: AssertionError
_ TestPlanReviewerVerdictNormalization.test_verdict_accept_normalized_to_approve _

self = <test_planner_validation.TestPlanReviewerVerdictNormalization object at 0x134ccefd0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_verdict_accept_normalized_to_approve(self, base_state):
        """Test that 'accept' verdict is normalized to 'approve'."""
        plan = MockResponseFactory.planner_response()
        base_state["plan"] = plan
    
        with patch("src.agents.planning.call_agent_with_metrics") as mock_llm:
            mock_llm.return_value = {"verdict": "accept", "summary": "OK"}
            result = plan_reviewer_node(base_state)
    
>       assert result["last_plan_review_verdict"] == "approve"
E       AssertionError: assert 'needs_revision' == 'approve'
E         
E         - approve
E         + needs_revision

tests/workflow/test_planner_validation.py:519: AssertionError
__ TestPlanReviewerReplanCounter.test_replan_count_not_incremented_on_approve __

self = <test_planner_validation.TestPlanReviewerReplanCounter object at 0x134c87250>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_replan_count_not_incremented_on_approve(self, base_state):
        """Test that replan count is not incremented on approve."""
        plan = MockResponseFactory.planner_response()
        base_state["plan"] = plan
        base_state["replan_count"] = 1
    
        with patch("src.agents.planning.call_agent_with_metrics") as mock_llm:
            mock_llm.return_value = MockResponseFactory.reviewer_approve()
            result = plan_reviewer_node(base_state)
    
>       assert result["last_plan_review_verdict"] == "approve"
E       AssertionError: assert 'needs_revision' == 'approve'
E         
E         - approve
E         + needs_revision

tests/workflow/test_planner_validation.py:562: AssertionError
___ TestPlanReviewerFeedbackExtraction.test_feedback_extracted_from_response ___

self = <test_planner_validation.TestPlanReviewerFeedbackExtraction object at 0x134c874d0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_feedback_extracted_from_response(self, base_state):
        """Test that feedback is extracted from LLM response."""
        plan = MockResponseFactory.planner_response()
        base_state["plan"] = plan
    
        with patch("src.agents.planning.call_agent_with_metrics") as mock_llm:
            mock_llm.return_value = {
                "verdict": "needs_revision",
                "feedback": "The plan needs more detail about boundary conditions.",
                "summary": "Needs work",
            }
            result = plan_reviewer_node(base_state)
    
        assert result["last_plan_review_verdict"] == "needs_revision"
>       assert "boundary conditions" in result["planner_feedback"]
E       assert 'boundary conditions' in "Plan has 1 blocking issue(s) requiring revision:\nPLAN_ISSUE: Stage 'stage_0_materials' targets 'material_gold' which doesn't exist in paper_figures. Available IDs: ['Fig1', 'Fig2']"

tests/workflow/test_planner_validation.py:628: AssertionError
_ TestPlanReviewerFeedbackExtraction.test_summary_used_as_fallback_for_feedback _

self = <test_planner_validation.TestPlanReviewerFeedbackExtraction object at 0x134c87610>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_summary_used_as_fallback_for_feedback(self, base_state):
        """Test that summary is used if feedback not provided."""
        plan = MockResponseFactory.planner_response()
        base_state["plan"] = plan
    
        with patch("src.agents.planning.call_agent_with_metrics") as mock_llm:
            mock_llm.return_value = {
                "verdict": "needs_revision",
                "summary": "Plan needs more simulation details.",
            }
            result = plan_reviewer_node(base_state)
    
        assert result["last_plan_review_verdict"] == "needs_revision"
>       assert "simulation details" in result["planner_feedback"]
E       assert 'simulation details' in "Plan has 1 blocking issue(s) requiring revision:\nPLAN_ISSUE: Stage 'stage_0_materials' targets 'material_gold' which doesn't exist in paper_figures. Available IDs: ['Fig1', 'Fig2']"

tests/workflow/test_planner_validation.py:643: AssertionError
_________ TestCodeRevisionCycle.test_code_reviewer_feedback_extraction _________

self = <test_workflow_revisions.TestCodeRevisionCycle object at 0x134bedae0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_code_reviewer_feedback_extraction(self, base_state):
        """Code reviewer extracts feedback from response."""
        plan = MockResponseFactory.planner_response()
        base_state["plan"] = plan
        base_state["progress"] = deepcopy(plan["progress"])
        base_state["current_stage_id"] = "stage_1_extinction"
        base_state["code"] = "# test code"
        base_state["design_description"] = "Test design"
        base_state["runtime_config"] = {"max_code_revisions": 10}
    
        # Test feedback from 'feedback' field
        with patch("src.agents.code.call_agent_with_metrics") as mock_llm:
            mock_llm.return_value = {
                "verdict": "needs_revision",
                "feedback": "Use vectorized operations",
                "summary": "Different summary"
            }
            result = code_reviewer_node(base_state)
>       assert result["reviewer_feedback"] == "Use vectorized operations"
E       AssertionError: assert 'Different summary' == 'Use vectorized operations'
E         
E         - Use vectorized operations
E         + Different summary

tests/workflow/test_workflow_revisions.py:391: AssertionError
_____ TestCodeGeneratorValidation.test_stub_detection_short_code_with_todo _____

self = <test_workflow_revisions.TestCodeGeneratorValidation object at 0x134b2a3f0>
base_state = {'analysis_feedback': None, 'analysis_revision_count': 0, 'analysis_summary': None, 'ask_user_trigger': None, ...}

    def test_stub_detection_short_code_with_todo(self, base_state):
        """Short code with TODO marker is detected as stub."""
        base_state["current_stage_id"] = "stage_1_extinction"
        base_state["current_stage_type"] = "SINGLE_STRUCTURE"
        base_state["validated_materials"] = [{"material_id": "gold"}]
        base_state["design_description"] = "Valid design " * 5
    
        stub_response = MockResponseFactory.code_generator_response()
        stub_response["code"] = "# TODO: Implement simulation"
    
        with patch("src.agents.code.call_agent_with_metrics") as mock_llm:
            mock_llm.return_value = stub_response
            result = code_generator_node(base_state)
    
        assert "reviewer_feedback" in result
        assert "stub" in result["reviewer_feedback"].lower()
>       assert result["code_revision_count"] == 1
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       KeyError: 'code_revision_count'

tests/workflow/test_workflow_revisions.py:464: KeyError
------------------------------ Captured log call -------------------------------
ERROR    src.agents.code:code.py:359 Generated code is stub or empty (stub=True, empty=True). Code generation must produce valid simulation code.
=============================== warnings summary ===============================
src/code_runner.py:200
  /Users/shaiber/Documents/GitHub/biroclick/src/code_runner.py:200: RuntimeWarning: Running on Apple Silicon. Ensure Meep is installed for ARM64 or via Rosetta for best compatibility.
    warnings.warn(warning, RuntimeWarning)

tests/integration/real/test_schemas.py:8
tests/integration/real/test_schemas.py:8
  /Users/shaiber/Documents/GitHub/biroclick/tests/integration/real/test_schemas.py:8: DeprecationWarning: jsonschema.RefResolver is deprecated as of v4.18.0, in favor of the https://github.com/python-jsonschema/referencing library, which provides more compliant referencing behavior as well as more flexible APIs for customization. A future release will remove RefResolver. Please file a feature request (on referencing) if you are missing an API for the kind of customization you need.
    from jsonschema import Draft7Validator, RefResolver, ValidationError, validate

tests/agents/helpers/test_numeric.py::TestQuantitativeCurveMetrics::test_peak_position_error_not_computed_when_ref_peak_value_zero
tests/agents/helpers/test_numeric.py::TestQuantitativeCurveMetrics::test_peak_height_ratio_not_computed_when_ref_zero
tests/agents/helpers/test_numeric.py::TestQuantitativeCurveMetrics::test_fwhm_ratio_not_computed_when_ref_fwhm_missing
tests/agents/helpers/test_numeric.py::TestQuantitativeCurveMetrics::test_handles_constant_reference_gracefully
  /opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/numpy/lib/_function_base_impl.py:3065: RuntimeWarning: invalid value encountered in divide
    c /= stddev[:, None]

tests/agents/helpers/test_numeric.py::TestQuantitativeCurveMetrics::test_peak_position_error_not_computed_when_ref_peak_value_zero
tests/agents/helpers/test_numeric.py::TestQuantitativeCurveMetrics::test_peak_height_ratio_not_computed_when_ref_zero
tests/agents/helpers/test_numeric.py::TestQuantitativeCurveMetrics::test_fwhm_ratio_not_computed_when_ref_fwhm_missing
tests/agents/helpers/test_numeric.py::TestQuantitativeCurveMetrics::test_handles_constant_reference_gracefully
  /opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/numpy/lib/_function_base_impl.py:3066: RuntimeWarning: invalid value encountered in divide
    c /= stddev[None, :]

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
============================== slowest durations ===============================
1.11s call     tests/integration/test_run_folder_structure.py::TestMultipleRuns::test_multiple_runs_create_separate_folders
1.03s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_timeout_with_output_capture
1.02s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_real_timeout_enforcement
1.01s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_timeout_error_structure
0.53s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_runtime_seconds_tracking
0.36s call     tests/state_machine/test_full_flow.py::TestPhysicsDesignFlawFlow::test_physics_design_flaw_routes_to_design
0.24s call     tests/state_machine/test_code.py::TestCodePhase::test_code_revision_flow
0.23s setup    tests/integration/graph/test_graph_structure.py::TestGraphConnectivity::test_end_node_has_no_outgoing_edges
0.22s call     tests/state_machine/test_full_flow.py::TestExecutionFailureFlow::test_execution_failure_routes_to_code_regeneration
0.21s setup    tests/integration/graph/test_graph_structure.py::TestRoutingTargetIntegrity::test_all_routed_destinations_exist_in_graph
0.21s call     tests/state_machine/test_code.py::TestCodePhase::test_code_approve_flow
0.20s call     tests/test_validate_schema_access/test_integration.py::TestDynamicSchemaMapping::test_build_mapping_is_deterministic
0.20s setup    tests/integration/graph/test_graph_structure.py::TestConditionalEdgeCompleteness::test_physics_check_edges_cover_all_router_returns
0.20s setup    tests/integration/graph/test_graph_structure.py::TestGraphInterruptConfiguration::test_graph_has_checkpointer
0.19s setup    tests/integration/graph/test_graph_structure.py::TestGraphStructureInvariants::test_all_workflow_nodes_are_reachable_from_start
0.18s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_all_nodes_have_outgoing_edges_except_end
0.18s setup    tests/integration/graph/test_graph_structure.py::TestConditionalEdgeCompleteness::test_comparison_check_edges_include_ask_user
0.15s setup    tests/integration/graph/test_graph_structure.py::TestConditionalEdgeCompleteness::test_execution_check_edges_cover_all_router_returns
0.15s setup    tests/integration/graph/test_graph_structure.py::TestGraphConnectivity::test_no_self_loops
0.15s call     tests/state_machine/test_full_flow.py::TestSupervisorReplanFlow::test_supervisor_replan_routes_to_plan
0.14s setup    tests/integration/graph/test_graph_structure.py::TestGraphConnectivity::test_start_node_has_no_incoming_edges
0.14s setup    tests/integration/graph/test_graph_structure.py::TestConditionalEdgeCompleteness::test_comparison_check_edges_cover_all_router_returns
0.13s setup    tests/integration/graph/test_graph_structure.py::TestWorkflowPathIntegrity::test_all_ask_user_escalations_return_to_supervisor
0.13s call     tests/state_machine/test_limits.py::TestExecutionFailureLimit::test_execution_failure_below_limit_retries_code_generation
0.13s call     tests/state_machine/test_full_flow.py::TestFullSingleStageHappyPath::test_full_single_stage_happy_path
0.13s call     tests/state_machine/test_full_flow.py::TestMultiStageCompletion::test_all_complete_routes_to_report
0.11s call     tests/state_machine/test_full_flow.py::TestSupervisorBacktrackFlow::test_supervisor_backtrack_routes_to_handle_backtrack
0.11s call     tests/state_machine/test_execution.py::TestExecutionPhaseGraphFlow::test_physics_design_flaw_routes_to_design
0.11s call     tests/state_machine/test_execution.py::TestExecutionPhaseGraphFlow::test_physics_pass_routes_to_analyze
0.11s call     tests/state_machine/test_full_flow.py::TestPhysicsWarningFlow::test_physics_warning_proceeds_to_analyze
0.11s call     tests/state_machine/test_execution.py::TestExecutionPhaseGraphFlow::test_execution_fail_routes_to_generate_code
0.11s call     tests/state_machine/test_execution.py::TestExecutionPhaseGraphFlow::test_execution_success_flow
0.11s call     tests/state_machine/test_full_flow.py::TestExecutionWarningFlow::test_execution_warning_proceeds_to_physics_check
0.11s call     tests/llm_client/test_call_agent_metrics.py::TestCallAgentMetrics::test_duration_reflects_actual_time
0.10s call     tests/state_machine/test_full_flow.py::TestEdgeCases::test_comparison_revision_limit_routes_to_ask_user
0.10s call     tests/state_machine/test_limits.py::TestPhysicsFailureLimit::test_physics_design_flaw_routes_to_design
0.10s call     tests/integration/test_run_folder_structure.py::TestBackwardsCompatibility::test_load_checkpoint_prefers_most_recent_across_structures
0.10s call     tests/state_machine/test_limits.py::TestAnalysisRevisionLimit::test_analysis_revision_limit_routes_to_ask_user
0.10s call     tests/state_machine/test_full_flow.py::TestCodeRevisionLimitEscalation::test_code_revision_limit_triggers_ask_user
0.10s call     tests/state_machine/test_limits.py::TestCodeReviewLimit::test_code_review_no_limit_when_below_max
0.10s call     tests/state_machine/test_limits.py::TestExecutionFailureLimit::test_execution_failure_limit_interrupts
0.10s call     tests/state_machine/test_limits.py::TestPhysicsFailureLimit::test_physics_failure_limit_interrupts
0.10s call     tests/state_machine/test_code.py::TestCodePhase::test_code_review_limit_escalates_to_ask_user
0.09s setup    tests/integration/graph/test_graph_structure.py::TestConditionalEdgeCompleteness::test_plan_review_edges_cover_all_router_returns
0.09s setup    tests/integration/graph/test_graph_structure.py::TestGraphStructureInvariants::test_exactly_one_start_edge
0.09s call     tests/agents/planning/test_plan_reviewer.py::TestPlanReviewerNode::test_reviewer_blocking_self_dependency
0.09s call     tests/state_machine/test_planning.py::TestStageSelection::test_stage_selection_picks_first_stage
0.09s call     tests/state_machine/test_full_flow.py::TestInterruptResumeFlow::test_interrupt_resume_with_user_response
0.09s call     tests/state_machine/test_full_flow.py::TestInterruptResumeFlow::test_invoke_pattern_handles_interrupt_correctly
0.09s call     tests/paper_loader/test_markdown_helpers.py::TestResolveFigureUrl::test_relative_url_dot_dot_multiple
0.09s call     tests/state_machine/test_design.py::TestDesignPhase::test_design_revision_limit_escalates
0.09s call     tests/state_machine/test_full_flow.py::TestMaterialCheckpointFlow::test_material_checkpoint_triggers_interrupt
0.08s call     tests/state_machine/test_design.py::TestDesignPhase::test_design_revision_flow
0.08s call     tests/state_machine/test_limits.py::TestCodeReviewLimit::test_code_review_limit_interrupts_at_exact_boundary
0.08s setup    tests/integration/graph/test_graph_structure.py::TestGraphCompilation::test_graph_compiles_successfully
0.08s call     tests/state_machine/test_full_flow.py::TestDesignRevisionFlow::test_design_rejection_triggers_revision
0.08s call     tests/state_machine/test_limits.py::TestCodeReviewLimit::test_code_review_limit_resume_with_hint_resets_counter
0.08s setup    tests/integration/graph/test_graph_structure.py::TestGraphInterruptConfiguration::test_checkpointer_is_memory_saver
0.08s call     tests/state_machine/test_planning.py::TestStageSelection::test_stage_selection_with_dependencies
0.08s setup    tests/integration/graph/test_graph_structure.py::TestConditionalEdgeCompleteness::test_supervisor_edges_cover_all_router_returns
0.08s call     tests/state_machine/test_full_flow.py::TestVerdictNormalization::test_alternative_verdict_strings_normalized
0.08s call     tests/state_machine/test_design.py::TestDesignPhase::test_design_state_changes_are_preserved
0.08s call     tests/state_machine/test_full_flow.py::TestPlanRevisionFlow::test_plan_rejection_triggers_revision
0.08s call     tests/state_machine/test_full_flow.py::TestVerdictNormalization::test_missing_verdict_defaults_to_needs_revision
0.08s setup    tests/integration/graph/test_graph_structure.py::TestEdgeCountVerification::test_conditional_edge_nodes_have_multiple_outgoing
0.08s call     tests/state_machine/test_planning.py::TestVerdictNormalization::test_plan_reviewer_normalizes_pass_to_approve
0.08s call     tests/state_machine/test_limits.py::TestCodeReviewLimit::test_code_review_escalates_at_boundary_with_higher_max
0.08s call     tests/state_machine/test_limits.py::TestLimitEdgeCases::test_counter_not_reset_on_skip_response
0.08s call     tests/state_machine/test_design.py::TestDesignPhase::test_design_approve_flow
0.08s call     tests/state_machine/test_planning.py::TestPlanReviewerStructuralValidation::test_plan_with_missing_targets_rejected
0.08s call     tests/state_machine/test_planning.py::TestVerdictNormalization::test_plan_reviewer_normalizes_reject_to_needs_revision
0.08s call     tests/state_machine/test_limits.py::TestLimitEdgeCases::test_zero_max_limit_triggers_immediately
0.08s call     tests/state_machine/test_planning.py::TestPlanReviewerStructuralValidation::test_plan_with_self_dependency_rejected
0.08s call     tests/state_machine/test_planning.py::TestPlanReviewerStructuralValidation::test_plan_with_circular_dependency_rejected
0.08s call     tests/state_machine/test_planning.py::TestAdaptPromptsNode::test_adapt_prompts_handles_llm_failure
0.08s call     tests/state_machine/test_planning.py::TestPlanningPhase::test_planning_revision_flow
0.08s call     tests/state_machine/test_planning.py::TestPlanReviewerStructuralValidation::test_plan_with_missing_dependency_rejected
0.08s setup    tests/integration/graph/test_graph_structure.py::TestWorkflowPathIntegrity::test_happy_path_is_connected
0.08s call     tests/state_machine/test_limits.py::TestDesignReviewLimit::test_design_review_limit_resume_resets_counter
0.08s call     tests/state_machine/test_planning.py::TestPlanningPhase::test_interrupt_resume_routes_through_supervisor
0.08s setup    tests/integration/graph/test_graph_structure.py::TestConditionalEdgeCompleteness::test_design_review_edges_cover_all_router_returns
0.08s call     tests/state_machine/test_full_flow.py::TestEdgeCases::test_empty_stages_rejected_by_plan_reviewer
0.08s call     tests/state_machine/test_planning.py::TestAdaptPromptsNode::test_adapt_prompts_stores_adaptations
0.08s setup    tests/integration/graph/test_graph_structure.py::TestGraphStructureInvariants::test_supervisor_node_is_central_hub
0.08s call     tests/state_machine/test_limits.py::TestLimitEdgeCases::test_multiple_limits_in_same_stage
0.08s call     tests/state_machine/test_planning.py::TestPlanningPhase::test_planning_approve_flow
0.07s setup    tests/integration/graph/test_graph_structure.py::TestConditionalEdgeCompleteness::test_code_review_edges_cover_all_router_returns
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphStructureInvariants::test_ask_user_node_exists_for_escalation
0.07s setup    tests/integration/graph/test_graph_structure.py::TestEdgeCountVerification::test_static_edge_nodes_have_single_outgoing
0.07s setup    tests/integration/graph/test_graph_structure.py::TestWorkflowPathIntegrity::test_escalation_paths_to_ask_user
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphConnectivity::test_all_nodes_except_start_have_incoming_edges
0.07s call     tests/state_machine/test_planning.py::TestPlanningPhase::test_planning_max_replan_limit_triggers_ask_user
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphStructureInvariants::test_all_workflow_nodes_can_reach_end
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphStructureInvariants::test_exactly_one_end_connection
0.07s call     tests/state_machine/test_limits.py::TestDesignReviewLimit::test_design_review_limit_interrupts_at_max
0.07s call     tests/state_machine/test_planning.py::TestPlanReviewerStructuralValidation::test_plan_with_duplicate_stage_ids_rejected
0.07s call     tests/state_machine/test_limits.py::TestPlanReviewLimit::test_plan_review_limit_interrupts_at_max_replans
0.07s call     tests/state_machine/test_limits.py::TestPlanReviewLimit::test_plan_review_accepts_before_limit_allows_continuation
0.07s call     tests/state_machine/test_full_flow.py::TestShouldStopFlag::test_should_stop_routes_to_report_non_material_stage
0.07s setup    tests/integration/graph/test_graph_structure.py::TestWorkflowPathIntegrity::test_revision_loops_exist
0.07s setup    tests/integration/graph/test_graph_structure.py::TestWorkflowPathIntegrity::test_supervisor_can_terminate_workflow
0.07s call     tests/state_machine/test_planning.py::TestPlanReviewerStructuralValidation::test_plan_with_no_stages_rejected
0.07s call     tests/state_machine/test_full_flow.py::TestReplanLimitEscalation::test_replan_limit_escalates_to_ask_user
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphCompilation::test_graph_has_all_expected_nodes
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_supervisor_has_all_expected_routes
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphCompilation::test_graph_has_start_and_end_nodes
0.07s call     tests/state_machine/test_planning.py::TestPlanningPhase::test_planning_with_empty_paper_text_escalates
0.07s call     tests/llm_client/test_call_agent_metrics.py::TestCallAgentMetricsIntegration::test_duration_increases_monotonically_with_delay
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_ask_user_routes_to_supervisor
0.07s call     tests/state_machine/test_planning.py::TestPlanReviewerStructuralValidation::test_plan_with_missing_stage_id_rejected
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_comparison_check_has_two_routes
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_handle_backtrack_connects_to_select_stage
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_physics_check_has_four_routes
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_adapt_prompts_connects_to_plan
0.07s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_output_file_listing
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_run_code_connects_to_execution_check
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_generate_report_connects_to_end
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_analyze_connects_to_comparison_check
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_design_connects_to_design_review
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_plan_review_has_three_routes
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_material_checkpoint_connects_to_ask_user_or_select_stage
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_generate_code_connects_to_code_review
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphCompilation::test_graph_node_count_exactly_correct
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_select_stage_has_two_routes
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_execution_check_has_three_routes
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_plan_has_conditional_edge_to_plan_review
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_design_review_has_three_routes
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_code_review_has_three_routes
0.07s setup    tests/integration/graph/test_graph_structure.py::TestGraphEdgeConnectivity::test_start_connects_only_to_adapt_prompts
0.06s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_output_files_sorted
0.06s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_multiple_output_files
0.06s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_inf_in_output_does_not_cause_false_positive
0.06s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_nan_in_output_does_not_cause_false_positive
0.06s call     tests/test_schema_code_consistency.py::TestWhitelistValidation::test_agent_files_use_whitelisted_patterns
0.06s call     tests/test_schema_code_consistency.py::TestSchemaFieldConsistency::test_agent_output_fields_exist_in_schema
0.06s call     tests/test_schema_code_consistency.py::TestIntegration::test_full_validation_runs_without_error
0.06s call     tests/paper_loader/test_markdown_figures.py::TestResolveFigureUrl::test_relative_path_with_base_path
0.06s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_special_characters_in_stage_id
0.06s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_config_merging_with_defaults
0.06s call     tests/integration/graph/test_checkpointing.py::TestLoadCheckpoint::test_load_checkpoint_with_timestamp_pattern_matching
0.06s call     tests/test_validate_schema_access/test_integration.py::TestRealCodebaseValidation::test_field_accesses_are_recorded
0.06s call     tests/test_validate_schema_access/test_integration.py::TestRealCodebaseValidation::test_agent_files_pass_validation
0.06s call     tests/test_validate_schema_access/test_integration.py::TestRealCodebaseValidation::test_validation_scans_all_agent_files
0.06s call     tests/integration/graph/test_checkpointing.py::TestLoadCheckpoint::test_load_checkpoint_multiple_checkpoints_returns_most_recent
0.06s call     tests/llm_client/test_call_agent_metrics.py::TestCallAgentMetrics::test_duration_recorded_on_failure
0.06s call     tests/paper_loader/test_markdown_figures.py::TestResolveFigureUrl::test_parent_relative_path_with_base_path
0.05s call     tests/integration/graph/test_checkpointing.py::TestListCheckpoints::test_list_checkpoints_with_same_name_different_timestamps
0.05s call     tests/integration/graph/test_checkpointing.py::TestListCheckpoints::test_list_checkpoints_sorted_by_timestamp_descending
0.05s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_invalid_config_values
0.05s call     tests/paper_loader/test_markdown_helpers.py::TestResolveFigureUrl::test_base_path_url_encoded
0.04s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_environment_variables_set
0.03s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_large_output_handling
0.03s call     tests/code_runner/test_run_code_node.py::TestRunCodeNode::test_run_code_node_propagates_run_error
0.03s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_runtime_error_handling
0.03s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_import_error_handling
0.03s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_error_result_structure
0.03s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_binary_stdout_handling
0.03s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_syntax_error_reporting
0.03s call     tests/integration/test_run_folder_structure.py::TestDownstreamCodeRunnerIntegration::test_code_runner_output_base_uses_run_output_dir
0.03s call     tests/integration/graph/test_checkpointing.py::TestListCheckpoints::test_list_checkpoints_returns_all_checkpoints
0.02s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_custom_environment_variables
0.02s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_output_dir_creation
0.02s call     tests/code_runner/test_run_code_node.py::TestRunCodeNode::test_run_code_node_integration_real_run_with_output_file
0.02s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_very_large_timeout
0.02s call     tests/integration/test_run_folder_structure.py::TestDownstreamCodeRunnerIntegration::test_code_runner_falls_back_to_legacy_without_run_output_dir
0.02s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_nonexistent_output_dir_parent
0.02s call     tests/code_runner/test_run_code_node.py::TestRunCodeNode::test_run_code_node_integration_real_run
0.02s call     tests/code_runner/test_run_code_node.py::TestRunCodeNode::test_run_code_node_integration_real_run_with_error
0.02s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_unicode_in_stage_id
0.02s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_stderr_capture
0.02s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_none_output_dir_creates_temp
0.02s call     tests/code_runner/test_run_code_node.py::TestRunCodeNode::test_run_code_node_integration_real_run_with_stderr
0.02s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_script_cleanup_when_keep_script_false
0.02s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_very_short_timeout
0.02s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_empty_output_files_list
0.02s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_empty_code_handling
0.02s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_script_file_creation
0.02s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_run_code_node_success
0.02s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_materials_directory_symlink
0.02s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_planner_all_progress_status_enums
0.02s call     tests/code_runner/test_run_code_node.py::TestRunCodeNode::test_run_code_node_whitespace_only_code
0.02s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_very_long_code_string
0.02s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_unicode_in_code
0.02s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_planner_all_paper_domain_enums
0.02s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_script_persists_when_keep_script_true
0.02s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_simulation_designer_all_structure_type_enums
0.02s call     tests/code_runner/test_execution_robustness.py::TestExecutionRobustness::test_result_structure_completeness
0.02s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_code_reviewer_all_issue_category_enums
0.02s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_simulation_designer_all_source_component_enums
0.02s call     tests/integration/graph/test_graph_structure.py::TestDynamicTriggerConsistency::test_all_trigger_assignments_have_handlers
0.02s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_planner_all_artifact_type_enums
0.02s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_planner_all_parameter_source_enums
0.02s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_planner_all_complexity_class_enums
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_plan_reviewer_all_issue_category_enums
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_simulation_designer_all_material_model_type_enums
0.01s call     tests/integration/graph/test_graph_structure.py::TestRouteAfterPhysicsCheck::test_design_flaw_routes_to_design_under_limit
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_design_reviewer_all_issue_category_enums
0.01s call     tests/integration/graph/test_checkpointing.py::TestCheckpointIntegration::test_checkpoint_creates_unique_timestamps
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_planner_all_target_type_enums
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_planner_all_stage_type_enums
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_planner_all_simulation_class_enums
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_planner_all_assumption_category_enums
0.01s call     tests/agents/helpers/test_metrics.py::TestLogAgentCall::test_records_duration
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_simulation_designer_all_monitor_type_enums
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_simulation_designer_all_boundary_condition_enums
0.01s call     tests/llm_contracts/test_contract_shapes.py::TestReviewerContract::test_invalid_verdict_rejected[plan_reviewer-plan_reviewer_output_schema.json]
0.01s call     tests/llm_contracts/test_contract_shapes.py::TestReviewerContract::test_invalid_verdict_rejected[code_reviewer-code_reviewer_output_schema.json]
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_planner_all_assumption_source_enums
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_planner_reproduction_scope_all_skipped_classifications
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_planner_all_precision_requirement_enums
0.01s call     tests/integration/graph/test_graph_structure.py::TestComplexE2EFlows::test_complete_single_stage_with_review_cycles
0.01s call     tests/llm_contracts/test_contract_shapes.py::TestReviewerContract::test_invalid_verdict_rejected[design_reviewer-design_reviewer_output_schema.json]
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_simulation_designer_all_source_type_enums
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_simulation_designer_all_symmetry_direction_enums
0.01s call     tests/integration/graph/test_checkpointing.py::TestCheckpointIntegration::test_checkpoint_very_large_state
0.01s teardown tests/integration/graph/test_graph_structure.py::TestTriggerNameConsistency::test_all_trigger_names_have_handlers
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_planner_geometry_interpretation_confidence_enums
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_planner_all_fallback_strategy_enums
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_code_generator_all_artifact_type_enums
0.01s call     tests/llm_contracts/test_contract_shapes.py::TestSupervisorContract::test_invalid_verdict_rejected
0.01s call     tests/llm_contracts/test_contract_shapes.py::TestSupervisorContract::test_validation_hierarchy_status_invalid_enum
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_supervisor_all_verdict_enums
0.01s call     tests/llm_contracts/test_negative_responses.py::TestArrayItemValidation::test_source_missing_type
0.01s call     tests/llm_contracts/test_negative_responses.py::TestWrongTypeDetection::test_unit_system_as_string
0.01s call     tests/integration/graph/test_graph_structure.py::TestRouteAfterPlanReview::test_needs_revision_routes_to_plan_under_limit
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_results_analyzer_all_overall_classification_enums
0.01s call     tests/llm_contracts/test_contract_shapes.py::TestSupervisorContract::test_all_verdict_enum_values
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_plan_reviewer_all_issue_severity_enums
0.01s call     tests/llm_contracts/test_negative_responses.py::TestArrayItemValidation::test_monitor_missing_name
0.01s call     tests/llm_contracts/test_negative_responses.py::TestInvalidEnumValues::test_material_model_type_invalid
0.01s call     tests/llm_contracts/test_negative_responses.py::TestArrayItemValidation::test_source_missing_center
0.01s call     tests/llm_contracts/test_negative_responses.py::TestMissingRequiredFields::test_simulation_designer_missing_stage_id
0.01s call     tests/llm_contracts/test_negative_responses.py::TestArrayItemValidation::test_material_missing_id
0.01s call     tests/llm_contracts/test_negative_responses.py::TestNestedObjectValidation::test_performance_estimate_missing_runtime
0.01s call     tests/llm_contracts/test_negative_responses.py::TestMissingRequiredFields::test_simulation_designer_missing_unit_system
0.01s call     tests/llm_contracts/test_negative_responses.py::TestNestedObjectValidation::test_unit_system_missing_characteristic_length
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_code_reviewer_all_issue_severity_enums
0.01s call     tests/llm_contracts/test_negative_responses.py::TestArrayItemValidation::test_material_missing_model_type
0.01s call     tests/llm_contracts/test_negative_responses.py::TestArrayItemValidation::test_monitor_missing_type
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_simulation_designer_all_dimensionality_enums
0.01s call     tests/llm_contracts/test_negative_responses.py::TestNestedObjectValidation::test_unit_system_missing_length_unit
0.01s call     tests/llm_contracts/test_negative_responses.py::TestNestedObjectValidation::test_performance_estimate_missing_memory
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_code_reviewer_backtrack_suggestion_all_severity_enums
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_simulation_designer_all_run_until_type_enums
0.01s call     tests/llm_contracts/test_negative_responses.py::TestBoundaryValuesAndSpecialCases::test_empty_response_planner
0.01s call     tests/llm_contracts/test_contract_shapes.py::TestReviewerContract::test_approve_verdict_full_schema_validation[design_reviewer-design_reviewer_output_schema.json]
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_plan_reviewer_all_checklist_status_enums
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_code_reviewer_all_checklist_status_enums
0.01s call     tests/llm_contracts/test_negative_responses.py::TestMissingRequiredFields::test_planner_missing_stages
0.01s call     tests/llm_contracts/test_negative_responses.py::TestMissingRequiredFields::test_planner_missing_paper_id
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_results_analyzer_all_discrepancy_classification_enums
0.01s call     tests/llm_contracts/test_edge_cases.py::TestEdgeCaseResponses::test_results_analyzer_all_comparison_type_enums
0.01s call     tests/state_machine/test_code.py::TestCodeReviewerNode::test_verdict_normalization[needs_work-needs_revision]
0.01s call     tests/llm_client/test_schema_validation.py::TestSchemaEdgeCases::test_concurrent_cache_access_safety

(27664 durations < 0.005s hidden.  Use -vv to show these durations.)
=========================== short test summary info ============================
FAILED tests/agents/code/test_code_generator.py::TestCodeGeneratorNode::test_generator_code_extraction[llm_output1-sim codesim codesim codesim codesim codesim codesim codesim codesim codesim code-expected_outputs1]
FAILED tests/agents/code/test_code_generator.py::TestCodeGeneratorNode::test_generator_stub_code_output[# TODO: Implement simulation]
FAILED tests/agents/code/test_code_generator.py::TestCodeGeneratorNode::test_generator_stub_code_output[STUB: code here]
FAILED tests/agents/code/test_code_generator.py::TestCodeGeneratorNode::test_generator_stub_code_output[PLACEHOLDER: add code]
FAILED tests/agents/code/test_code_generator.py::TestCodeGeneratorNode::test_generator_stub_code_output[# Replace this with actual code]
FAILED tests/agents/code/test_code_generator.py::TestCodeGeneratorNode::test_generator_stub_code_output[would be generated]
FAILED tests/agents/code/test_code_generator.py::TestCodeGeneratorNode::test_generator_stub_code_output[TODO: fix this]
FAILED tests/agents/code/test_code_generator.py::TestCodeGeneratorNode::test_generator_stub_code_respects_max_revisions
FAILED tests/agents/code/test_code_generator.py::TestCodeGeneratorNode::test_generator_stub_code_respects_custom_max_revisions
FAILED tests/agents/code/test_code_generator.py::TestCodeGeneratorNode::test_generator_short_code_without_stub_markers[print('short run')]
FAILED tests/agents/code/test_code_generator.py::TestCodeGeneratorNode::test_generator_short_code_without_stub_markers[x = 1]
FAILED tests/agents/code/test_code_generator.py::TestCodeGeneratorNode::test_generator_short_code_without_stub_markers[a]
FAILED tests/agents/code/test_code_generator.py::TestCodeGeneratorNode::test_generator_short_code_without_stub_markers[   ]
FAILED tests/agents/code/test_code_generator.py::TestCodeGeneratorNode::test_generator_short_code_without_stub_markers[\n\n]
FAILED tests/agents/code/test_code_generator.py::TestCodeGeneratorNode::test_generator_short_code_without_stub_markers[AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA]
FAILED tests/agents/code/test_code_generator.py::TestCodeGeneratorNode::test_generator_code_extraction_empty_code_string
FAILED tests/agents/code/test_code_generator.py::TestCodeGeneratorNode::test_generator_code_extraction_whitespace_only_code
FAILED tests/agents/code/test_code_generator.py::TestCodeGeneratorNode::test_generator_stub_detection_case_insensitive
FAILED tests/agents/code/test_code_reviewer.py::TestCodeReviewerNode::test_reviewer_needs_revision
FAILED tests/agents/code/test_code_reviewer.py::TestCodeReviewerNode::test_reviewer_neither_feedback_nor_summary
FAILED tests/agents/code/test_code_reviewer.py::TestCodeReviewerNode::test_reviewer_max_revisions
FAILED tests/agents/code/test_code_reviewer.py::TestCodeReviewerNode::test_reviewer_missing_verdict_key
FAILED tests/agents/code/test_code_reviewer.py::TestCodeReviewerNode::test_reviewer_escalation_message_format
FAILED tests/agents/design/test_design_reviewer.py::TestDesignReviewerNode::test_reviewer_needs_revision
FAILED tests/agents/design/test_design_reviewer.py::TestDesignReviewerNode::test_reviewer_max_revisions
FAILED tests/agents/design/test_design_reviewer.py::TestDesignReviewerNode::test_reviewer_missing_verdict_defaults_to_needs_revision
FAILED tests/agents/planning/test_adapt_prompts.py::TestAdaptPromptsNode::test_adapt_prompts_success_with_domain
FAILED tests/agents/planning/test_adapt_prompts.py::TestAdaptPromptsNode::test_adapt_prompts_state_not_mutated
FAILED tests/agents/planning/test_plan_reviewer.py::TestPlanReviewerNode::test_reviewer_llm_rejection
FAILED tests/agents/supervision/test_supervisor_deadlock_misc.py::TestDeadlockTrigger::test_asks_clarification_on_unclear
FAILED tests/agents/supervision/test_supervisor_deadlock_misc.py::TestDeadlockTrigger::test_handles_empty_user_responses
FAILED tests/agents/supervision/test_supervisor_deadlock_misc.py::TestDeadlockTriggerEdgeCases::test_handles_invalid_user_responses_type
FAILED tests/agents/supervision/test_supervisor_deadlock_misc.py::TestDeadlockTriggerEdgeCases::test_handles_none_user_responses
FAILED tests/agents/supervision/test_supervisor_limits.py::TestExecutionFailureLimitTrigger::test_resets_count_on_retry
FAILED tests/agents/supervision/test_supervisor_limits.py::TestExecutionFailureLimitTrigger::test_resets_count_on_guidance
FAILED tests/agents/supervision/test_supervisor_limits.py::TestClarificationTrigger::test_continues_with_clarification
FAILED tests/agents/supervision/test_supervisor_limits.py::TestClarificationTrigger::test_preserves_clarification_text
FAILED tests/agents/supervision/test_supervisor_limits.py::TestUserResponseVariations::test_handles_unicode_response
FAILED tests/agents/supervision/test_supervisor_material_checkpoint.py::TestMaterialCheckpointTrigger::test_handles_need_help
FAILED tests/agents/supervision/test_supervisor_material_checkpoint.py::TestMaterialCheckpointTrigger::test_handles_unclear_response
FAILED tests/agents/supervision/test_supervisor_material_checkpoint.py::TestMaterialCheckpointTrigger::test_handles_rejection_without_specifics
FAILED tests/agents/supervision/test_supervisor_material_checkpoint.py::TestMaterialCheckpointTrigger::test_handles_missing_pending_materials_on_approve
FAILED tests/agents/supervision/test_supervisor_node_core.py::TestSupervisorNode::test_triggers_backtrack_on_failure
FAILED tests/agents/supervision/test_supervisor_node_core.py::TestSupervisorNode::test_returns_supervisor_verdict
FAILED tests/agents/supervision/test_supervisor_node_core.py::TestSupervisorNode::test_handles_finish_verdict
FAILED tests/agents/supervision/test_supervisor_node_core.py::TestSupervisorNode::test_handles_missing_verdict_in_response
FAILED tests/agents/supervision/test_supervisor_node_core.py::TestSupervisorNode::test_complex_flow_with_context_update_and_archiving
FAILED tests/agents/supervision/test_supervisor_node_core.py::TestSupervisorNode::test_backtrack_verdict_from_llm
FAILED tests/agents/supervision/test_supervisor_node_core.py::TestSupervisorNode::test_replan_needed_verdict
FAILED tests/agents/supervision/test_supervisor_node_core.py::TestSupervisorNode::test_reasoning_with_newlines_and_special_chars
FAILED tests/agents/supervision/test_supervisor_recovery.py::TestNormalSupervision::test_normal_supervision_calls_llm
FAILED tests/agents/supervision/test_supervisor_recovery.py::TestNormalSupervision::test_normal_supervision_backtrack_verdict
FAILED tests/agents/supervision/test_supervisor_recovery.py::TestNormalSupervision::test_normal_supervision_replan_verdict
FAILED tests/agents/supervision/test_supervisor_recovery.py::TestNormalSupervision::test_normal_supervision_backtrack_decision_structure
FAILED tests/agents/supervision/test_supervisor_recovery.py::TestTriggerHandlerIntegration::test_execution_failure_limit_retry
FAILED tests/agents/trigger_handlers/test_execution_physics_limits.py::TestExecutionFailureLimitTrigger::test_resets_count_on_retry
FAILED tests/agents/trigger_handlers/test_execution_physics_limits.py::TestExecutionFailureLimitTrigger::test_resets_count_on_guidance
FAILED tests/agents/trigger_handlers/test_execution_physics_limits.py::TestHandleExecutionFailureLimit::test_handle_execution_failure_limit_retry
FAILED tests/agents/trigger_handlers/test_execution_physics_limits.py::TestHandleExecutionFailureLimit::test_handle_execution_failure_limit_guidance_keyword
FAILED tests/agents/trigger_handlers/test_execution_physics_limits.py::TestHandleExecutionFailureLimit::test_handle_execution_failure_limit_retry_and_guidance_both_present
FAILED tests/agents/trigger_handlers/test_execution_physics_limits.py::TestHandlePhysicsFailureLimit::test_handle_physics_failure_limit_retry_with_guidance
FAILED tests/agents/trigger_handlers/test_execution_physics_limits.py::TestHandlePhysicsFailureLimit::test_handle_physics_failure_limit_feedback_overwrites_existing
FAILED tests/agents/trigger_handlers/test_execution_physics_limits.py::TestHandlePhysicsFailureLimit::test_handle_execution_failure_limit_feedback_overwrites_existing
FAILED tests/agents/trigger_handlers/test_execution_physics_limits.py::TestFeedbackFormat::test_execution_retry_feedback_includes_raw_response
FAILED tests/agents/trigger_handlers/test_execution_physics_limits.py::TestFeedbackFormat::test_physics_retry_feedback_includes_raw_response
FAILED tests/agents/trigger_handlers/test_execution_physics_limits.py::TestFeedbackFormat::test_execution_retry_empty_guidance_produces_valid_feedback
FAILED tests/agents/trigger_handlers/test_execution_physics_limits.py::TestFeedbackFormat::test_execution_guidance_keyword_feedback
FAILED tests/agents/trigger_handlers/test_execution_physics_limits.py::TestMultipleUserResponses::test_physics_uses_last_response_for_raw
FAILED tests/agents/trigger_handlers/test_execution_physics_limits.py::TestSpecialCharactersInResponses::test_execution_response_with_newlines
FAILED tests/agents/trigger_handlers/test_execution_physics_limits.py::TestSpecialCharactersInResponses::test_execution_response_with_unicode
FAILED tests/agents/trigger_handlers/test_execution_physics_limits.py::TestBoundaryConditions::test_execution_very_long_response
FAILED tests/agents/trigger_handlers/test_execution_physics_limits.py::TestDictKeyOrdering::test_execution_response_order_matters
FAILED tests/agents/trigger_handlers/test_execution_physics_limits.py::TestKeywordOnlyResponses::test_execution_exact_retry_keyword
FAILED tests/agents/trigger_handlers/test_misc_dispatcher.py::TestHandleClarification::test_handle_clarification_with_response
FAILED tests/agents/trigger_handlers/test_misc_dispatcher.py::TestHandleClarification::test_handle_clarification_multiple_responses
FAILED tests/integration/graph/test_graph_structure.py::TestUserGuidanceIntegration::test_execution_retry_resets_counter_effectively
FAILED tests/integration/planning/test_plan_reviewer_rules.py::TestPlannerFeedback::test_plan_reviewer_sets_planner_feedback_on_llm_rejection
FAILED tests/integration/planning/test_prompt_adaptation.py::TestAdaptPromptsNode::test_adapt_prompts_success
FAILED tests/integration/planning/test_prompt_adaptation.py::TestAdaptPromptsNode::test_adapt_prompts_node_updates_state
FAILED tests/integration/planning/test_prompt_adaptation.py::TestAdaptPromptsNode::test_adapt_prompts_handles_none_adaptations
FAILED tests/integration/planning/test_prompt_adaptation.py::TestAdaptPromptsNode::test_adapt_prompts_handles_missing_adaptations_key
FAILED tests/integration/planning/test_prompt_adaptation.py::TestAdaptPromptsNode::test_adapt_prompts_handles_non_list_adaptations
FAILED tests/integration/planning/test_prompt_adaptation.py::TestAdaptPromptsNode::test_adapt_prompts_handles_empty_adaptations_list
FAILED tests/integration/planning/test_prompt_adaptation.py::TestAdaptPromptsNode::test_adapt_prompts_handles_none_paper_domain
FAILED tests/integration/planning/test_prompt_adaptation.py::TestAdaptPromptsNode::test_adapt_prompts_handles_missing_paper_domain_key
FAILED tests/integration/planning/test_prompt_adaptation.py::TestAdaptPromptsNode::test_adapt_prompts_handles_empty_paper_domain
FAILED tests/integration/planning/test_prompt_adaptation.py::TestAdaptPromptsNode::test_adapt_prompts_preserves_existing_paper_domain
FAILED tests/integration/planning/test_prompt_adaptation.py::TestAdaptPromptsNode::test_adapt_prompts_handles_none_paper_domain_in_state
FAILED tests/integration/planning/test_prompt_adaptation.py::TestAdaptPromptsNode::test_adapt_prompts_does_not_mutate_input_state
FAILED tests/integration/planning/test_prompt_adaptation.py::TestAdaptPromptsNode::test_adapt_prompts_handles_context_check_returning_metrics
FAILED tests/integration/planning/test_prompt_adaptation.py::TestAdaptPromptsNode::test_adapt_prompts_handles_large_adaptations_list
FAILED tests/integration/planning/test_prompt_adaptation.py::TestAdaptPromptsNode::test_adapt_prompts_handles_complex_adaptation_objects
FAILED tests/integration/real/test_prompts.py::TestPromptAdaptations::test_replace_adaptation_replaces_all_occurrences
FAILED tests/integration/real/test_prompts.py::TestPromptAdaptations::test_adaptation_with_special_characters_in_content
FAILED tests/integration/real/test_prompts.py::TestPromptAdaptations::test_adaptation_with_multiline_content
FAILED tests/integration/real/test_prompts.py::TestBuildAgentPromptOrder::test_placeholders_substituted_before_adaptations
FAILED tests/integration/real/test_prompts.py::TestErrorHandling::test_get_agent_prompt_cached_bypasses_cache_with_adaptations
FAILED tests/integration/real/test_routing.py::TestRoutingReturnsValidValues::test_supervisor_verdicts_match_routing
FAILED tests/integration/real/test_state_mutations.py::TestStateMutations::test_plan_reviewer_increments_replan_count_on_needs_revision
FAILED tests/integration/real/test_state_mutations.py::TestStateMutations::test_counter_increments_on_revision
FAILED tests/integration/real/test_state_mutations.py::TestStateMutations::test_design_reviewer_counter_does_not_increment_on_approve
FAILED tests/integration/real/test_state_mutations.py::TestStateMutations::test_design_reviewer_normalizes_verdict_reject_to_needs_revision
FAILED tests/integration/real/test_state_mutations.py::TestStateMutations::test_code_generator_rejects_stub_code
FAILED tests/integration/real/test_state_mutations.py::TestStateMutations::test_code_generator_rejects_code_with_todo_at_start
FAILED tests/integration/real/test_state_mutations.py::TestStateMutations::test_code_generator_rejects_short_code
FAILED tests/integration/real/test_state_mutations.py::TestStateMutations::test_code_generator_uses_simulation_code_key
FAILED tests/integration/real/test_state_mutations.py::TestStateMutations::test_code_generator_respects_max_code_revisions
FAILED tests/integration/real/test_state_mutations.py::TestStateMutations::test_code_reviewer_sets_verdict_field
FAILED tests/integration/real/test_state_mutations.py::TestStateMutations::test_code_reviewer_increments_counter_on_needs_revision
FAILED tests/integration/real/test_state_mutations.py::TestStateMutations::test_code_reviewer_normalizes_verdict_reject_to_needs_revision
FAILED tests/integration/supervision/test_supervisor_triggers.py::TestSupervisorMaterialCheckpoint::test_supervisor_material_checkpoint_without_pending_materials_escalates
FAILED tests/integration/supervision/test_supervisor_triggers.py::TestSupervisorCodeReviewLimit::test_supervisor_handles_code_review_limit_with_hint
FAILED tests/integration/supervision/test_supervisor_triggers.py::TestSupervisorCodeReviewLimit::test_supervisor_handles_code_review_limit_hint_keyword_only
FAILED tests/integration/supervision/test_supervisor_triggers.py::TestSupervisorDesignReviewLimit::test_supervisor_handles_design_review_limit_with_hint
FAILED tests/integration/supervision/test_supervisor_triggers.py::TestSupervisorExecutionFailureLimit::test_supervisor_handles_execution_failure_with_retry
FAILED tests/integration/supervision/test_supervisor_triggers.py::TestSupervisorExecutionFailureLimit::test_supervisor_handles_execution_failure_with_just_retry
FAILED tests/integration/supervision/test_supervisor_triggers.py::TestSupervisorExecutionFailureLimit::test_supervisor_handles_execution_failure_with_just_guidance
FAILED tests/integration/supervision/test_supervisor_triggers.py::TestSupervisorPhysicsFailureLimit::test_supervisor_handles_physics_failure_retry
FAILED tests/integration/supervision/test_supervisor_triggers.py::TestSupervisorPhysicsFailureLimit::test_supervisor_handles_physics_failure_accept_partial
FAILED tests/integration/supervision/test_supervisor_triggers.py::TestSupervisorPhysicsFailureLimit::test_supervisor_handles_physics_failure_partial_keyword
FAILED tests/integration/supervision/test_supervisor_triggers.py::TestSupervisorClarification::test_supervisor_handles_clarification_with_response
FAILED tests/integration/test_code_agents.py::TestCodeGeneratorAlternativeKeys::test_code_generator_uses_simulation_code_key
FAILED tests/integration/test_design_agents.py::TestDesignReviewerVerdictHandling::test_design_reviewer_needs_revision_verdict_increments_counter
FAILED tests/integration/test_design_agents.py::TestDesignReviewerFeedback::test_design_reviewer_populates_feedback_from_feedback_field
FAILED tests/integration/test_invariants.py::TestPromptAdaptorInvariants::test_adapt_prompts_preserves_paper_domain
FAILED tests/llm_client/test_content_builders.py::TestBuildUserContentForPlanner::test_full_state_all_fields_present
FAILED tests/llm_client/test_content_builders.py::TestBuildUserContentForPlanner::test_multiple_figures
FAILED tests/llm_client/test_content_builders.py::TestBuildUserContentForPlanner::test_figure_without_id
FAILED tests/llm_client/test_content_builders.py::TestBuildUserContentForPlanner::test_figure_without_description
FAILED tests/llm_client/test_content_builders.py::TestBuildUserContentForPlanner::test_empty_paper_text
FAILED tests/llm_client/test_content_builders.py::TestBuildUserContentForPlanner::test_missing_paper_text_key
FAILED tests/llm_client/test_content_builders.py::TestBuildUserContentForPlanner::test_figure_with_empty_strings
FAILED tests/llm_client/test_content_builders.py::TestBuildUserContentForPlanner::test_figure_with_only_unknown_keys
FAILED tests/llm_client/test_content_builders.py::TestBuildUserContentForPlanner::test_non_dict_figure_in_list
FAILED tests/llm_client/test_content_builders.py::TestBuildUserContentForPlanner::test_sections_order_consistency
FAILED tests/llm_client/test_content_builders.py::TestBuildUserContentForPlanner::test_separator_format
FAILED tests/llm_client/test_content_builders.py::TestBuildUserContentForPlanner::test_many_figures_performance
FAILED tests/llm_client/test_content_builders.py::TestBuildUserContentForDesigner::test_empty_state
FAILED tests/llm_client/test_schema_loading.py::TestGetAgentSchema::test_get_agent_schema_supervisor
FAILED tests/llm_client/test_schema_loading.py::TestSchemaConsistency::test_supervisor_verdict_enum_is_complete
FAILED tests/state_machine/test_code.py::TestCodePhase::test_code_review_limit_escalates_to_ask_user
FAILED tests/state_machine/test_code.py::TestEdgeCases::test_code_generator_extracts_code_from_simulation_code_key
FAILED tests/state_machine/test_full_flow.py::TestFullSingleStageHappyPath::test_full_single_stage_happy_path
FAILED tests/state_machine/test_full_flow.py::TestCodeRevisionLimitEscalation::test_code_revision_limit_triggers_ask_user
FAILED tests/state_machine/test_full_flow.py::TestSupervisorReplanFlow::test_supervisor_replan_routes_to_plan
FAILED tests/state_machine/test_full_flow.py::TestMultiStageCompletion::test_all_complete_routes_to_report
FAILED tests/state_machine/test_full_flow.py::TestInterruptResumeFlow::test_invoke_pattern_handles_interrupt_correctly
FAILED tests/state_machine/test_full_flow.py::TestSupervisorBacktrackFlow::test_supervisor_backtrack_routes_to_handle_backtrack
FAILED tests/state_machine/test_planning.py::TestAdaptPromptsNode::test_adapt_prompts_stores_adaptations
FAILED tests/test_schema_code_consistency.py::TestWhitelistValidation::test_agent_files_use_whitelisted_patterns
FAILED tests/test_schema_code_consistency.py::TestWhitelistValidation::test_prompts_module_uses_whitelisted_patterns
FAILED tests/test_schema_code_consistency.py::TestSchemaFieldConsistency::test_agent_output_fields_exist_in_schema
FAILED tests/test_schema_code_consistency.py::TestSchemaFieldConsistency::test_adaptation_fields_exist_in_schema
FAILED tests/test_schema_code_consistency.py::TestValidatorFunctionality::test_detects_static_get
FAILED tests/test_schema_code_consistency.py::TestValidatorFunctionality::test_detects_static_get_with_default
FAILED tests/test_schema_code_consistency.py::TestValidatorFunctionality::test_detects_static_in_check
FAILED tests/test_schema_code_consistency.py::TestValidatorFunctionality::test_flags_dynamic_get
FAILED tests/test_schema_code_consistency.py::TestValidatorFunctionality::test_flags_subscript_access
FAILED tests/test_schema_code_consistency.py::TestValidatorFunctionality::test_flags_iteration
FAILED tests/test_schema_code_consistency.py::TestValidatorFunctionality::test_flags_keys_method
FAILED tests/test_schema_code_consistency.py::TestValidatorFunctionality::test_flags_items_method
FAILED tests/test_schema_code_consistency.py::TestValidatorFunctionality::test_flags_getattr
FAILED tests/test_schema_code_consistency.py::TestValidatorFunctionality::test_ignores_non_tracked_variables
FAILED tests/test_schema_code_consistency.py::TestValidatorFunctionality::test_tracks_function_context
FAILED tests/test_schema_code_consistency.py::TestIntegration::test_all_schemas_loadable
FAILED tests/test_schema_code_consistency.py::TestIntegration::test_full_validation_runs_without_error
FAILED tests/workflow/test_full_workflow_success.py::TestPlanReviewPhase::test_plan_review_approve
FAILED tests/workflow/test_full_workflow_success.py::TestPlanReviewPhase::test_plan_review_verdict_normalization
FAILED tests/workflow/test_full_workflow_success.py::TestCodeGenerator::test_code_generator_stub_code_increments_revision
FAILED tests/workflow/test_full_workflow_success.py::TestFullWorkflowSuccess::test_plan_review_approve
FAILED tests/workflow/test_multi_stage_and_supervisor.py::TestSupervisorTriggerHandling::test_code_review_limit_with_hint
FAILED tests/workflow/test_multi_stage_and_supervisor.py::TestSupervisorTriggerHandling::test_design_review_limit_with_hint
FAILED tests/workflow/test_multi_stage_and_supervisor.py::TestSupervisorTriggerHandling::test_execution_failure_limit_retry
FAILED tests/workflow/test_multi_stage_and_supervisor.py::TestSupervisorTriggerHandling::test_physics_failure_limit_accept_partial
FAILED tests/workflow/test_multi_stage_and_supervisor.py::TestSupervisorEdgeCases::test_supervisor_backtrack_decision_from_llm
FAILED tests/workflow/test_planner_validation.py::TestPlanReviewerStructureValidation::test_plan_with_none_stages
FAILED tests/workflow/test_planner_validation.py::TestPlanReviewerVerdictNormalization::test_verdict_pass_normalized_to_approve
FAILED tests/workflow/test_planner_validation.py::TestPlanReviewerVerdictNormalization::test_verdict_approved_normalized_to_approve
FAILED tests/workflow/test_planner_validation.py::TestPlanReviewerVerdictNormalization::test_verdict_accept_normalized_to_approve
FAILED tests/workflow/test_planner_validation.py::TestPlanReviewerReplanCounter::test_replan_count_not_incremented_on_approve
FAILED tests/workflow/test_planner_validation.py::TestPlanReviewerFeedbackExtraction::test_feedback_extracted_from_response
FAILED tests/workflow/test_planner_validation.py::TestPlanReviewerFeedbackExtraction::test_summary_used_as_fallback_for_feedback
FAILED tests/workflow/test_workflow_revisions.py::TestCodeRevisionCycle::test_code_reviewer_feedback_extraction
FAILED tests/workflow/test_workflow_revisions.py::TestCodeGeneratorValidation::test_stub_detection_short_code_with_todo
184 failed, 9121 passed, 1 skipped, 12 deselected, 11 warnings in 31.19s
