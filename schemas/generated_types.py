"""
Auto-generated TypedDict types from JSON schemas.

╔══════════════════════════════════════════════════════════════════════╗
║  DO NOT EDIT MANUALLY - Changes will be overwritten!                 ║
║                                                                       ║
║  Regenerate with: python scripts/generate_types.py                   ║
╚══════════════════════════════════════════════════════════════════════╝

Source schemas:
  - plan_schema.json
  - progress_schema.json
  - metrics_schema.json
  - report_schema.json
  - assumptions_schema.json
  - prompt_adaptations_schema.json
  - planner_output_schema.json
  - simulation_designer_output_schema.json
  - code_generator_output_schema.json
  - prompt_adaptor_output_schema.json
  - supervisor_output_schema.json
  - plan_reviewer_output_schema.json
  - design_reviewer_output_schema.json
  - code_reviewer_output_schema.json
  - execution_validator_output_schema.json
  - physics_sanity_output_schema.json
  - results_analyzer_output_schema.json
  - comparison_validator_output_schema.json

These types are imported by schemas/state.py for type-safe access
to schema-defined structures (ExtractedParameter, Discrepancy, etc.)

Usage:
    from schemas.generated_types import ExtractedParameter, StageProgress
"""

# generated by datamodel-codegen:
#   filename:  comparison_validator_output_schema.json
#   timestamp: 2025-12-01T23:49:06+00:00

from __future__ import annotations

from typing import List, Literal, NotRequired, TypedDict


class AccuracyCheck(TypedDict):
    status: Literal['pass', 'fail', 'warning']
    paper_values_verified: NotRequired[bool]
    simulation_values_verified: NotRequired[bool]
    units_consistent: NotRequired[bool]
    axis_ranges_appropriate: NotRequired[bool]
    notes: NotRequired[str]


class ErrorsFoundItem(TypedDict):
    calculation: NotRequired[str]
    reported_value: NotRequired[float]
    correct_value: NotRequired[float]
    impact: NotRequired[str]


class MathCheck(TypedDict):
    status: Literal['pass', 'fail', 'warning']
    discrepancy_calculations_correct: NotRequired[bool]
    percentage_calculations_correct: NotRequired[bool]
    classification_matches_thresholds: NotRequired[bool]
    errors_found: NotRequired[List[ErrorsFoundItem]]
    notes: NotRequired[str]


class Misclassification(TypedDict):
    result_id: NotRequired[str]
    reported_classification: NotRequired[str]
    correct_classification: NotRequired[str]
    discrepancy_value: NotRequired[float]
    threshold_used: NotRequired[float]


class ClassificationCheck(TypedDict):
    status: NotRequired[Literal['pass', 'fail', 'warning']]
    misclassifications: NotRequired[List[Misclassification]]
    notes: NotRequired[str]


class DocumentationCheck(TypedDict):
    status: NotRequired[Literal['pass', 'fail', 'warning']]
    all_discrepancies_logged: NotRequired[bool]
    sources_cited: NotRequired[bool]
    assumptions_documented: NotRequired[bool]
    missing_documentation: NotRequired[List[str]]
    notes: NotRequired[str]


class Issue(TypedDict):
    severity: Literal['blocking', 'major', 'minor']
    category: Literal['accuracy', 'math', 'classification', 'documentation']
    description: str
    suggested_fix: NotRequired[str]


class ComparisonValidatorAgentOutput(TypedDict):
    stage_id: str
    verdict: Literal['approve', 'needs_revision']
    accuracy_check: AccuracyCheck
    math_check: MathCheck
    classification_check: NotRequired[ClassificationCheck]
    documentation_check: NotRequired[DocumentationCheck]
    issues: NotRequired[List[Issue]]
    revision_suggestions: NotRequired[List[str]]
    summary: str
