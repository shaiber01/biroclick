# generated by datamodel-codegen:
#   filename:  prompt_adaptor_output_schema.json
#   timestamp: 2025-12-05T23:05:02+00:00

from __future__ import annotations

from typing import List, Literal, NotRequired, Optional, TypedDict


class AnalysisSummary(TypedDict):
    domain: str
    sub_domain: NotRequired[str]
    key_materials: List[str]
    key_phenomena: List[str]
    simulation_techniques_mentioned: NotRequired[List[str]]
    simulation_challenges: NotRequired[List[str]]
    identified_gaps: NotRequired[List[str]]


class PromptModification(TypedDict):
    id: str
    target_agent: Literal[
        'PlannerAgent',
        'PlanReviewerAgent',
        'SimulationDesignerAgent',
        'DesignReviewerAgent',
        'CodeGeneratorAgent',
        'CodeReviewerAgent',
        'ExecutionValidatorAgent',
        'PhysicsSanityAgent',
        'ResultsAnalyzerAgent',
        'ComparisonValidatorAgent',
        'SupervisorAgent',
        'ReportGeneratorAgent',
    ]
    target_file: NotRequired[str]
    section: NotRequired[str]
    modification_type: Literal['append', 'modify', 'disable']
    confidence: float
    original_content: NotRequired[Optional[str]]
    new_content: str
    reasoning: str
    impact: NotRequired[Literal['high', 'medium', 'low']]


class AgentsNotModifiedItem(TypedDict):
    agent: NotRequired[str]
    reason: NotRequired[str]


class PromptAdaptorAgentOutput(TypedDict):
    paper_id: str
    analysis_summary: AnalysisSummary
    prompt_modifications: List[PromptModification]
    agents_not_modified: NotRequired[List[AgentsNotModifiedItem]]
    warnings: NotRequired[List[str]]
    escalation_needed: NotRequired[bool]
    escalation_question: NotRequired[Optional[str]]
    domain_confidence: NotRequired[float]
    adaptation_log_file: NotRequired[str]
