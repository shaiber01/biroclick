# generated by datamodel-codegen:
#   filename:  results_analyzer_output_schema.json
#   timestamp: 2025-12-03T07:47:14+00:00

from __future__ import annotations

from typing import List, Literal, NotRequired, TypedDict


class SimulatedValue(TypedDict):
    value: NotRequired[float]
    unit: NotRequired[str]


class PaperValue(TypedDict):
    value: NotRequired[float]
    unit: NotRequired[str]
    source: NotRequired[str]


class Discrepancy(TypedDict):
    absolute: NotRequired[float]
    relative_percent: NotRequired[float]
    classification: NotRequired[
        Literal['excellent', 'acceptable', 'investigate', 'unacceptable']
    ]


class PerResultReport(TypedDict):
    result_id: str
    target_figure: str
    quantity: str
    simulated_value: NotRequired[SimulatedValue]
    paper_value: NotRequired[PaperValue]
    discrepancy: Discrepancy
    notes: NotRequired[str]


class FigureComparison(TypedDict):
    paper_figure_id: str
    simulated_figure_path: str
    comparison_type: Literal[
        'overlay', 'side_by_side', 'difference_map', 'quantitative_only'
    ]
    visual_agreement: NotRequired[Literal['excellent', 'good', 'fair', 'poor']]
    key_features_matched: NotRequired[List[str]]
    key_features_missed: NotRequired[List[str]]
    notes: NotRequired[str]


class SystematicDiscrepancy(TypedDict):
    pattern: NotRequired[str]
    affected_results: NotRequired[List[str]]
    possible_cause: NotRequired[str]


class ResultsAnalyzerAgentOutput(TypedDict):
    stage_id: str
    per_result_reports: List[PerResultReport]
    figure_comparisons: List[FigureComparison]
    overall_classification: Literal[
        'EXCELLENT_MATCH', 'ACCEPTABLE_MATCH', 'PARTIAL_MATCH', 'POOR_MATCH', 'FAILED'
    ]
    classification_rationale: NotRequired[str]
    confidence: NotRequired[float]
    confidence_reason: NotRequired[str]
    confidence_factors: NotRequired[List[str]]
    systematic_discrepancies: NotRequired[List[SystematicDiscrepancy]]
    recommendations: NotRequired[List[str]]
    summary: str
